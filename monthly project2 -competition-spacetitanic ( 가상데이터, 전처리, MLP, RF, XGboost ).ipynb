{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train.csv - 승객의 약 3분의 2(~8700)에 대한 개인 기록으로, 교육 데이터로 사용됩니다.\n",
    "* PassengerId - 각 승객의 고유 ID입니다. 각 ID는 gggg_pp 형식을 취합니다. 여기서 gggg는 승객이 함께 여행하는 그룹을 나타내고 pp는 그룹 내에서 승객의 번호입니다. 한 그룹의 사람들은 종종 가족 구성원이지만 항상은 아닙니다.\n",
    "* 홈 플래닛 - 승객이 출발한 행성, 일반적으로 영구 거주하는 행성.\n",
    "* CryoSleep - 승객이 항해 중에 일시 중단된 애니메이션으로 전환되도록 선택했는지 여부를 나타냅니다. 냉동 수면 중인 승객들은 객실에 제한되어 있습니다.\n",
    "* 객실 - 승객이 머물고 있는 객실 번호입니다. deck/num/side 형식을 취합니다. 여기서 side는 좌현의 경우 P 또는 우현의 경우 S입니다.\n",
    "* 목적지 - 승객이 출발할 행성.\n",
    "* 연령 - 탑승자의 연령입니다.\n",
    "* VIP - 승객이 항해 중에 특별 VIP 서비스에 대한 비용을 지불했는지 여부.\n",
    "* 룸 서비스, 푸드 코트, 쇼핑몰, 스파, VR덱 - 승객이 우주선 타이타닉의 다양한 고급 편의 시설에 청구한 금액.\n",
    "* 이름 - 승객의 이름과 성입니다.\n",
    "* Transported - 승객이 다른 차원으로 운송되었는지 여부를 나타냅니다. 이것이 예측하려는 대상 열입니다.\n",
    "---\n",
    "* test.csv - 나머지 승객의 3분의 1(~4300명)에 대한 개인 기록을 테스트 데이터로 사용합니다. 당신의 임무는 이 세트의 승객들을 위한 운송의 가치를 예측하는 것입니다.\n",
    "* sample_submission.csv - 올바른 형식의 제출 파일입니다.\n",
    "* PassengerId - 테스트 세트의 각 승객에 대한 ID.\n",
    "\n",
    "\n",
    "\n",
    "어떤 안좋은 시고로 인해서 일부 승객이 다른 차원으로 transported 되버림. 이런 불쌍한 승객들을 찾아서 구조하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼 gggg 가 같으면 같은 그룹이라는 거고. 그룹 라벨을 하나 만들면 좋겠고\n",
    "홈플래닛 -> 인코딩\n",
    "룸서비스는 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (추가적으로 고려할것) 참고 전략\n",
    "\n",
    "\n",
    "전체적으로 이상치가 많은 데이터이므로(몇몇 매우 큰값들이 존재), 이를 방지할수 있는 log 스케일링이라던가, 이상치에 덜 민감한 랜덤포레스트 같은 전략을 취할 필요가 있다.\n",
    "\n",
    "EDA\n",
    "\n",
    "1.target 기준으로 나눠서 violin plot, 누적 bar 그래프 참고\n",
    "\n",
    "scaling\n",
    "\n",
    "We're going to see what transformation works better for each feature and select them, the idea is to compress the data. There could be situations where you will have to stretch the data. These are the methods applied:\n",
    "\n",
    "**Log Transformation**: This transformation involves taking the logarithm of each data point. It is useful when the data is highly skewed and the variance increases with the mean.\n",
    "\n",
    "                                  y = log(x)\n",
    "**Square Root Transformation**: This transformation involves taking the square root of each data point. It is useful when the data is highly skewed and the variance increases with the mean.\n",
    "\n",
    "                                  y = sqrt(x)\n",
    "**Box-Cox Transformation**: This transformation is a family of power transformations that includes the log and square root transformations as special cases. It is useful when the data is highly skewed and the variance increases with the mean.\n",
    "\n",
    "                                  y = [(x^lambda) - 1] / lambda if lambda != 0\n",
    "                                  y = log(x) if lambda = 0\n",
    "**Yeo-Johnson Transformation**: This transformation is similar to the Box-Cox transformation, but it can be applied to both positive and negative values. It is useful when the data is highly skewed and the variance increases with the mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_row', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(\"C:\\\\Users\\\\김승호\\\\spaceship-titanic\\\\train.csv\")\n",
    "df = df_original.copy()\n",
    "df\n",
    "\n",
    "\n",
    "df2 = pd.read_csv(\"C:\\\\Users\\\\김승호\\\\spaceship-titanic\\\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maham Ofracculy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Juanna Vines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Altark Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003_02</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>Solam Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/1/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Willy Santantines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8688</th>\n",
       "      <td>9276_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/98/P</td>\n",
       "      <td>55 Cancri e</td>\n",
       "      <td>41.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6819.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1643.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Gravior Noxnuther</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8689</th>\n",
       "      <td>9278_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>True</td>\n",
       "      <td>G/1499/S</td>\n",
       "      <td>PSO J318.5-22</td>\n",
       "      <td>18.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Kurta Mondalley</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8690</th>\n",
       "      <td>9279_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>G/1500/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>26.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1872.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Fayey Connon</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8691</th>\n",
       "      <td>9280_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>E/608/S</td>\n",
       "      <td>55 Cancri e</td>\n",
       "      <td>32.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>3235.0</td>\n",
       "      <td>Celeon Hontichre</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>9280_02</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>E/608/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>44.0</td>\n",
       "      <td>False</td>\n",
       "      <td>126.0</td>\n",
       "      <td>4688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Propsh Hontichre</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8693 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n",
       "0        0001_01     Europa     False     B/0/P    TRAPPIST-1e  39.0  False   \n",
       "1        0002_01      Earth     False     F/0/S    TRAPPIST-1e  24.0  False   \n",
       "2        0003_01     Europa     False     A/0/S    TRAPPIST-1e  58.0   True   \n",
       "3        0003_02     Europa     False     A/0/S    TRAPPIST-1e  33.0  False   \n",
       "4        0004_01      Earth     False     F/1/S    TRAPPIST-1e  16.0  False   \n",
       "...          ...        ...       ...       ...            ...   ...    ...   \n",
       "8688     9276_01     Europa     False    A/98/P    55 Cancri e  41.0   True   \n",
       "8689     9278_01      Earth      True  G/1499/S  PSO J318.5-22  18.0  False   \n",
       "8690     9279_01      Earth     False  G/1500/S    TRAPPIST-1e  26.0  False   \n",
       "8691     9280_01     Europa     False   E/608/S    55 Cancri e  32.0  False   \n",
       "8692     9280_02     Europa     False   E/608/S    TRAPPIST-1e  44.0  False   \n",
       "\n",
       "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
       "0             0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
       "1           109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
       "2            43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
       "3             0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
       "4           303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
       "...           ...        ...           ...     ...     ...                ...   \n",
       "8688          0.0     6819.0           0.0  1643.0    74.0  Gravior Noxnuther   \n",
       "8689          0.0        0.0           0.0     0.0     0.0    Kurta Mondalley   \n",
       "8690          0.0        0.0        1872.0     1.0     0.0       Fayey Connon   \n",
       "8691          0.0     1049.0           0.0   353.0  3235.0   Celeon Hontichre   \n",
       "8692        126.0     4688.0           0.0     0.0    12.0   Propsh Hontichre   \n",
       "\n",
       "      Transported  \n",
       "0           False  \n",
       "1            True  \n",
       "2           False  \n",
       "3           False  \n",
       "4            True  \n",
       "...           ...  \n",
       "8688        False  \n",
       "8689        False  \n",
       "8690         True  \n",
       "8691        False  \n",
       "8692         True  \n",
       "\n",
       "[8693 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8693 entries, 0 to 8692\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   8693 non-null   object \n",
      " 1   HomePlanet    8492 non-null   object \n",
      " 2   CryoSleep     8476 non-null   object \n",
      " 3   Cabin         8494 non-null   object \n",
      " 4   Destination   8511 non-null   object \n",
      " 5   Age           8514 non-null   float64\n",
      " 6   VIP           8490 non-null   object \n",
      " 7   RoomService   8512 non-null   float64\n",
      " 8   FoodCourt     8510 non-null   float64\n",
      " 9   ShoppingMall  8485 non-null   float64\n",
      " 10  Spa           8510 non-null   float64\n",
      " 11  VRDeck        8505 non-null   float64\n",
      " 12  Name          8493 non-null   object \n",
      " 13  Transported   8693 non-null   bool   \n",
      "dtypes: bool(1), float64(6), object(7)\n",
      "memory usage: 891.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId \n",
      " ['0001_01' '0002_01' '0003_01' ... '9279_01' '9280_01' '9280_02']\n",
      "유니크한 카테고리 수 :  8693\n",
      "null 수              :  0 \n",
      "\n",
      "HomePlanet \n",
      " ['Europa' 'Earth' 'Mars' nan]\n",
      "유니크한 카테고리 수 :  4\n",
      "null 수              :  201 \n",
      "\n",
      "CryoSleep \n",
      " [False True nan]\n",
      "유니크한 카테고리 수 :  3\n",
      "null 수              :  217 \n",
      "\n",
      "Cabin \n",
      " ['B/0/P' 'F/0/S' 'A/0/S' ... 'G/1499/S' 'G/1500/S' 'E/608/S']\n",
      "유니크한 카테고리 수 :  6561\n",
      "null 수              :  199 \n",
      "\n",
      "Destination \n",
      " ['TRAPPIST-1e' 'PSO J318.5-22' '55 Cancri e' nan]\n",
      "유니크한 카테고리 수 :  4\n",
      "null 수              :  182 \n",
      "\n",
      "Age \n",
      " [39. 24. 58. 33. 16. 44. 26. 28. 35. 14. 34. 45. 32. 48. 31. 27.  0.  1.\n",
      " 49. 29. 10.  7. 21. 62. 15. 43. 47.  2. 20. 23. 30. 17. 55.  4. 19. 56.\n",
      " nan 25. 38. 36. 22. 18. 42. 37. 13.  8. 40.  3. 54.  9.  6. 64. 67. 61.\n",
      " 50. 41. 57. 11. 52. 51. 46. 60. 63. 59.  5. 79. 68. 74. 12. 53. 65. 71.\n",
      " 75. 70. 76. 78. 73. 66. 69. 72. 77.]\n",
      "유니크한 카테고리 수 :  81\n",
      "null 수              :  179 \n",
      "\n",
      "VIP \n",
      " [False True nan]\n",
      "유니크한 카테고리 수 :  3\n",
      "null 수              :  203 \n",
      "\n",
      "RoomService \n",
      " [   0.  109.   43. ... 1569. 8586.  745.]\n",
      "유니크한 카테고리 수 :  1274\n",
      "null 수              :  181 \n",
      "\n",
      "FoodCourt \n",
      " [   0.    9. 3576. ... 3208. 6819. 4688.]\n",
      "유니크한 카테고리 수 :  1508\n",
      "null 수              :  183 \n",
      "\n",
      "ShoppingMall \n",
      " [   0.   25.  371. ... 1085.  510. 1872.]\n",
      "유니크한 카테고리 수 :  1116\n",
      "null 수              :  208 \n",
      "\n",
      "Spa \n",
      " [   0.  549. 6715. ... 2868. 1107. 1643.]\n",
      "유니크한 카테고리 수 :  1328\n",
      "null 수              :  183 \n",
      "\n",
      "VRDeck \n",
      " [   0.   44.   49. ... 1164.  971. 3235.]\n",
      "유니크한 카테고리 수 :  1307\n",
      "null 수              :  188 \n",
      "\n",
      "Name \n",
      " ['Maham Ofracculy' 'Juanna Vines' 'Altark Susent' ... 'Fayey Connon'\n",
      " 'Celeon Hontichre' 'Propsh Hontichre']\n",
      "유니크한 카테고리 수 :  8474\n",
      "null 수              :  200 \n",
      "\n",
      "Transported \n",
      " [False  True]\n",
      "유니크한 카테고리 수 :  2\n",
      "null 수              :  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns :\n",
    "    print(col,\"\\n\", df[col].unique())\n",
    "    print('유니크한 카테고리 수 : ',len(df[col].unique()))\n",
    "    print('null 수              : ', df[col].isna().sum(),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['group'] = [int(s[:4]) for s in df.PassengerId]\n",
    "df2['group'] = [int(s[:4]) for s in df2.PassengerId]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "656     0686_01       Mars      True  F/146/P  TRAPPIST-1e  18.0  False   \n",
      "657     0686_02       Mars     False   D/30/P  TRAPPIST-1e  29.0   True   \n",
      "\n",
      "     RoomService  FoodCourt  ShoppingMall   Spa  VRDeck         Name  \\\n",
      "656          0.0        0.0           0.0   0.0     0.0  Permit Bert   \n",
      "657       3887.0        0.0           NaN  12.0     0.0   Cheat Bert   \n",
      "\n",
      "     Transported  group  \n",
      "656         True    686  \n",
      "657        False    686  \n",
      "=================================== \n",
      "\n",
      "    PassengerId HomePlanet CryoSleep   Cabin  Destination   Age    VIP  \\\n",
      "350     0384_01      Earth     False  G/56/S  55 Cancri e   0.0  False   \n",
      "351     0384_02      Earth     False  G/56/S  TRAPPIST-1e  14.0  False   \n",
      "\n",
      "     RoomService  FoodCourt  ShoppingMall   Spa  VRDeck              Name  \\\n",
      "350          0.0        0.0           0.0   0.0     0.0  Cliney Simpsonks   \n",
      "351          3.0        0.0         881.0  25.0     0.0  Ellard Simpsonks   \n",
      "\n",
      "     Transported  group  \n",
      "350         True    384  \n",
      "351         True    384  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "1516     1611_01      Earth     False  G/248/S  TRAPPIST-1e  42.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck          Name  \\\n",
      "1516          0.0      865.0           0.0  0.0    44.0  Lorey Torrez   \n",
      "\n",
      "      Transported  group  \n",
      "1516         True   1611  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "5464     5825_01     Europa     False  B/198/P  TRAPPIST-1e  43.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall    Spa  VRDeck               Name  \\\n",
      "5464          0.0     8808.0           0.0  580.0     0.0  Betenar Apenelexy   \n",
      "\n",
      "      Transported  group  \n",
      "5464         True   5825  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "3581     3851_01     Europa     False  C/127/P  TRAPPIST-1e  57.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck             Name  \\\n",
      "3581        205.0     4415.0           0.0  1589.0   305.0  Alino Namsheary   \n",
      "\n",
      "      Transported  group  \n",
      "3581        False   3851  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "5422     5792_01     Europa     False  B/196/P  55 Cancri e  40.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck            Name  \\\n",
      "5422          0.0     3338.0           0.0  6344.0   199.0  Eltares Subwor   \n",
      "\n",
      "      Transported  group  \n",
      "5422        False   5792  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "4334     4621_01       Mars     False  E/289/P  TRAPPIST-1e  28.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck        Name  \\\n",
      "4334          0.0       79.0        2003.0  0.0     0.0  Nios Jaflé   \n",
      "\n",
      "      Transported  group  \n",
      "4334         True   4621  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "4443     4725_01      Earth     False  G/767/P  55 Cancri e  27.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck             Name  \\\n",
      "4443        866.0        0.0           0.0  0.0    23.0  Leony Georgasey   \n",
      "\n",
      "      Transported  group  \n",
      "4443        False   4725  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "1820     1945_01      Earth     False  E/120/P  TRAPPIST-1e  15.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck             Name  \\\n",
      "1820          0.0        0.0         787.0  0.0     0.0  Ambert Guezaley   \n",
      "\n",
      "      Transported  group  \n",
      "1820        False   1945  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "2556     2741_01       Mars     False  E/164/P  TRAPPIST-1e  43.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck         Name  \\\n",
      "2556        471.0        0.0         133.0  2.0   692.0  Jackix Cake   \n",
      "\n",
      "      Transported  group  \n",
      "2556        False   2741  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "4648     4946_01      Earth      True  G/799/P  TRAPPIST-1e  25.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck            Name  \\\n",
      "4648          0.0        0.0           0.0  0.0     0.0  Helley Kiddley   \n",
      "\n",
      "      Transported  group  \n",
      "4648        False   4946  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "4430     4709_01      Earth     False  F/957/P  55 Cancri e  38.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck            Name  \\\n",
      "4430       2614.0        0.0         103.0  0.0     0.0  Terrie Morelly   \n",
      "\n",
      "      Transported  group  \n",
      "4430        False   4709  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "3606     3872_01       Mars      True  F/799/P  TRAPPIST-1e  30.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck          Name  \\\n",
      "3606          0.0        0.0           0.0  0.0     0.0  Cupers Quart   \n",
      "\n",
      "      Transported  group  \n",
      "3606         True   3872  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "7294     7802_01       Mars     False  F/1620/P  TRAPPIST-1e  33.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall    Spa  VRDeck         Name  \\\n",
      "7294        166.0      634.0         419.0  201.0  1178.0  Backse Pead   \n",
      "\n",
      "      Transported  group  \n",
      "7294        False   7802  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "4806     5130_01      Earth      True  G/838/S  TRAPPIST-1e  37.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck              Name  \\\n",
      "4806          0.0        NaN           0.0  0.0     0.0  Timmy Klindsayer   \n",
      "\n",
      "      Transported  group  \n",
      "4806         True   5130  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "6440     6799_01     Europa      True  C/252/S  55 Cancri e  40.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck              Name  \\\n",
      "6440          0.0        0.0           0.0  0.0     0.0  Terops Namsheary   \n",
      "\n",
      "      Transported  group  \n",
      "6440         True   6799  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep   Cabin  Destination   Age    VIP  \\\n",
      "1965     2097_01     Europa      True  B/86/S  55 Cancri e  29.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck              Name  \\\n",
      "1965          0.0        0.0           0.0  0.0     0.0  Tetra Roftfuling   \n",
      "\n",
      "      Transported  group  \n",
      "1965         True   2097  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "6903     7313_01       Mars     False  D/231/P  TRAPPIST-1e  41.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck         Name  \\\n",
      "6903       2754.0      398.0          21.0  0.0     0.0  Conth Kashe   \n",
      "\n",
      "      Transported  group  \n",
      "6903        False   7313  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "4955     5282_01      Earth     False   E/337/S  TRAPPIST-1e  35.0  False   \n",
      "4956     5282_02      Earth     False  F/1016/S  TRAPPIST-1e  26.0  False   \n",
      "4957     5282_03      Earth      True   G/860/S  55 Cancri e   0.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck           Name  \\\n",
      "4955         35.0        0.0          14.0  0.0   816.0  Florey Torrez   \n",
      "4956          0.0      196.0           0.0  0.0   487.0   Eulah Torrez   \n",
      "4957          0.0        0.0           0.0  0.0     0.0   Kenda Torrez   \n",
      "\n",
      "      Transported  group  \n",
      "4955         True   5282  \n",
      "4956         True   5282  \n",
      "4957         True   5282  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "7982     8539_01      Earth     False  G/1372/S  TRAPPIST-1e  20.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck          Name  \\\n",
      "7982         59.0       29.0          27.0  NaN     0.0  Tance Simsey   \n",
      "\n",
      "      Transported  group  \n",
      "7982        False   8539  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "7986     8542_01       Mars      True  E/544/P  TRAPPIST-1e  19.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck       Name  \\\n",
      "7986          0.0        0.0           0.0  0.0     0.0  Dow Chate   \n",
      "\n",
      "      Transported  group  \n",
      "7986        False   8542  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin    Destination  Age    VIP  \\\n",
      "1311     1390_01      Earth      True  G/215/P  PSO J318.5-22  1.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck            Name  \\\n",
      "1311          0.0        0.0           NaN  0.0     0.0  Mathy Meltoney   \n",
      "\n",
      "      Transported  group  \n",
      "1311         True   1390  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep   Cabin  Destination   Age    VIP  \\\n",
      "6919     7337_01     Europa     False  A/89/S  TRAPPIST-1e  25.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck              Name  \\\n",
      "6919          0.0      115.0           0.0  1823.0  4041.0  Ranons Folkshbox   \n",
      "\n",
      "      Transported  group  \n",
      "6919        False   7337  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "7002     7447_01       Mars     False  F/1539/P  TRAPPIST-1e  24.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall    Spa  VRDeck        Name  \\\n",
      "7002          NaN        0.0         299.0  814.0     0.0  Chees Oncy   \n",
      "\n",
      "      Transported  group  \n",
      "7002        False   7447  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "5917     6276_01      Earth      True  G/1011/P  55 Cancri e  23.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck             Name  \\\n",
      "5917          0.0        0.0           0.0  0.0     0.0  Coracy Grifford   \n",
      "\n",
      "      Transported  group  \n",
      "5917         True   6276  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "4843     5166_01       Mars     False  D/163/P  TRAPPIST-1e  32.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall   Spa  VRDeck       Name  \\\n",
      "4843        382.0        0.0         477.0  22.0     2.0  Cor Porki   \n",
      "\n",
      "      Transported  group  \n",
      "4843         True   5166  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "7930     8467_01      Earth      True  G/1379/P  55 Cancri e  14.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck                Name  \\\n",
      "7930          0.0        0.0           0.0  0.0     0.0  Branda Hebernandry   \n",
      "\n",
      "      Transported  group  \n",
      "7930         True   8467  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "3558     3823_01       Mars     False  F/719/S  TRAPPIST-1e  27.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck         Name  \\\n",
      "3558       1192.0        0.0           0.0  0.0     0.0  Crisp Spore   \n",
      "\n",
      "      Transported  group  \n",
      "3558         True   3823  \n",
      "=================================== \n",
      "\n",
      "   PassengerId HomePlanet CryoSleep   Cabin  Destination   Age    VIP  \\\n",
      "39     0038_01      Earth     False  F/10/S  55 Cancri e  20.0  False   \n",
      "\n",
      "    RoomService  FoodCourt  ShoppingMall     Spa  VRDeck           Name  \\\n",
      "39        554.0      195.0           0.0  2606.0     0.0  Loise Wheelez   \n",
      "\n",
      "    Transported  group  \n",
      "39        False     38  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "7610     8130_01      Earth      True  G/1308/S  TRAPPIST-1e  23.0  False   \n",
      "7611     8130_02      Earth     False  G/1308/S  55 Cancri e   4.0  False   \n",
      "7612     8130_03      Earth      True  G/1308/S  TRAPPIST-1e   0.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck               Name  \\\n",
      "7610          0.0        0.0           0.0  0.0     0.0  Morrie Mcbriddley   \n",
      "7611          0.0        0.0           0.0  0.0     0.0    Brie Mcbriddley   \n",
      "7612          0.0        0.0           0.0  0.0     0.0   Ferry Mcbriddley   \n",
      "\n",
      "      Transported  group  \n",
      "7610        False   8130  \n",
      "7611        False   8130  \n",
      "7612        False   8130  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "6945     7370_01      Earth     False  F/1525/P  TRAPPIST-1e  21.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck           Name  \\\n",
      "6945          0.0       70.0           2.0  0.0   486.0  Frandy Cartiz   \n",
      "\n",
      "      Transported  group  \n",
      "6945        False   7370  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "7903     8444_01      Earth     False  F/1733/P  TRAPPIST-1e  48.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck             Name  \\\n",
      "7903          0.0     1570.0           1.0  0.0     2.0  Robyna Monsoney   \n",
      "\n",
      "      Transported  group  \n",
      "7903        False   8444  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "8121     8672_01      Earth     False  G/1406/P  TRAPPIST-1e  40.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall    Spa  VRDeck         Name  \\\n",
      "8121         21.0      155.0           0.0  267.0     0.0  Cary Rilley   \n",
      "\n",
      "      Transported  group  \n",
      "8121        False   8672  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age  VIP  \\\n",
      "6978     7416_01     Europa     False  B/278/S  55 Cancri e  41.0  NaN   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck              Name  \\\n",
      "6978          0.0    10049.0         214.0  5287.0  1845.0  Astrah Burcaling   \n",
      "\n",
      "      Transported  group  \n",
      "6978        False   7416  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "8141     8700_01       Mars     False  F/1684/S  TRAPPIST-1e  13.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck        Name  \\\n",
      "8141        217.0        7.0         928.0  0.0   728.0  Mose Teake   \n",
      "\n",
      "      Transported  group  \n",
      "8141        False   8700  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination  Age    VIP  \\\n",
      "4580     4882_01     Europa     False  B/193/S  TRAPPIST-1e  NaN  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall   Spa  VRDeck                Name  \\\n",
      "4580          9.0     2031.0           0.0  20.0  1837.0  Tabdhib Switmorary   \n",
      "\n",
      "      Transported  group  \n",
      "4580        False   4882  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "7348     7863_01      Earth     False  F/1505/S  TRAPPIST-1e  17.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck             Name  \\\n",
      "7348         99.0      516.0        1967.0  0.0     6.0  Tine Popelacruz   \n",
      "\n",
      "      Transported  group  \n",
      "7348         True   7863  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep Cabin  Destination   Age    VIP  \\\n",
      "3345     3598_01      Earth     False   NaN  55 Cancri e  19.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck             Name  \\\n",
      "3345        616.0        0.0          46.0  0.0    14.0  Dony Olivasquez   \n",
      "\n",
      "      Transported  group  \n",
      "3345        False   3598  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin    Destination   Age   VIP  \\\n",
      "4336     4623_01     Europa     False  C/146/P  PSO J318.5-22  34.0  True   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck               Name  \\\n",
      "4336          0.0        NaN          21.0  7.0    16.0  Sadrix Repumparte   \n",
      "\n",
      "      Transported  group  \n",
      "4336         True   4623  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n",
      "7272     7777_01      Earth      True  G/1249/P  PSO J318.5-22  21.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck          Name  \\\n",
      "7272          0.0        0.0           0.0  0.0     0.0  Camie Meleon   \n",
      "\n",
      "      Transported  group  \n",
      "7272         True   7777  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "4792     5115_01     Europa      True  C/199/S  TRAPPIST-1e  41.0  False   \n",
      "4793     5115_02     Europa      True  C/199/S  55 Cancri e  28.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck            Name  \\\n",
      "4792          0.0        0.0           0.0  0.0     0.0   Diason Whibid   \n",
      "4793          0.0        0.0           0.0  0.0     0.0  Megrezn Whibid   \n",
      "\n",
      "      Transported  group  \n",
      "4792         True   5115  \n",
      "4793         True   5115  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin    Destination   Age    VIP  \\\n",
      "4353     4633_01      Earth     False  G/754/P  PSO J318.5-22  41.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck            Name  \\\n",
      "4353        454.0      297.0           0.0  2.0     0.0  Glenna Gallard   \n",
      "\n",
      "      Transported  group  \n",
      "4353        False   4633  \n",
      "=================================== \n",
      "\n",
      "    PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "860     0925_01       Mars     False  F/191/P  TRAPPIST-1e  34.0  False   \n",
      "861     0925_02       Mars      True  F/191/P  TRAPPIST-1e  11.0  False   \n",
      "862     0925_03       Mars      True  F/191/P  TRAPPIST-1e  41.0  False   \n",
      "\n",
      "     RoomService  FoodCourt  ShoppingMall  Spa  VRDeck          Name  \\\n",
      "860        379.0        0.0        1626.0  0.0     0.0    Skix Kraie   \n",
      "861          0.0        0.0           0.0  0.0     0.0  Varkes Kraie   \n",
      "862          0.0        NaN           0.0  0.0     0.0  Almone Kraie   \n",
      "\n",
      "     Transported  group  \n",
      "860        False    925  \n",
      "861         True    925  \n",
      "862         True    925  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep   Cabin  Destination   Age    VIP  \\\n",
      "2695     2890_01     Europa     False  B/89/P  TRAPPIST-1e  44.0  False   \n",
      "2696     2890_02     Europa      True  B/89/P  TRAPPIST-1e  36.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck              Name  \\\n",
      "2695          0.0      351.0           0.0  3.0  3447.0  Jabbab Cattyried   \n",
      "2696          0.0        0.0           0.0  0.0     0.0  Suhain Cattyried   \n",
      "\n",
      "      Transported  group  \n",
      "2695        False   2890  \n",
      "2696         True   2890  \n",
      "=================================== \n",
      "\n",
      "   PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
      "18     0016_01       Mars      True  F/5/P  TRAPPIST-1e  45.0  False   \n",
      "\n",
      "    RoomService  FoodCourt  ShoppingMall  Spa  VRDeck        Name  \\\n",
      "18          0.0        0.0           0.0  0.0     0.0  Alus Upead   \n",
      "\n",
      "    Transported  group  \n",
      "18         True     16  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "3847     4116_01      Earth     False  F/774/S  TRAPPIST-1e  21.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck          Name  \\\n",
      "3847         40.0        0.0         736.0  0.0     0.0  Ryany Torrez   \n",
      "\n",
      "      Transported  group  \n",
      "3847         True   4116  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "6890     7297_01      Earth     False  G/1188/S  TRAPPIST-1e  40.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck Name  Transported  \\\n",
      "6890        678.0        0.0         229.0  0.0     0.0  NaN        False   \n",
      "\n",
      "      group  \n",
      "6890   7297  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "5081     5427_01        NaN      True  F/1118/P  TRAPPIST-1e  18.0  False   \n",
      "5082     5427_02       Mars     False  F/1118/P  TRAPPIST-1e  18.0  False   \n",
      "5083     5427_03       Mars     False  F/1118/P  TRAPPIST-1e  25.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall    Spa  VRDeck          Name  \\\n",
      "5081          0.0        0.0           0.0    0.0     0.0    Frohs Stie   \n",
      "5082       3458.0        0.0         216.0    0.0     0.0  Panbug Blane   \n",
      "5083        948.0        0.0           1.0  170.0     0.0   Craird Stie   \n",
      "\n",
      "      Transported  group  \n",
      "5081         True   5427  \n",
      "5082        False   5427  \n",
      "5083        False   5427  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep   Cabin  Destination   Age    VIP  \\\n",
      "2375     2555_01     Europa      True  B/78/P  55 Cancri e  24.0  False   \n",
      "2376     2555_02     Europa      True  B/78/P  TRAPPIST-1e  34.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck               Name  \\\n",
      "2375          0.0        0.0           0.0  0.0     0.0  Branasl Glisormal   \n",
      "2376          0.0        0.0           0.0  0.0     0.0   Aldera Glisormal   \n",
      "\n",
      "      Transported  group  \n",
      "2375         True   2555  \n",
      "2376         True   2555  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n",
      "7795     8320_01      Earth      True  G/1352/P  PSO J318.5-22  19.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck             Name  \\\n",
      "7795          0.0        0.0           0.0  0.0     0.0  Beula Mckinsond   \n",
      "\n",
      "      Transported  group  \n",
      "7795         True   8320  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "7563     8084_01     Europa     False  C/264/P  TRAPPIST-1e  31.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
      "7563          NaN      745.0           0.0  6688.0  2161.0  Schyon Idercaling   \n",
      "\n",
      "      Transported  group  \n",
      "7563        False   8084  \n",
      "=================================== \n",
      "\n",
      "    PassengerId HomePlanet CryoSleep   Cabin  Destination   Age    VIP  \\\n",
      "474     0510_01     Europa     False  C/16/P  TRAPPIST-1e  59.0  False   \n",
      "\n",
      "     RoomService  FoodCourt  ShoppingMall    Spa  VRDeck               Name  \\\n",
      "474          0.0    16521.0           0.0  323.0     0.0  Astrark Sinecthex   \n",
      "\n",
      "     Transported  group  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474         True    510  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "4109     4388_01     Europa     False  B/171/S  TRAPPIST-1e  33.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck Name  Transported  \\\n",
      "4109          0.0        0.0           0.0  0.0     0.0  NaN         True   \n",
      "\n",
      "      group  \n",
      "4109   4388  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "4326     4609_01     Europa     False  C/173/S  TRAPPIST-1e  37.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck              Name  \\\n",
      "4326          1.0      477.0           0.0  0.0  7087.0  Algork Amblereld   \n",
      "\n",
      "      Transported  group  \n",
      "4326        False   4609  \n",
      "=================================== \n",
      "\n",
      "    PassengerId HomePlanet CryoSleep   Cabin  Destination   Age    VIP  \\\n",
      "680     0710_01       Mars     False  E/42/P  TRAPPIST-1e  23.0  False   \n",
      "\n",
      "     RoomService  FoodCourt  ShoppingMall   Spa  VRDeck        Name  \\\n",
      "680       1695.0        0.0           0.0  31.0   349.0  Nute Coflé   \n",
      "\n",
      "     Transported  group  \n",
      "680        False    710  \n",
      "=================================== \n",
      "\n",
      "    PassengerId HomePlanet CryoSleep   Cabin    Destination   Age    VIP  \\\n",
      "511     0549_01       Mars     False  D/19/S  PSO J318.5-22  32.0  False   \n",
      "512     0549_02       Mars      True  F/98/S    TRAPPIST-1e  43.0  False   \n",
      "\n",
      "     RoomService  FoodCourt  ShoppingMall    Spa  VRDeck          Name  \\\n",
      "511        666.0        0.0           0.0  252.0     1.0  Warets Watte   \n",
      "512          0.0        0.0           0.0    0.0     0.0   Guans Watte   \n",
      "\n",
      "     Transported  group  \n",
      "511         True    549  \n",
      "512         True    549  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "4369     4651_01     Europa     False  C/179/S  TRAPPIST-1e  33.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall    Spa  VRDeck             Name  \\\n",
      "4369          0.0     7039.0           0.0  902.0   405.0  Aludram Dectory   \n",
      "\n",
      "      Transported  group  \n",
      "4369         True   4651  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "6892     7300_01      Earth     False  G/1189/S  TRAPPIST-1e  35.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck          Name  \\\n",
      "6892          0.0        0.0         885.0  0.0     0.0  Joana Emenez   \n",
      "\n",
      "      Transported  group  \n",
      "6892         True   7300  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "7575     8097_01     Europa      True  B/317/S  TRAPPIST-1e  41.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck           Name  \\\n",
      "7575          0.0        0.0           NaN  0.0     0.0  Asch Meroliss   \n",
      "\n",
      "      Transported  group  \n",
      "7575         True   8097  \n",
      "=================================== \n",
      "\n",
      "    PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "975     1034_01      Earth     False  G/155/P  TRAPPIST-1e  31.0  False   \n",
      "\n",
      "     RoomService  FoodCourt  ShoppingMall    Spa  VRDeck           Name  \\\n",
      "975          0.0        0.0           0.0  675.0   213.0  Aliedy Cleman   \n",
      "\n",
      "     Transported  group  \n",
      "975        False   1034  \n",
      "=================================== \n",
      "\n",
      "    PassengerId HomePlanet CryoSleep   Cabin  Destination   Age    VIP  \\\n",
      "902     0972_01      Earth     False  E/54/P  TRAPPIST-1e  21.0  False   \n",
      "\n",
      "     RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
      "902          0.0      314.0         323.0  1026.0     6.0  Glendy Humphreyes   \n",
      "\n",
      "     Transported  group  \n",
      "902        False    972  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "1655     1757_01      Earth     False  F/334/S  TRAPPIST-1e  67.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall   Spa  VRDeck             Name  \\\n",
      "1655         26.0      809.0           0.0  39.0    26.0  Juline Bradamss   \n",
      "\n",
      "      Transported  group  \n",
      "1655        False   1757  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep   Cabin  Destination   Age    VIP  \\\n",
      "7246     7748_01     Europa     False  A/94/S  TRAPPIST-1e  44.0  False   \n",
      "7247     7748_02     Europa     False  A/94/S  TRAPPIST-1e  39.0   True   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck              Name  \\\n",
      "7246         18.0     1451.0           0.0   279.0    64.0  Alamin Obsentork   \n",
      "7247          0.0      395.0           0.0  1012.0  1080.0  Almino Obsentork   \n",
      "\n",
      "      Transported  group  \n",
      "7246        False   7748  \n",
      "7247        False   7748  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "1178     1246_01      Earth     False  G/185/P  TRAPPIST-1e  30.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall    Spa  VRDeck             Name  \\\n",
      "1178          0.0      192.0           0.0  565.0     0.0  Valey Schmondez   \n",
      "\n",
      "      Transported  group  \n",
      "1178        False   1246  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "3780     4036_01      Earth     False  G/662/S  TRAPPIST-1e  22.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall    Spa  VRDeck             Name  \\\n",
      "3780          7.0       61.0         110.0  708.0     0.0  Jodia Evedorsen   \n",
      "\n",
      "      Transported  group  \n",
      "3780        False   4036  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "6150     6489_01      Earth      True  G/1049/P  TRAPPIST-1e  16.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck           Name  \\\n",
      "6150          0.0        0.0           0.0  0.0     0.0  Elsane Balley   \n",
      "\n",
      "      Transported  group  \n",
      "6150        False   6489  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "4947     5274_01      Earth     False  F/1078/P  TRAPPIST-1e  25.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall   Spa  VRDeck            Name  \\\n",
      "4947        662.0       34.0           0.0  79.0   148.0  Weney Hineyley   \n",
      "\n",
      "      Transported  group  \n",
      "4947        False   5274  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep   Cabin  Destination   Age    VIP  \\\n",
      "2468     2646_01     Europa     False  B/80/P  55 Cancri e  33.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall    Spa  VRDeck            Name  \\\n",
      "2468          0.0      286.0        1795.0  712.0  7041.0  Hallah Roureed   \n",
      "\n",
      "      Transported  group  \n",
      "2468        False   2646  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep   Cabin  Destination   Age    VIP  \\\n",
      "2692     2884_01       Mars     False  D/90/P  TRAPPIST-1e  37.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck        Name  \\\n",
      "2692        912.0        0.0           0.0  NaN     0.0  Coats Erle   \n",
      "\n",
      "      Transported  group  \n",
      "2692        False   2884  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "7241     7743_01      Earth     False  G/1244/P  TRAPPIST-1e  63.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall    Spa  VRDeck              Name  \\\n",
      "7241          0.0      243.0           0.0  777.0     0.0  Jilley Alshipson   \n",
      "\n",
      "      Transported  group  \n",
      "7241        False   7743  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "8267     8827_01      Earth     False  F/1820/P  55 Cancri e  37.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall   Spa  VRDeck               Name  \\\n",
      "8267         88.0        0.0         719.0  88.0     0.0  Rickie Mclarseney   \n",
      "\n",
      "      Transported  group  \n",
      "8267         True   8827  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "2754     2960_01      Earth     False  F/562/S  TRAPPIST-1e  54.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck              Name  \\\n",
      "2754          NaN        NaN           0.0  0.0     0.0  Gracia Carterson   \n",
      "\n",
      "      Transported  group  \n",
      "2754        False   2960  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "6510     6868_01      Earth     False  F/1427/P  TRAPPIST-1e  18.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck              Name  \\\n",
      "6510          0.0       18.0          73.0  2134.0    25.0  Jerek Brookenson   \n",
      "\n",
      "      Transported  group  \n",
      "6510        False   6868  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age  VIP  \\\n",
      "1070     1146_01      Earth     False  F/232/P  55 Cancri e  26.0  NaN   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck          Name  \\\n",
      "1070        620.0        0.0         187.0  0.0  1304.0  Dianie Roses   \n",
      "\n",
      "      Transported  group  \n",
      "1070        False   1146  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "7267     7771_01      Earth     False  G/1259/S  TRAPPIST-1e  22.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall   Spa  VRDeck              Name  \\\n",
      "7267          0.0        NaN         692.0  18.0     0.0  Feline Cofferson   \n",
      "\n",
      "      Transported  group  \n",
      "7267         True   7771  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin    Destination   Age    VIP  \\\n",
      "2281     2451_01        NaN      True  G/395/S  PSO J318.5-22  16.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck              Name  \\\n",
      "2281          0.0        0.0           0.0  0.0     0.0  Valey Currisones   \n",
      "\n",
      "      Transported  group  \n",
      "2281        False   2451  \n",
      "=================================== \n",
      "\n",
      "    PassengerId HomePlanet CryoSleep   Cabin  Destination   Age    VIP  \\\n",
      "162     0186_01      Earth     False  G/28/P  TRAPPIST-1e  25.0  False   \n",
      "\n",
      "     RoomService  FoodCourt  ShoppingMall  Spa  VRDeck               Name  \\\n",
      "162         62.0        0.0           0.0  0.0   956.0  Camiet Shephendry   \n",
      "\n",
      "     Transported  group  \n",
      "162        False    186  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin    Destination   Age    VIP  \\\n",
      "1921     2056_01      Earth      True  G/331/P    TRAPPIST-1e  52.0  False   \n",
      "1922     2056_02      Earth     False  F/407/P    TRAPPIST-1e  14.0  False   \n",
      "1923     2056_03      Earth     False  F/407/P    TRAPPIST-1e  68.0  False   \n",
      "1924     2056_04      Earth      True  G/331/P  PSO J318.5-22  18.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
      "1921          0.0        0.0           0.0     0.0     0.0  Marynk Hanghanson   \n",
      "1922         16.0        0.0           0.0   712.0   149.0  Claura Hanghanson   \n",
      "1923          0.0      286.0           0.0  1739.0    13.0   Elany Hanghanson   \n",
      "1924          0.0        0.0           0.0     0.0     0.0  Philia Hanghanson   \n",
      "\n",
      "      Transported  group  \n",
      "1921        False   2056  \n",
      "1922        False   2056  \n",
      "1923        False   2056  \n",
      "1924         True   2056  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "4681     4990_01     Europa      True  B/164/P  TRAPPIST-1e  35.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck             Name  \\\n",
      "4681          0.0        0.0           0.0  0.0     0.0  Nasliak Listive   \n",
      "\n",
      "      Transported  group  \n",
      "4681        False   4990  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "1554     1650_01      Earth      True  G/258/S  55 Cancri e  28.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck         Name  \\\n",
      "1554          0.0        0.0           0.0  0.0     0.0  Walla Hodes   \n",
      "\n",
      "      Transported  group  \n",
      "1554         True   1650  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age   VIP  \\\n",
      "3949     4215_01       Mars     False  D/145/P  TRAPPIST-1e  34.0  True   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck          Name  \\\n",
      "3949        992.0        0.0          14.0  0.0     0.0  Gocock Floaf   \n",
      "\n",
      "      Transported  group  \n",
      "3949        False   4215  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "2684     2875_01     Europa      True  B/107/S  55 Cancri e  55.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck               Name  \\\n",
      "2684          0.0        0.0           0.0  0.0     0.0  Zinonon Frolestty   \n",
      "\n",
      "      Transported  group  \n",
      "2684         True   2875  \n",
      "=================================== \n",
      "\n",
      "    PassengerId HomePlanet CryoSleep   Cabin  Destination   Age    VIP  \\\n",
      "323     0360_01      Earth     False  F/67/S  TRAPPIST-1e  15.0  False   \n",
      "\n",
      "     RoomService  FoodCourt  ShoppingMall  Spa  VRDeck               Name  \\\n",
      "323        358.0      212.0          33.0  0.0   112.0  Elancy Staffersby   \n",
      "\n",
      "     Transported  group  \n",
      "323        False    360  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "2426     2598_01       Mars     False  F/535/P  TRAPPIST-1e  37.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck         Name  \\\n",
      "2426         27.0        0.0        1131.0  0.0  1318.0  Babix Scake   \n",
      "\n",
      "      Transported  group  \n",
      "2426        False   2598  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "2924     3169_01      Earth     False  F/598/S  TRAPPIST-1e  26.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck            Name  \\\n",
      "2924        207.0      350.0          74.0  0.0     3.0  Lyn Leblanchez   \n",
      "\n",
      "      Transported  group  \n",
      "2924         True   3169  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "4987     5323_01       Mars      True  F/1091/P  TRAPPIST-1e  38.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck         Name  \\\n",
      "4987          0.0        0.0           0.0  NaN     0.0  Coakey Cort   \n",
      "\n",
      "      Transported  group  \n",
      "4987         True   5323  \n",
      "=================================== \n",
      "\n",
      "    PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "846     0910_01      Earth      True  G/137/S  TRAPPIST-1e  19.0  False   \n",
      "\n",
      "     RoomService  FoodCourt  ShoppingMall  Spa  VRDeck               Name  \\\n",
      "846          0.0        0.0           0.0  0.0     0.0  Violey Burryerson   \n",
      "\n",
      "     Transported  group  \n",
      "846         True    910  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "7191     7683_01      Earth     False  F/1467/S  TRAPPIST-1e  28.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck             Name  \\\n",
      "7191          0.0      817.0          58.0  0.0    25.0  Katen Avisnydes   \n",
      "\n",
      "      Transported  group  \n",
      "7191         True   7683  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "5202     5546_01     Europa     False  D/176/P  55 Cancri e  28.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck              Name  \\\n",
      "5202        550.0     5203.0           0.0  2244.0  1624.0  Wasatan Brathful   \n",
      "\n",
      "      Transported  group  \n",
      "5202        False   5546  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "5230     5581_01     Europa      True  B/212/S  55 Cancri e  27.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck             Name  \\\n",
      "5230          0.0        0.0           0.0  0.0     NaN  Alpha Frolestty   \n",
      "\n",
      "      Transported  group  \n",
      "5230         True   5581  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "2283     2453_01       Mars      True  E/147/P  TRAPPIST-1e  27.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck       Name  \\\n",
      "2283          0.0        0.0           0.0  0.0     0.0  Ter Teake   \n",
      "\n",
      "      Transported  group  \n",
      "2283         True   2453  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination  Age    VIP  \\\n",
      "2922     3166_01       Mars      True  F/657/P  TRAPPIST-1e  NaN  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck         Name  \\\n",
      "2922          NaN        0.0           0.0  0.0     0.0  Morms Cheen   \n",
      "\n",
      "      Transported  group  \n",
      "2922         True   3166  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin  Destination   Age    VIP  \\\n",
      "4889     5210_01      Earth     False  F/1063/P  TRAPPIST-1e  29.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck Name  Transported  \\\n",
      "4889          0.0       43.0         693.0  0.0     0.0  NaN         True   \n",
      "\n",
      "      group  \n",
      "4889   5210  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "7747     8268_01     Europa      True  B/321/S  TRAPPIST-1e  40.0  False   \n",
      "7748     8268_02     Europa     False  B/321/S  TRAPPIST-1e  19.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall    Spa  VRDeck             Name  \\\n",
      "7747          0.0        0.0           0.0    0.0     0.0  Astrark Impress   \n",
      "7748          0.0        3.0           0.0  918.0  2346.0  Shatrix Impress   \n",
      "\n",
      "      Transported  group  \n",
      "7747         True   8268  \n",
      "7748        False   8268  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age    VIP  \\\n",
      "1994     2133_01       Mars     False   D/74/S  TRAPPIST-1e   NaN  False   \n",
      "1995     2133_02       Mars      True  F/416/S  TRAPPIST-1e  16.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall   Spa  VRDeck         Name  \\\n",
      "1994       1054.0        0.0          43.0  85.0     0.0  Caish Chrie   \n",
      "1995          0.0        0.0           0.0   0.0     0.0   Elfs Chrie   \n",
      "\n",
      "      Transported  group  \n",
      "1994        False   2133  \n",
      "1995         True   2133  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep   Cabin  Destination   Age    VIP  \\\n",
      "1404     1481_01       Mars     False  E/91/P  TRAPPIST-1e  27.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck        Name  \\\n",
      "1404        555.0        0.0         744.0  0.0     0.0  Krus Prist   \n",
      "\n",
      "      Transported  group  \n",
      "1404        False   1481  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination  Age    VIP  \\\n",
      "2482     2662_01       Mars     False  F/551/P  55 Cancri e  NaN  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck      Name  \\\n",
      "2482       1722.0        4.0         937.0  0.0     0.0  Gies Ban   \n",
      "\n",
      "      Transported  group  \n",
      "2482        False   2662  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep    Cabin  Destination   Age  VIP  \\\n",
      "7386     7897_01     Europa      True  C/295/S  55 Cancri e  43.0  NaN   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck                Name  \\\n",
      "7386          0.0        0.0           0.0  0.0     0.0  Altares Ancontaked   \n",
      "\n",
      "      Transported  group  \n",
      "7386         True   7897  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n",
      "6198     6553_01      Earth     False  F/1360/P  PSO J318.5-22  39.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck         Name  \\\n",
      "6198          0.0        0.0           0.0  0.0   744.0  Bel Phillis   \n",
      "\n",
      "      Transported  group  \n",
      "6198        False   6553  \n",
      "=================================== \n",
      "\n",
      "     PassengerId HomePlanet CryoSleep Cabin  Destination   Age    VIP  \\\n",
      "1519     1613_01      Earth     False   NaN  55 Cancri e  16.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall  Spa  VRDeck         Name  \\\n",
      "1519        825.0        9.0           0.0  0.0    42.0  Judya Josey   \n",
      "\n",
      "      Transported  group  \n",
      "1519        False   1613  \n",
      "=================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_ = random.sample(list(df['group'].unique()),100)\n",
    "\n",
    "for group_ in random_ : \n",
    "    print(df[(df['group'] == group_ )])\n",
    "    print(\"===================================\",'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_str(x):\n",
    "    return isinstance(x, str)\n",
    "\n",
    "df['last_name'] = [name_.split(' ')[1] if is_str(name_) else name_ for name_ in df['Name']]\n",
    "\n",
    "df2['last_name'] = [name_.split(' ')[1] if is_str(name_) else name_ for name_ in df2['Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[(df['VIP'] == True )].to_csv(\"C:\\\\Users\\\\김승호\\\\Desktop\\\\데브코스 ai\\\\vip인사람 특징.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 그룹이 같으면 vip도 같아?\n",
    "\n",
    "# for i in df['group'].unique() :\n",
    "#     test2 = df[(df['group'] == i)]\n",
    "#     vip = sum([1 if v  else 0 for v in test2['VIP']])\n",
    "#     if vip :\n",
    "#         print(test2.shape[0])\n",
    "#         print(vip)\n",
    "#         print(\"====\")\n",
    "\n",
    "# ## 결론 : 그룹당 한명씩 vip긴 한데 이게 상관관계가 있다고는 못하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group이 동일한 승객끼리는 \n",
    "# 일단 last name이 거의 동일하고\n",
    "# 출발지는 거의 똑같음\n",
    "# 도착지 다른경우도 있는데 대부분 똑같은듯, \n",
    "# vip랑은 관련없어보임\n",
    "# cabin 도 비슷함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # for group in df['group'].unique() :\n",
    "\n",
    "# group_sim = ['HomePlanet','Cabin','Destination','last_name']\n",
    "\n",
    "# for i in df['group'].unique():\n",
    "#     test1 = df[(df['group'] == i )]\n",
    "#     for col in group_sim :\n",
    "#         if len(test1) >= 1 and test1[col].isnull().sum() >= 1 :\n",
    "#             print(test1)\n",
    "#             print(\"============\")\n",
    "            \n",
    "#         else :\n",
    "#             continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sim = ['HomePlanet','Cabin','Destination','last_name']\n",
    "\n",
    "for col in group_sim:\n",
    "    if col in df.columns:\n",
    "        df[col] = df.groupby('group')[col].transform(lambda x: x.fillna(method='ffill'))\n",
    "        # 같은 그룹이면 모두 윗열 참고해서 채워버리고\n",
    "    df[col] = df[col].transform(lambda x: x.fillna(method='bfill'))\n",
    "    # 그룹이 1개짜리, 또는 가장 윗열이 nan인경우 그냥 아래열 참고해서 채워넣자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sim = ['HomePlanet','Cabin','Destination','last_name']\n",
    "\n",
    "for col in group_sim:\n",
    "    if col in df2.columns:\n",
    "        df2[col] = df2.groupby('group')[col].transform(lambda x: x.fillna(method='ffill'))\n",
    "        # 같은 그룹이면 모두 윗열 참고해서 채워버리고\n",
    "    df2[col] = df2[col].transform(lambda x: x.fillna(method='bfill'))\n",
    "    # 그룹이 1개짜리, 또는 가장 윗열이 nan인경우 그냥 아래열 참고해서 채워넣자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_col = ['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']\n",
    "\n",
    "for col in cost_col :\n",
    "    df[col] = df[col].fillna(0)\n",
    "    df2[col] = df2[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['spend_fillna_0'] = df[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']].sum(axis = 1)\n",
    "df2['spend_fillna_0'] = df2[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VIP_fill_na_False'] = df['VIP'].fillna(False)\n",
    "df2['VIP_fill_na_False'] = df2['VIP'].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, a in enumerate(df['CryoSleep']) :\n",
    "    if np.isnan(a) :\n",
    "        if df['spend_fillna_0'][ind] == 0:\n",
    "            df.loc[ind, 'CryoSleep'] = True\n",
    "        else :\n",
    "            df.loc[ind, 'CryoSleep'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, a in enumerate(df2['CryoSleep']) :\n",
    "    if np.isnan(a) :\n",
    "        if df2['spend_fillna_0'][ind] == 0:\n",
    "            df2.loc[ind, 'CryoSleep'] = True\n",
    "        else :\n",
    "            df2.loc[ind, 'CryoSleep'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId            0\n",
       "HomePlanet             0\n",
       "CryoSleep              0\n",
       "Cabin                  0\n",
       "Destination            0\n",
       "Age                  179\n",
       "VIP                  203\n",
       "RoomService            0\n",
       "FoodCourt              0\n",
       "ShoppingMall           0\n",
       "Spa                    0\n",
       "VRDeck                 0\n",
       "Name                 200\n",
       "Transported            0\n",
       "group                  0\n",
       "last_name              0\n",
       "spend_fillna_0         0\n",
       "VIP_fill_na_False      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck = []\n",
    "num = []\n",
    "side = []\n",
    "\n",
    "for i in df.Cabin :\n",
    "    if str(i).find('/') >= 0 :\n",
    "        try :\n",
    "            d,n,s = str(i).split('/')\n",
    "            deck.append(d)\n",
    "            num.append(int(n))\n",
    "            side.append(s)\n",
    "        except :\n",
    "            print(i)\n",
    "            break\n",
    "    else : \n",
    "        print(i)\n",
    "        deck.append(np.nan)\n",
    "        num.append(np.nan)\n",
    "        side.append(np.nan)\n",
    "\n",
    "df['deck'] = deck\n",
    "df['num'] = num\n",
    "df['side'] = side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck = []\n",
    "num = []\n",
    "side = []\n",
    "\n",
    "\n",
    "for i in df2.Cabin :\n",
    "    if str(i).find('/') >= 0 :\n",
    "        try :\n",
    "            d,n,s = str(i).split('/')\n",
    "            deck.append(d)\n",
    "            num.append(int(n))\n",
    "            side.append(s)\n",
    "        except :\n",
    "            print(i)\n",
    "            break\n",
    "    else : \n",
    "        print(i)\n",
    "        deck.append(np.nan)\n",
    "        num.append(np.nan)\n",
    "        side.append(np.nan)\n",
    "\n",
    "df2['deck'] = deck\n",
    "df2['num'] = num\n",
    "df2['side'] = side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종적으로 버릴 변수 PassengerId, VIP, Name, Cabin\n",
    "\n",
    "col_drop = ['PassengerId', 'VIP', 'Name', 'Cabin']\n",
    "\n",
    "df = df[[col for col in df.columns if col not in col_drop]]\n",
    "df2 = df2[[col for col in df2.columns if col not in col_drop]]\n",
    "\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "df2['Age'].fillna(df['Age'].median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [HomePlanet, CryoSleep, Destination, Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, Transported, group, last_name, spend_fillna_0, VIP_fill_na_False, deck, num, side]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df[df['Age'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8693 entries, 0 to 8692\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   HomePlanet         8693 non-null   object \n",
      " 1   CryoSleep          8693 non-null   object \n",
      " 2   Destination        8693 non-null   object \n",
      " 3   Age                8693 non-null   float64\n",
      " 4   RoomService        8693 non-null   float64\n",
      " 5   FoodCourt          8693 non-null   float64\n",
      " 6   ShoppingMall       8693 non-null   float64\n",
      " 7   Spa                8693 non-null   float64\n",
      " 8   VRDeck             8693 non-null   float64\n",
      " 9   Transported        8693 non-null   bool   \n",
      " 10  group              8693 non-null   int64  \n",
      " 11  last_name          8693 non-null   object \n",
      " 12  spend_fillna_0     8693 non-null   float64\n",
      " 13  VIP_fill_na_False  8693 non-null   bool   \n",
      " 14  deck               8693 non-null   object \n",
      " 15  num                8693 non-null   int64  \n",
      " 16  side               8693 non-null   object \n",
      "dtypes: bool(2), float64(7), int64(2), object(6)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HomePlanet', 'CryoSleep', 'Destination', 'Age', 'RoomService',\n",
       "       'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Transported', 'group',\n",
       "       'last_name', 'spend_fillna_0', 'VIP_fill_na_False', 'deck', 'num',\n",
       "       'side'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HomePlanet', 'CryoSleep', 'Destination', 'Age', 'RoomService',\n",
       "       'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'group', 'last_name',\n",
       "       'spend_fillna_0', 'VIP_fill_na_False', 'deck', 'num', 'side'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = df['Transported']\n",
    "# df = df.drop('Transported', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HomePlanet \n",
      " ['Europa' 'Earth' 'Mars']\n",
      "유니크한 카테고리 수 :  3\n",
      "null 수              :  0 \n",
      "\n",
      "CryoSleep \n",
      " [False True]\n",
      "유니크한 카테고리 수 :  2\n",
      "null 수              :  0 \n",
      "\n",
      "Destination \n",
      " ['TRAPPIST-1e' 'PSO J318.5-22' '55 Cancri e']\n",
      "유니크한 카테고리 수 :  3\n",
      "null 수              :  0 \n",
      "\n",
      "Age \n",
      " [39. 24. 58. 33. 16. 44. 26. 28. 35. 14. 34. 45. 32. 48. 31. 27.  0.  1.\n",
      " 49. 29. 10.  7. 21. 62. 15. 43. 47.  2. 20. 23. 30. 17. 55.  4. 19. 56.\n",
      " 25. 38. 36. 22. 18. 42. 37. 13.  8. 40.  3. 54.  9.  6. 64. 67. 61. 50.\n",
      " 41. 57. 11. 52. 51. 46. 60. 63. 59.  5. 79. 68. 74. 12. 53. 65. 71. 75.\n",
      " 70. 76. 78. 73. 66. 69. 72. 77.]\n",
      "유니크한 카테고리 수 :  80\n",
      "null 수              :  0 \n",
      "\n",
      "RoomService \n",
      " [   0.  109.   43. ... 1569. 8586.  745.]\n",
      "유니크한 카테고리 수 :  1273\n",
      "null 수              :  0 \n",
      "\n",
      "FoodCourt \n",
      " [   0.    9. 3576. ... 3208. 6819. 4688.]\n",
      "유니크한 카테고리 수 :  1507\n",
      "null 수              :  0 \n",
      "\n",
      "ShoppingMall \n",
      " [   0.   25.  371. ... 1085.  510. 1872.]\n",
      "유니크한 카테고리 수 :  1115\n",
      "null 수              :  0 \n",
      "\n",
      "Spa \n",
      " [   0.  549. 6715. ... 2868. 1107. 1643.]\n",
      "유니크한 카테고리 수 :  1327\n",
      "null 수              :  0 \n",
      "\n",
      "VRDeck \n",
      " [   0.   44.   49. ... 1164.  971. 3235.]\n",
      "유니크한 카테고리 수 :  1306\n",
      "null 수              :  0 \n",
      "\n",
      "Transported \n",
      " [False  True]\n",
      "유니크한 카테고리 수 :  2\n",
      "null 수              :  0 \n",
      "\n",
      "group \n",
      " [   1    2    3 ... 9278 9279 9280]\n",
      "유니크한 카테고리 수 :  6217\n",
      "null 수              :  0 \n",
      "\n",
      "last_name \n",
      " ['Ofracculy' 'Vines' 'Susent' ... 'Fort' 'Workmanson' 'Sionerorly']\n",
      "유니크한 카테고리 수 :  2217\n",
      "null 수              :  0 \n",
      "\n",
      "spend_fillna_0 \n",
      " [    0.   736. 10383. ...  8803.  8536.  4637.]\n",
      "유니크한 카테고리 수 :  2336\n",
      "null 수              :  0 \n",
      "\n",
      "VIP_fill_na_False \n",
      " [False  True]\n",
      "유니크한 카테고리 수 :  2\n",
      "null 수              :  0 \n",
      "\n",
      "deck \n",
      " ['B' 'F' 'A' 'G' 'E' 'D' 'C' 'T']\n",
      "유니크한 카테고리 수 :  8\n",
      "null 수              :  0 \n",
      "\n",
      "num \n",
      " [   0    1    2 ... 1892 1893 1894]\n",
      "유니크한 카테고리 수 :  1817\n",
      "null 수              :  0 \n",
      "\n",
      "side \n",
      " ['P' 'S']\n",
      "유니크한 카테고리 수 :  2\n",
      "null 수              :  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns :\n",
    "    print(col,\"\\n\", df[col].unique())\n",
    "    print('유니크한 카테고리 수 : ',len(df[col].unique()))\n",
    "    print('null 수              : ', df[col].isna().sum(),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HomePlanet', 'CryoSleep', 'Destination', 'Age', 'RoomService',\n",
       "       'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Transported', 'group',\n",
       "       'last_name', 'spend_fillna_0', 'VIP_fill_na_False', 'deck', 'num',\n",
       "       'side'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HomePlanet \n",
      " ['Earth' 'Europa' 'Mars']\n",
      "유니크한 카테고리 수 :  3\n",
      "null 수              :  0 \n",
      "\n",
      "CryoSleep \n",
      " [True False]\n",
      "유니크한 카테고리 수 :  2\n",
      "null 수              :  0 \n",
      "\n",
      "Destination \n",
      " ['TRAPPIST-1e' '55 Cancri e' 'PSO J318.5-22']\n",
      "유니크한 카테고리 수 :  3\n",
      "null 수              :  0 \n",
      "\n",
      "Age \n",
      " [27. 19. 31. 38. 20. 21. 23. 24. 45. 44. 46. 29. 40. 30. 14. 66. 36. 18.\n",
      " 26. 48.  6. 33.  1. 17. 34.  5. 22. 16.  7.  2.  0. 56. 39. 35.  8.  3.\n",
      "  9. 25. 42. 50. 41. 32. 49. 55. 60. 47. 58. 28. 12. 52. 15. 43. 54. 70.\n",
      " 37. 59. 61. 63. 53.  4. 10. 65. 13. 51. 79. 74. 67. 69. 57. 72. 75. 11.\n",
      " 73. 71. 64. 62. 78. 77. 68.]\n",
      "유니크한 카테고리 수 :  79\n",
      "null 수              :  0 \n",
      "\n",
      "RoomService \n",
      " [0.0000e+00 1.0000e+01 3.3900e+02 9.3200e+02 2.0000e+00 2.6000e+01\n",
      " 3.9000e+01 1.7840e+03 2.9300e+02 5.9800e+02 1.4000e+01 4.8300e+02\n",
      " 6.6000e+01 1.0000e+00 1.0370e+03 3.9660e+03 4.6000e+01 4.1300e+02\n",
      " 3.7800e+02 3.2000e+01 8.3200e+02 3.6200e+02 7.5600e+02 1.9700e+02\n",
      " 6.0000e+00 2.6100e+02 1.6690e+03 4.4000e+01 3.3000e+01 7.1700e+02\n",
      " 4.7620e+03 7.4100e+02 3.0000e+00 7.8600e+02 2.5560e+03 1.8600e+02\n",
      " 1.3400e+03 6.9600e+02 1.5420e+03 1.0800e+02 3.1860e+03 4.1000e+01\n",
      " 1.0540e+03 6.0000e+01 3.0100e+02 1.2300e+02 9.8000e+01 2.1200e+02\n",
      " 7.3000e+01 3.7600e+02 5.0000e+00 1.0310e+03 3.5600e+02 6.3800e+02\n",
      " 1.3950e+03 2.3000e+01 8.0000e+00 1.3000e+02 2.7300e+02 2.8800e+02\n",
      " 7.3100e+02 1.6780e+03 4.9080e+03 8.7000e+01 4.0000e+00 7.0800e+02\n",
      " 6.4200e+02 6.5800e+02 2.3900e+02 1.8900e+02 1.2900e+02 1.2800e+02\n",
      " 1.0300e+03 5.3000e+01 2.4600e+02 6.4800e+02 1.7290e+03 1.5000e+01\n",
      " 4.1050e+03 4.4100e+02 7.6000e+02 3.4240e+03 4.9220e+03 1.0630e+03\n",
      " 2.2210e+03 2.8600e+02 2.5430e+03 1.6440e+03 7.5500e+02 4.9900e+02\n",
      " 3.0000e+01 7.2000e+01 7.4800e+02 1.0450e+03 6.3200e+02 5.9600e+02\n",
      " 7.1000e+02 1.2790e+03 1.3000e+01 9.0000e+00 1.3440e+03 4.6600e+02\n",
      " 1.6610e+03 1.7000e+01 5.5900e+02 7.2000e+02 2.6600e+02 5.3000e+02\n",
      " 1.7050e+03 4.7200e+02 1.7800e+02 1.4900e+02 3.6820e+03 8.3000e+02\n",
      " 2.2270e+03 1.8200e+02 2.5770e+03 6.9800e+02 8.6900e+02 2.0240e+03\n",
      " 3.1600e+03 7.8000e+02 2.9900e+02 1.3990e+03 6.1000e+01 5.8900e+02\n",
      " 1.2460e+03 1.7600e+02 1.7930e+03 4.1100e+02 2.2000e+01 9.0100e+02\n",
      " 5.9000e+02 1.4100e+02 6.4000e+02 6.6600e+02 2.1640e+03 8.2600e+02\n",
      " 2.4000e+02 5.5300e+02 5.4000e+01 2.8900e+02 1.5000e+02 6.9900e+02\n",
      " 1.5400e+02 2.2330e+03 1.2160e+03 1.1990e+03 1.3200e+02 8.4700e+02\n",
      " 1.4770e+03 4.3000e+01 5.5500e+02 1.3710e+03 7.7500e+02 2.2000e+02\n",
      " 5.2700e+02 1.8000e+01 6.7800e+02 1.7100e+03 1.7890e+03 7.7000e+02\n",
      " 1.4590e+03 4.8000e+01 5.3900e+02 5.0100e+02 1.3630e+03 6.7300e+02\n",
      " 9.8700e+02 1.5300e+02 1.2400e+02 9.7000e+01 9.9000e+01 2.2460e+03\n",
      " 1.0060e+03 2.2010e+03 3.4000e+01 1.2420e+03 8.9000e+01 8.0700e+02\n",
      " 9.3600e+02 6.3000e+01 1.0180e+03 9.0000e+01 1.3300e+02 2.0120e+03\n",
      " 4.5000e+01 5.8000e+01 3.0000e+02 4.7300e+02 2.5650e+03 2.0000e+01\n",
      " 1.0420e+03 1.8300e+02 7.9900e+02 6.7900e+02 5.1500e+02 5.8300e+02\n",
      " 3.1600e+02 1.6000e+01 6.2800e+02 6.7400e+02 1.5560e+03 3.3030e+03\n",
      " 1.6920e+03 1.1060e+03 8.6000e+01 1.0930e+03 2.2400e+02 6.3300e+02\n",
      " 1.7200e+02 2.0100e+02 1.5070e+03 1.0220e+03 1.1000e+02 9.8000e+02\n",
      " 3.6700e+02 6.8400e+02 2.4700e+02 6.9000e+02 5.3200e+02 4.7700e+02\n",
      " 1.2960e+03 1.1500e+02 1.5700e+02 1.0100e+02 2.3400e+02 3.7900e+02\n",
      " 1.2820e+03 1.4020e+03 6.4300e+02 6.2300e+02 1.1820e+03 6.0300e+02\n",
      " 5.8200e+02 1.1560e+03 5.5700e+02 1.2860e+03 9.0800e+02 1.9000e+01\n",
      " 3.5290e+03 3.1100e+02 8.0900e+02 1.6510e+03 4.0700e+02 5.9900e+02\n",
      " 1.2850e+03 2.1000e+01 2.7000e+01 1.6240e+03 1.8440e+03 1.4300e+02\n",
      " 1.0200e+02 8.5000e+01 4.0000e+01 2.6200e+02 8.7800e+02 6.6400e+02\n",
      " 2.0800e+02 6.3700e+02 9.0400e+02 3.0800e+02 1.2260e+03 7.6700e+02\n",
      " 2.5800e+02 4.2500e+02 5.1000e+01 3.9900e+02 3.4400e+02 2.1000e+02\n",
      " 1.7750e+03 1.5200e+02 1.1000e+01 6.9300e+02 1.4230e+03 1.6700e+02\n",
      " 1.4200e+02 1.2000e+01 7.6000e+01 5.6700e+02 1.2590e+03 2.0400e+02\n",
      " 8.3900e+02 3.4000e+02 8.7700e+02 8.1700e+02 8.5300e+02 1.8720e+03\n",
      " 9.9500e+02 1.0300e+02 9.4000e+02 6.5100e+02 1.1050e+03 1.4480e+03\n",
      " 8.8400e+02 3.4300e+02 1.6900e+02 8.7400e+02 8.5800e+02 5.7400e+02\n",
      " 6.2000e+01 6.1100e+02 4.7000e+02 7.8500e+02 2.5000e+01 5.4900e+02\n",
      " 2.1380e+03 6.6700e+02 2.7800e+02 4.6000e+02 5.7200e+02 6.1700e+02\n",
      " 1.9380e+03 6.5000e+01 5.9500e+02 1.2700e+02 5.9000e+01 3.8100e+02\n",
      " 2.8300e+02 8.2400e+02 8.1000e+01 2.4690e+03 6.4400e+02 9.3800e+02\n",
      " 1.1567e+04 4.1900e+02 7.3300e+02 1.2360e+03 1.0000e+02 2.7300e+03\n",
      " 8.5500e+02 6.7100e+02 1.4160e+03 2.1940e+03 7.9800e+02 4.5400e+02\n",
      " 5.4500e+02 9.1500e+02 8.4200e+02 2.0300e+02 1.4400e+02 1.2080e+03\n",
      " 2.3660e+03 9.1000e+01 2.4360e+03 8.9000e+02 2.5210e+03 7.4300e+02\n",
      " 1.7220e+03 6.4600e+02 3.6800e+02 7.5000e+01 5.4600e+02 7.8300e+02\n",
      " 1.2140e+03 1.2280e+03 7.6800e+02 1.7400e+02 1.7260e+03 5.6000e+01\n",
      " 8.8500e+02 1.0700e+02 1.8580e+03 9.5100e+02 3.3370e+03 2.4000e+01\n",
      " 2.9800e+02 1.2600e+02 8.5400e+02 9.6600e+02 1.7680e+03 1.9300e+02\n",
      " 1.6800e+02 3.2000e+02 2.5450e+03 1.4540e+03 1.6200e+02 6.4380e+03\n",
      " 2.6470e+03 1.3380e+03 1.6410e+03 6.4000e+01 1.1660e+03 1.6390e+03\n",
      " 1.5450e+03 4.9300e+02 1.6930e+03 1.1100e+02 3.8590e+03 1.2600e+03\n",
      " 3.3000e+02 4.4600e+02 1.1270e+03 1.4810e+03 1.6960e+03 1.9540e+03\n",
      " 4.7000e+01 9.3000e+02 2.0900e+02 1.2530e+03 3.7000e+01 3.3700e+02\n",
      " 1.3600e+02 1.0230e+03 2.8000e+01 6.7000e+01 5.5000e+01 4.2000e+01\n",
      " 4.9600e+02 1.1940e+03 8.2000e+01 9.7700e+02 3.6350e+03 9.1400e+02\n",
      " 7.9300e+02 1.0430e+03 2.7000e+02 2.4400e+02 1.8530e+03 1.1140e+03\n",
      " 5.0400e+02 1.6040e+03 7.0000e+00 1.0140e+03 8.8300e+02 1.8400e+02\n",
      " 4.3300e+02 8.4000e+01 1.1100e+03 1.7570e+03 2.4390e+03 2.7510e+03\n",
      " 2.8100e+02 5.2900e+02 3.8800e+02 3.5500e+02 8.0100e+02 5.9000e+03\n",
      " 4.3800e+02 4.0690e+03 4.4300e+02 1.4790e+03 6.2000e+02 2.4660e+03\n",
      " 1.3800e+03 3.2100e+02 1.3400e+02 5.7800e+02 5.8620e+03 1.0390e+03\n",
      " 7.1500e+02 6.1400e+02 6.1000e+02 2.9200e+02 2.6800e+02 1.8000e+02\n",
      " 2.9000e+01 1.2470e+03 6.3400e+02 3.1000e+02 1.0740e+03 3.2400e+02\n",
      " 1.0410e+03 3.5100e+02 1.3700e+02 1.2510e+03 1.4750e+03 1.0860e+03\n",
      " 1.5600e+02 1.7640e+03 6.8800e+02 4.9800e+02 4.3000e+02 5.2500e+02\n",
      " 5.1700e+02 7.3600e+02 1.1280e+03 8.4600e+02 9.7500e+02 6.2700e+02\n",
      " 6.9000e+01 5.6600e+02 3.4900e+02 4.3200e+02 1.0900e+02 1.3200e+03\n",
      " 1.4000e+02 3.9500e+02 8.0800e+02 9.5300e+02 5.1800e+02 1.2200e+02\n",
      " 5.7000e+01 1.2940e+03 1.3940e+03 1.0400e+02 9.8100e+02 5.3700e+02\n",
      " 1.2180e+03 3.6750e+03 2.1950e+03 5.3330e+03 4.1600e+02 8.2700e+02\n",
      " 7.0400e+02 1.7340e+03 1.0710e+03 3.9000e+02 7.9400e+02 5.6800e+02\n",
      " 1.6500e+02 7.1000e+01 1.3650e+03 2.0700e+02 3.3410e+03 7.0000e+01\n",
      " 1.5380e+03 8.0000e+01 1.2990e+03 1.5960e+03 3.4500e+02 1.1170e+03\n",
      " 1.1040e+03 7.8000e+01 2.1300e+02 4.6500e+02 1.3370e+03 6.6300e+02\n",
      " 3.8000e+01 1.6270e+03 1.1710e+03 2.0190e+03 1.1080e+03 5.4200e+02\n",
      " 1.3300e+03 1.4490e+03 4.5800e+02 7.4200e+02 1.3470e+03 2.3100e+02\n",
      " 1.7100e+02 4.1950e+03 6.8000e+01 1.6730e+03 1.4310e+03 8.9300e+02\n",
      " 9.0500e+02 1.0820e+03 4.7810e+03 6.4100e+02 1.1810e+03 2.3350e+03\n",
      " 7.6600e+02 7.4400e+02 9.4000e+01 3.7200e+02 4.4400e+02 4.9000e+01\n",
      " 1.3010e+03 1.4000e+03 4.7800e+02 8.6700e+02 1.6010e+03 5.8800e+02\n",
      " 1.6600e+02 2.2100e+02 2.2870e+03 1.6300e+02 1.0040e+03 1.2370e+03\n",
      " 2.9600e+02 2.0830e+03 3.9400e+02 7.3000e+02 2.1090e+03 7.9000e+01\n",
      " 1.2210e+03 8.3800e+02 7.8700e+02 9.2100e+02 1.1300e+02 1.8640e+03\n",
      " 5.8400e+02 1.1630e+03 3.5130e+03 1.7000e+02 6.8600e+02 4.6700e+02\n",
      " 1.7230e+03 6.0500e+02 3.6900e+02 8.8900e+02 6.7600e+02 1.6640e+03\n",
      " 6.4500e+02 2.7800e+03 9.3400e+02 2.4900e+02 7.4070e+03 1.1020e+03\n",
      " 1.1390e+03 1.5170e+03 3.8200e+02 2.9100e+02 3.1000e+01 1.0600e+02\n",
      " 7.7000e+01 3.6000e+01 1.0490e+03 1.3430e+03 7.1100e+02 8.7000e+02\n",
      " 5.0000e+02 5.6000e+02 9.1100e+02 1.4620e+03 3.5040e+03 1.0090e+03\n",
      " 2.0710e+03 1.2430e+03 2.5700e+02 1.8390e+03 1.2490e+03 7.1800e+02\n",
      " 2.2800e+02 2.6420e+03 1.4550e+03 1.1900e+02 7.0500e+02 1.4630e+03\n",
      " 1.0960e+03 9.4600e+02 1.1780e+03 9.2800e+02 2.5200e+02 7.0100e+02\n",
      " 1.1620e+03 2.4550e+03 8.8100e+02 8.6400e+02 1.2100e+02 1.8500e+02\n",
      " 4.8400e+02 1.7300e+02 1.5480e+03 4.8700e+02 1.4780e+03 5.0700e+02\n",
      " 1.1770e+03 2.3790e+03 4.5100e+02 9.4900e+02 1.1550e+03 1.2730e+03\n",
      " 4.5500e+02 8.8000e+01 9.8800e+02 5.1200e+02 3.6500e+02 6.9500e+02\n",
      " 6.3100e+02 2.6760e+03 8.0400e+02 4.2070e+03 3.3300e+02 5.1000e+03\n",
      " 4.4900e+02 7.0300e+02 2.5960e+03 1.2650e+03 2.5000e+02 2.5380e+03\n",
      " 7.5700e+02 1.7520e+03 1.8940e+03 8.8800e+02 2.4380e+03 1.7180e+03\n",
      " 1.3100e+02 5.0000e+01 4.0900e+02 8.4400e+02 8.7900e+02 1.2980e+03\n",
      " 1.2190e+03 4.1000e+02 1.3500e+03 5.0900e+02 1.1400e+02 1.4150e+03\n",
      " 1.3500e+02 1.2300e+03 9.7600e+02 7.5800e+02 2.4500e+02 9.3300e+02\n",
      " 2.2900e+02 5.1400e+02 9.6000e+01 1.0950e+03 1.0610e+03 6.1200e+02\n",
      " 3.5000e+02 1.1890e+03 1.1970e+03 8.4800e+02 3.3400e+02 8.3700e+02\n",
      " 3.6600e+02 7.9100e+02 1.5510e+03 1.0870e+03 1.2890e+03 1.9000e+02\n",
      " 6.0600e+02 3.3100e+02 4.8200e+02 1.4870e+03 1.1180e+03 1.1410e+03\n",
      " 1.3820e+03 2.0440e+03 1.6600e+03 1.1700e+02 1.6260e+03 2.7580e+03\n",
      " 4.1400e+02 1.9500e+02 8.2100e+02 4.8900e+02 1.1730e+03 2.9400e+02\n",
      " 5.0200e+02 1.3140e+03 6.9400e+02 1.8570e+03 3.6570e+03 1.9050e+03\n",
      " 1.6100e+02 6.7200e+02 9.1800e+02 1.3240e+03 2.2500e+02 7.9600e+02\n",
      " 8.8600e+02 4.1500e+02 1.0500e+03 2.3330e+03 7.7700e+02 2.8000e+02\n",
      " 6.5900e+02 2.1100e+02 6.8100e+02 1.0440e+03 2.4340e+03 3.8600e+02\n",
      " 1.9490e+03 2.4590e+03 1.0250e+03 9.0600e+02 1.2540e+03 9.9800e+02\n",
      " 2.3640e+03 8.4500e+02 3.4710e+03 4.7100e+02 7.4000e+01 9.5600e+02\n",
      " 5.2000e+01 1.0010e+03 2.2300e+02 6.0100e+02 1.5240e+03 3.9560e+03\n",
      " 3.5000e+01 2.1460e+03 2.5720e+03 2.2490e+03 1.4850e+03 8.2800e+02\n",
      " 8.8000e+02 3.5900e+02 2.2760e+03 2.5850e+03 4.2100e+02 2.5600e+02\n",
      " 4.7600e+02 1.2560e+03 2.3600e+02 3.3200e+02 1.0080e+03 4.5300e+02\n",
      " 2.7890e+03 6.0400e+02 1.5390e+03 2.5900e+02 3.5950e+03 8.1600e+02\n",
      " 1.9530e+03 9.4500e+02 2.0540e+03 1.6940e+03 2.7900e+02 2.3040e+03\n",
      " 4.9100e+02 5.4100e+02 1.2330e+03 1.4500e+02 1.5810e+03 1.1500e+03\n",
      " 9.9300e+02 1.6460e+03 2.6900e+02 1.1760e+03 3.2200e+02 6.3900e+02\n",
      " 7.7300e+02 5.4540e+03 2.3510e+03 6.6900e+02 2.4160e+03 2.8550e+03\n",
      " 6.8500e+02 3.8700e+02 1.0160e+03 9.3000e+01 7.8400e+02 1.5760e+03\n",
      " 3.2300e+02 1.7700e+02 1.2780e+03 3.8300e+02 9.9400e+02 1.7900e+02\n",
      " 1.0330e+03 5.4700e+02 2.8350e+03 5.2300e+02 6.5200e+02 8.8200e+02\n",
      " 1.2720e+03 1.4700e+02]\n",
      "유니크한 카테고리 수 :  842\n",
      "null 수              :  0 \n",
      "\n",
      "FoodCourt \n",
      " [0.0000e+00 9.0000e+00 6.6520e+03 1.6150e+03 6.3900e+02 3.0000e+00\n",
      " 7.4000e+01 1.5610e+03 1.2000e+01 7.7080e+03 1.9250e+03 8.7800e+02\n",
      " 6.9770e+03 6.4520e+03 8.0000e+00 3.4950e+03 6.1900e+02 8.7000e+02\n",
      " 2.0000e+01 1.7360e+03 1.7000e+01 2.2000e+01 8.6900e+02 1.0000e+00\n",
      " 2.8720e+03 3.1500e+02 5.1400e+02 4.0200e+02 8.8530e+03 4.4000e+01\n",
      " 2.6000e+01 4.6300e+02 1.1000e+01 7.7200e+02 1.5000e+01 1.1600e+02\n",
      " 2.7500e+02 2.0000e+00 5.1600e+02 3.8330e+03 2.5273e+04 1.4420e+03\n",
      " 3.3400e+02 8.6000e+01 1.0000e+01 6.7800e+02 6.2700e+02 1.1549e+04\n",
      " 3.4000e+01 7.5500e+02 1.2520e+03 6.9000e+01 1.0300e+03 1.4680e+03\n",
      " 6.7000e+01 1.7040e+03 7.2200e+02 5.3800e+02 2.5100e+02 1.1920e+03\n",
      " 1.8300e+02 7.5000e+01 2.1160e+03 6.0000e+00 1.3470e+03 2.5220e+03\n",
      " 1.4700e+02 1.3780e+03 3.4500e+02 2.0810e+03 6.4420e+03 7.5200e+02\n",
      " 5.4700e+02 1.3300e+02 9.3520e+03 8.0400e+02 3.7900e+02 1.0320e+03\n",
      " 4.9200e+02 2.4000e+01 4.2800e+02 4.3000e+02 4.6000e+01 1.5280e+03\n",
      " 1.0480e+03 5.7180e+03 7.7000e+01 4.0000e+00 9.0000e+01 8.7500e+02\n",
      " 1.4150e+03 6.5000e+01 1.5020e+03 4.3700e+02 2.3520e+03 4.3000e+01\n",
      " 7.2500e+02 4.8000e+02 9.8000e+01 8.8500e+02 1.6300e+02 1.5300e+02\n",
      " 1.0290e+03 2.4900e+03 4.7330e+03 1.4000e+01 3.4100e+03 1.2880e+03\n",
      " 1.8570e+03 3.3260e+03 1.6410e+03 3.7200e+02 3.6360e+03 3.2400e+02\n",
      " 2.6730e+03 1.4800e+02 6.9900e+02 1.7100e+02 2.9350e+03 4.9580e+03\n",
      " 5.4000e+01 1.1240e+03 1.2770e+03 8.1790e+03 6.9150e+03 4.0500e+02\n",
      " 6.9500e+02 1.5400e+02 6.0000e+01 1.5500e+02 7.1000e+01 1.7500e+02\n",
      " 3.8200e+02 3.6300e+02 9.0800e+02 5.0000e+00 1.6100e+02 2.9830e+03\n",
      " 4.5800e+02 1.2860e+03 1.1100e+02 4.7000e+01 9.2000e+01 7.8400e+02\n",
      " 4.9000e+01 1.8800e+02 1.9200e+02 7.6000e+03 7.0800e+02 2.7000e+01\n",
      " 5.7500e+02 2.8300e+02 2.0360e+03 3.1400e+02 2.2500e+02 1.6000e+01\n",
      " 1.4470e+03 3.6060e+03 1.0390e+03 7.3120e+03 1.5370e+03 1.0900e+02\n",
      " 6.5230e+03 1.7200e+02 2.2350e+03 1.5600e+02 5.5000e+01 4.0990e+03\n",
      " 3.5330e+03 2.6500e+03 4.9800e+02 5.8300e+02 1.0100e+02 6.1000e+02\n",
      " 2.9700e+02 2.8700e+02 2.5900e+02 6.8600e+03 5.3000e+01 3.2800e+02\n",
      " 1.8820e+03 6.5300e+02 8.0600e+02 1.3900e+02 3.8000e+01 3.0600e+02\n",
      " 4.7400e+02 9.7700e+02 4.1920e+03 4.6100e+02 1.6800e+02 5.5600e+02\n",
      " 6.8100e+02 9.0000e+02 1.8220e+03 1.4500e+02 2.7990e+03 8.3200e+02\n",
      " 1.5890e+03 5.5000e+02 1.0500e+02 2.6000e+02 1.7400e+02 5.7100e+02\n",
      " 3.6000e+01 6.2800e+02 3.5000e+01 3.6910e+03 1.5980e+03 3.0000e+01\n",
      " 1.7000e+02 3.6370e+03 4.0000e+01 2.1200e+02 6.3000e+01 3.5770e+03\n",
      " 4.2200e+02 5.1130e+03 2.2090e+03 1.0711e+04 6.9100e+02 8.6400e+02\n",
      " 6.1580e+03 1.2760e+03 1.1050e+03 2.9020e+03 2.1600e+02 3.0850e+03\n",
      " 6.2850e+03 6.7300e+02 1.1410e+03 6.7100e+02 8.1200e+02 6.1300e+02\n",
      " 5.9200e+02 1.3700e+02 8.7000e+01 2.2000e+02 1.9800e+03 4.6700e+02\n",
      " 3.1900e+02 2.0700e+02 1.0800e+03 1.6960e+03 4.2050e+03 8.9100e+02\n",
      " 1.0600e+02 6.0700e+02 2.5880e+03 1.9100e+02 1.6560e+03 5.2000e+01\n",
      " 1.4270e+03 2.1300e+02 9.0100e+02 1.1700e+02 8.2400e+02 1.2600e+02\n",
      " 2.0520e+03 3.0500e+02 1.2170e+03 7.8000e+01 6.8800e+02 4.1100e+02\n",
      " 3.6700e+02 1.9400e+02 4.8290e+03 2.8520e+03 5.9500e+03 5.9310e+03\n",
      " 1.9740e+03 5.7000e+01 9.3900e+02 5.7220e+03 1.3110e+03 5.3000e+02\n",
      " 6.1800e+02 4.6180e+03 4.4500e+02 7.7000e+02 3.9900e+02 3.1000e+01\n",
      " 5.2200e+02 6.7400e+02 5.4500e+02 2.5760e+03 4.6500e+02 5.5500e+02\n",
      " 5.6720e+03 1.8000e+01 1.6060e+03 2.3330e+03 3.2000e+01 2.1000e+02\n",
      " 2.5000e+01 2.3220e+03 1.4600e+02 1.4200e+02 1.8900e+02 5.6190e+03\n",
      " 3.5700e+02 2.1000e+01 3.3600e+02 3.5610e+03 1.3000e+01 1.7300e+02\n",
      " 2.6340e+03 7.9900e+02 1.3070e+03 1.3800e+02 1.8200e+02 2.4770e+03\n",
      " 3.0300e+03 4.7500e+02 1.1984e+04 1.6120e+03 6.2900e+02 5.9000e+01\n",
      " 1.2220e+03 9.5600e+02 1.0100e+03 1.1850e+03 6.3600e+02 5.2400e+02\n",
      " 2.4600e+02 1.3400e+02 1.9600e+02 7.7400e+02 3.7300e+02 3.8210e+03\n",
      " 2.6300e+02 3.9700e+02 1.0754e+04 2.0740e+03 2.4500e+02 1.2960e+03\n",
      " 1.0340e+03 2.9500e+02 7.6500e+02 1.3000e+02 1.0200e+02 4.4200e+02\n",
      " 1.0400e+02 9.6620e+03 6.9700e+02 9.7240e+03 4.3130e+03 1.3520e+03\n",
      " 2.9000e+02 5.6200e+02 7.2000e+01 3.6730e+03 9.0600e+02 2.0200e+02\n",
      " 8.2200e+02 7.0000e+00 8.8000e+01 3.8000e+02 6.3000e+02 9.3400e+02\n",
      " 6.8000e+02 3.9500e+02 2.0370e+03 8.4400e+02 2.0610e+03 4.7380e+03\n",
      " 2.2030e+03 3.5100e+02 4.7600e+02 5.7300e+02 9.9800e+02 2.9950e+03\n",
      " 4.8800e+02 3.3000e+01 4.8270e+03 7.6800e+02 8.0700e+02 9.3000e+01\n",
      " 5.9400e+02 9.0200e+02 4.7010e+03 8.4000e+01 2.6100e+02 5.6600e+02\n",
      " 3.1500e+03 4.3980e+03 1.1293e+04 9.8100e+02 8.8000e+02 8.7400e+02\n",
      " 6.2900e+03 1.5200e+02 3.5400e+02 4.2000e+01 2.4550e+03 8.4000e+02\n",
      " 4.8500e+02 7.9800e+03 2.6960e+03 8.4200e+02 1.6240e+03 2.0809e+04\n",
      " 6.5640e+03 8.1800e+02 7.5080e+03 5.8800e+02 9.1300e+02 4.2000e+02\n",
      " 4.2300e+03 5.2900e+02 2.9000e+01 1.9770e+03 1.0010e+03 5.2600e+02\n",
      " 3.7480e+03 9.3800e+02 7.4000e+02 8.1000e+02 5.9900e+02 3.2850e+03\n",
      " 1.1143e+04 3.3760e+03 5.3100e+02 2.3840e+03 5.0300e+02 2.4040e+03\n",
      " 1.4000e+02 3.3540e+03 8.5600e+02 4.1400e+02 2.2270e+03 4.2230e+03\n",
      " 2.5600e+02 7.7480e+03 3.0900e+02 1.8790e+03 7.5950e+03 2.8020e+03\n",
      " 3.0000e+02 5.1380e+03 1.4120e+03 9.5960e+03 7.6600e+02 5.1720e+03\n",
      " 8.5300e+02 5.6800e+02 1.4760e+03 1.4400e+03 3.3060e+03 1.0590e+03\n",
      " 2.9400e+02 5.3200e+02 6.7330e+03 7.3400e+02 2.4000e+02 6.4800e+02\n",
      " 2.1500e+02 6.6690e+03 6.1200e+02 1.5360e+03 2.5830e+03 1.2200e+02\n",
      " 1.2490e+03 2.2300e+02 1.9000e+02 1.2500e+02 7.2710e+03 2.4000e+03\n",
      " 2.2340e+03 7.7800e+02 2.2600e+02 8.1700e+02 7.9600e+02 2.3000e+02\n",
      " 2.8800e+02 6.3200e+02 3.2270e+03 7.0000e+03 2.0229e+04 5.9030e+03\n",
      " 6.9010e+03 8.3000e+02 2.2880e+03 1.6954e+04 4.7340e+03 3.7500e+02\n",
      " 7.4900e+02 1.6400e+02 7.0100e+02 6.7700e+02 8.7600e+02 6.3150e+03\n",
      " 8.9600e+02 9.0580e+03 4.3600e+02 1.0530e+03 7.1300e+02 2.3700e+02\n",
      " 1.0163e+04 4.6600e+02 6.2200e+02 9.1590e+03 7.1200e+02 7.3700e+02\n",
      " 7.6200e+02 1.2550e+03 4.3900e+02 8.2000e+01 7.1700e+02 2.1100e+02\n",
      " 8.0200e+02 7.5960e+03 1.0840e+03 7.7500e+02 4.1000e+01 1.4860e+03\n",
      " 1.6071e+04 2.0650e+03 7.0260e+03 6.0000e+02 9.2600e+02 3.2190e+03\n",
      " 3.5750e+03 1.2600e+03 6.6700e+02 2.8170e+03 3.2160e+03 1.1963e+04\n",
      " 1.6280e+03 1.7700e+02 1.2700e+02 2.3600e+02 2.5150e+03 1.0200e+03\n",
      " 2.5060e+03 6.6020e+03 2.6990e+03 8.3500e+02 2.0500e+02 9.2400e+02\n",
      " 1.3760e+03 9.1900e+02 4.4300e+02 2.3450e+03 5.7900e+02 1.0170e+03\n",
      " 6.6070e+03 1.3290e+03 2.3130e+03 2.7300e+02 2.3870e+03 4.6400e+02\n",
      " 6.6600e+02 7.8900e+02 9.9330e+03 5.1500e+02 8.1000e+01 1.1500e+02\n",
      " 9.7000e+01 4.3800e+02 5.2090e+03 1.2620e+03 3.9200e+02 1.4780e+03\n",
      " 1.4600e+03 1.6860e+03 6.8000e+01 8.3000e+01 9.1000e+01 2.9880e+03\n",
      " 5.8240e+03 5.0800e+02 8.2000e+02 1.6963e+04 2.9140e+03 8.3700e+02\n",
      " 1.8000e+02 5.3700e+02 2.1750e+03 2.9100e+02 7.4920e+03 4.5000e+01\n",
      " 4.5700e+02 2.3860e+03 7.2900e+02 2.7200e+02 1.7900e+02 1.2130e+03\n",
      " 3.7000e+02 3.1600e+02 1.9720e+03 3.1100e+02 1.3610e+03 4.1320e+03\n",
      " 3.2090e+03 1.0460e+03 2.3010e+03 2.8950e+03 7.9700e+02 1.6340e+03\n",
      " 5.0000e+01 1.7970e+03 4.8530e+03 7.9000e+01 2.5700e+03 4.7100e+02\n",
      " 2.0160e+03 1.0060e+03 3.7400e+02 2.8820e+03 7.6000e+01 6.4200e+02\n",
      " 4.0480e+03 1.4530e+03 1.2200e+03 3.2470e+03 9.4100e+02 9.0040e+03\n",
      " 1.6660e+03 2.8200e+02 2.7030e+03 1.1660e+03 5.9500e+02 8.0500e+02\n",
      " 1.8500e+02 2.1800e+02 2.3100e+02 3.5430e+03 4.9000e+02 3.3800e+02\n",
      " 1.9540e+03 5.1200e+02 3.2200e+02 1.4100e+02 5.2100e+02 1.4250e+03\n",
      " 5.3600e+02 1.1630e+03 4.9500e+02 6.9620e+03 1.8700e+02 4.1570e+03\n",
      " 1.5390e+03 1.1530e+03 6.7900e+02 1.2350e+04 1.7540e+03 8.4500e+02\n",
      " 2.3300e+02 1.4300e+02 1.0900e+03 2.0760e+03 4.2400e+02 5.8000e+02\n",
      " 2.4400e+02 3.2320e+03 1.9000e+01 7.1600e+02 2.8540e+03 6.0200e+02\n",
      " 6.8700e+02 2.8900e+02 1.4390e+03 9.7880e+03 2.9300e+02 7.2700e+02\n",
      " 2.8750e+03 3.5000e+02 8.5000e+01 1.4000e+03 9.3220e+03 7.8600e+02\n",
      " 7.3880e+03 9.5400e+03 7.0200e+02 1.5000e+02 9.5200e+02 1.6250e+04\n",
      " 1.3540e+03 9.6000e+01 6.1600e+02 1.9730e+03 5.0900e+02 3.7890e+03\n",
      " 2.3000e+01 5.6490e+03 7.4500e+02 3.6900e+02 2.8500e+02 1.8260e+03\n",
      " 4.8000e+01 5.4400e+02 2.2900e+02 3.8660e+03 3.2000e+02 4.2830e+03\n",
      " 1.6190e+03 6.4900e+02 7.3000e+01 4.2900e+02 1.6010e+03 2.0620e+03\n",
      " 6.2300e+02 2.2700e+02 6.5620e+03 2.3250e+03 9.9600e+02 6.3300e+02\n",
      " 1.5920e+03 6.8900e+02 3.1460e+03 6.7000e+02 1.5010e+03 2.7000e+02\n",
      " 2.7900e+02 6.2000e+02 2.8470e+03 8.5800e+02 7.1900e+02 4.0000e+02\n",
      " 1.1900e+03 3.3900e+02 3.7700e+03 6.2500e+02 1.0050e+03 1.8300e+03\n",
      " 9.3360e+03 5.2830e+03 2.1050e+03 5.1960e+03 2.5240e+03 3.1420e+03\n",
      " 2.9250e+03 1.2400e+03 1.4730e+03 4.6000e+02 1.8600e+02 4.0300e+02\n",
      " 7.4200e+02 3.8700e+02 2.3670e+03 2.7680e+03 1.2010e+03 9.4600e+02\n",
      " 1.4550e+03 2.4140e+03 2.4800e+02 4.9100e+02 7.2600e+02 2.8000e+01\n",
      " 7.3300e+02 2.7830e+03 3.1310e+03 4.5310e+03 1.9590e+03 9.1100e+02\n",
      " 6.4500e+02 8.0000e+01 1.0830e+03 2.2800e+02 3.8400e+02 2.1100e+03\n",
      " 2.0300e+02 1.1000e+02 3.4850e+03 7.0900e+02 3.3610e+03 7.4400e+02\n",
      " 4.4020e+03 1.3790e+03 1.0302e+04 3.4530e+03 1.4590e+03 2.9600e+02\n",
      " 1.1110e+03 1.3810e+03 1.3620e+03 1.0000e+03 1.5800e+02 2.5720e+03\n",
      " 4.5830e+03 9.9100e+02 3.9400e+02 3.7000e+01 2.5700e+02 7.4300e+02\n",
      " 2.9900e+02 2.1410e+03 3.4700e+02 5.6300e+02 1.2970e+03 2.0310e+03\n",
      " 2.8200e+03 9.4500e+02 6.6000e+01 5.6000e+01 1.1070e+03 6.8300e+02\n",
      " 4.5500e+02 3.3300e+02 2.5000e+02 1.8070e+03 2.6810e+03 3.4620e+03\n",
      " 7.8100e+02 2.3110e+03 6.3100e+02 3.0260e+03 4.7900e+02 1.1020e+03\n",
      " 8.0100e+02 4.0560e+03 4.9130e+03 2.0700e+03 4.5300e+02 1.3100e+02\n",
      " 8.1570e+03 4.5400e+02 1.8390e+03 2.7160e+03 3.8600e+02 4.0930e+03\n",
      " 3.5500e+02 3.7260e+03 1.6330e+03 5.3300e+03 4.4100e+02 7.5300e+02\n",
      " 1.0910e+03 2.0320e+03 2.4050e+03 2.3397e+04 6.4300e+02 2.2790e+03\n",
      " 4.9330e+03 7.8000e+02 5.5800e+02 1.1800e+03 1.2100e+03 8.8400e+02\n",
      " 1.1020e+04 3.1560e+03 2.1890e+03 7.3600e+02 1.6830e+03 2.6400e+02\n",
      " 5.3900e+02 1.6900e+02 4.1600e+02 2.3770e+03 8.2100e+02 7.5100e+02\n",
      " 4.2480e+03 2.2390e+03 1.2570e+03 5.6420e+03 2.4190e+03 7.1000e+02\n",
      " 2.0750e+03 4.8300e+02 1.3080e+03 1.9450e+03 3.9800e+02 7.2500e+03\n",
      " 2.1610e+03 9.0500e+02 2.4620e+03 2.1760e+03 2.1150e+03 3.9730e+03\n",
      " 1.7080e+03 3.4100e+02 5.4600e+02 5.9800e+02 7.6210e+03 2.1820e+03\n",
      " 1.0651e+04 6.3800e+02 5.7780e+03 1.0700e+02 7.1500e+02 1.5170e+03\n",
      " 2.9180e+03 6.8820e+03 2.9200e+02 1.0580e+03 1.2110e+03 3.8060e+03\n",
      " 6.5800e+02 1.9980e+03 7.9800e+02 2.0450e+03 4.7200e+02 5.9800e+03\n",
      " 4.0380e+03 8.7300e+02 4.8130e+03 6.0300e+02 6.0100e+02 8.6500e+02\n",
      " 8.4700e+02 2.6800e+03]\n",
      "유니크한 카테고리 수 :  902\n",
      "null 수              :  0 \n",
      "\n",
      "ShoppingMall \n",
      " [0.000e+00 6.350e+02 2.630e+02 1.360e+02 2.530e+02 2.430e+02 8.000e+00\n",
      " 2.600e+01 1.800e+01 5.273e+03 1.200e+01 3.000e+01 8.700e+01 1.350e+02\n",
      " 1.235e+03 1.585e+03 9.970e+02 3.900e+01 8.200e+01 2.000e+00 5.710e+02\n",
      " 2.305e+03 1.000e+00 6.108e+03 2.300e+01 5.210e+02 1.480e+02 7.050e+02\n",
      " 1.175e+03 1.404e+03 2.060e+02 2.420e+02 5.650e+02 4.000e+00 8.098e+03\n",
      " 2.200e+01 7.800e+01 7.280e+02 2.839e+03 4.620e+02 1.380e+02 4.100e+01\n",
      " 7.022e+03 7.900e+01 1.299e+03 4.220e+02 1.320e+02 8.100e+01 1.026e+03\n",
      " 2.880e+02 7.000e+00 7.550e+02 2.590e+02 6.100e+02 4.391e+03 7.370e+02\n",
      " 1.247e+03 3.000e+00 9.320e+02 5.170e+02 6.700e+01 2.219e+03 1.000e+01\n",
      " 7.590e+02 4.870e+02 6.252e+03 4.410e+02 2.000e+01 1.970e+02 8.580e+02\n",
      " 2.736e+03 5.410e+02 6.870e+02 1.022e+03 3.710e+02 6.300e+01 1.310e+02\n",
      " 6.720e+02 2.427e+03 9.900e+01 1.023e+03 5.500e+01 6.730e+02 1.580e+02\n",
      " 2.840e+02 1.281e+03 1.435e+03 1.740e+02 1.100e+01 1.469e+03 1.057e+03\n",
      " 1.456e+03 2.220e+02 1.110e+03 8.810e+02 6.910e+02 1.608e+03 2.640e+02\n",
      " 5.400e+01 1.790e+03 4.600e+01 2.210e+02 1.020e+03 5.830e+02 1.604e+03\n",
      " 3.500e+02 6.590e+02 3.300e+01 1.817e+03 5.000e+00 7.480e+02 1.487e+03\n",
      " 8.920e+02 1.195e+03 1.750e+02 2.780e+02 4.700e+01 3.500e+01 3.770e+02\n",
      " 8.000e+01 5.290e+02 2.690e+02 2.990e+02 7.620e+02 6.930e+02 5.900e+01\n",
      " 2.350e+02 3.767e+03 2.851e+03 6.440e+02 3.000e+02 5.120e+02 1.162e+03\n",
      " 3.100e+01 2.800e+01 6.970e+02 1.140e+02 4.970e+02 2.572e+03 7.830e+02\n",
      " 5.200e+01 3.440e+02 5.100e+01 1.510e+02 1.719e+03 6.300e+02 9.780e+02\n",
      " 4.230e+02 6.560e+02 5.400e+02 3.800e+01 1.161e+03 3.460e+02 3.200e+01\n",
      " 8.300e+01 2.940e+02 1.500e+02 3.520e+02 2.329e+03 1.810e+02 1.400e+01\n",
      " 5.390e+02 1.863e+03 1.600e+01 4.800e+01 1.094e+03 8.190e+02 2.250e+02\n",
      " 1.547e+03 9.270e+02 2.190e+02 5.970e+02 3.250e+02 1.690e+02 6.080e+02\n",
      " 9.950e+02 2.600e+02 2.936e+03 7.770e+02 8.292e+03 7.500e+01 6.500e+01\n",
      " 2.438e+03 4.300e+02 9.200e+01 3.320e+02 8.420e+02 3.720e+02 8.880e+02\n",
      " 2.620e+03 4.500e+01 3.630e+02 8.300e+02 1.166e+03 1.274e+03 2.130e+02\n",
      " 7.410e+02 8.710e+02 9.260e+02 2.400e+01 9.800e+01 1.090e+03 7.740e+02\n",
      " 6.290e+02 1.650e+02 3.600e+01 1.277e+03 2.160e+02 3.010e+02 2.521e+03\n",
      " 4.000e+01 7.400e+01 9.940e+02 1.566e+03 7.040e+02 5.850e+02 3.780e+02\n",
      " 5.840e+02 1.121e+03 8.017e+03 4.060e+02 2.768e+03 2.100e+01 4.760e+02\n",
      " 7.380e+02 9.000e+00 6.050e+02 1.217e+03 1.300e+01 5.600e+01 2.020e+02\n",
      " 2.700e+01 7.470e+02 7.090e+02 2.345e+03 5.740e+02 7.960e+02 2.980e+02\n",
      " 4.530e+02 6.061e+03 2.970e+02 3.380e+02 4.200e+01 2.000e+02 6.890e+02\n",
      " 8.450e+02 2.620e+02 3.760e+02 1.126e+03 4.661e+03 2.338e+03 5.300e+01\n",
      " 6.660e+02 1.290e+02 3.170e+02 1.400e+02 9.060e+02 2.570e+02 6.340e+02\n",
      " 1.120e+02 7.240e+02 7.150e+02 4.300e+01 2.950e+02 6.920e+02 4.510e+02\n",
      " 2.900e+01 2.080e+02 5.730e+02 7.270e+02 6.860e+02 5.320e+02 4.940e+02\n",
      " 9.690e+02 1.270e+02 1.160e+02 8.740e+02 1.990e+02 2.070e+02 1.522e+03\n",
      " 4.380e+02 8.560e+02 8.590e+02 7.060e+02 7.700e+02 3.950e+02 2.960e+02\n",
      " 1.059e+03 8.480e+02 7.400e+02 1.900e+01 3.870e+02 3.110e+02 4.780e+02\n",
      " 1.610e+03 8.500e+02 1.104e+03 5.649e+03 1.230e+02 2.267e+03 6.990e+02\n",
      " 2.670e+02 7.020e+02 1.352e+03 3.280e+02 1.819e+03 1.950e+02 2.760e+03\n",
      " 8.490e+02 1.330e+02 6.880e+02 3.765e+03 1.184e+03 1.103e+03 6.000e+01\n",
      " 1.388e+03 8.610e+02 9.100e+01 6.000e+00 4.310e+02 2.638e+03 8.500e+01\n",
      " 2.380e+02 1.631e+03 2.271e+03 1.202e+03 1.730e+02 3.030e+02 8.890e+02\n",
      " 3.270e+02 1.240e+02 7.000e+01 1.500e+01 7.700e+01 8.160e+02 3.450e+02\n",
      " 1.110e+02 8.800e+01 5.490e+02 1.923e+03 1.880e+02 9.580e+02 1.780e+02\n",
      " 1.600e+02 1.342e+03 9.820e+02 1.030e+02 3.070e+02 1.680e+02 3.820e+02\n",
      " 2.740e+02 9.770e+02 3.160e+02 2.710e+02 1.001e+03 8.400e+01 1.008e+03\n",
      " 4.600e+02 1.560e+02 6.760e+02 3.700e+01 1.170e+02 1.490e+02 1.425e+03\n",
      " 7.120e+02 7.230e+02 6.850e+02 1.300e+02 1.420e+02 1.698e+03 1.320e+03\n",
      " 4.410e+03 3.560e+02 6.400e+01 1.630e+03 2.451e+03 2.183e+03 5.080e+02\n",
      " 9.360e+02 7.140e+02 9.300e+01 3.790e+02 2.718e+03 2.751e+03 1.072e+03\n",
      " 2.450e+02 2.230e+02 2.820e+02 8.060e+02 1.188e+03 6.900e+01 5.000e+01\n",
      " 7.600e+01 1.930e+02 9.490e+02 4.850e+02 9.170e+02 8.550e+02 2.660e+02\n",
      " 4.150e+02 6.260e+02 3.540e+02 1.700e+01 6.310e+02 8.690e+02 1.275e+03\n",
      " 6.630e+02 1.583e+03 2.330e+02 3.129e+03 3.200e+02 7.720e+02 5.690e+02\n",
      " 2.050e+02 2.081e+03 5.040e+02 8.870e+02 8.790e+02 2.490e+02 6.540e+02\n",
      " 8.251e+03 5.450e+02 2.089e+03 2.320e+02 1.746e+03 1.710e+02 1.340e+02\n",
      " 7.100e+01 6.110e+02 4.610e+02 1.353e+03 1.050e+02 9.700e+01 9.500e+01\n",
      " 6.240e+02 4.390e+02 2.262e+03 3.610e+02 1.177e+03 1.530e+02 7.870e+02\n",
      " 7.690e+02 6.190e+02 1.000e+02 5.420e+02 7.930e+02 9.480e+02 1.460e+02\n",
      " 6.060e+02 2.110e+02 5.790e+02 1.613e+03 4.450e+02 1.168e+03 4.130e+02\n",
      " 4.420e+02 1.444e+03 9.960e+02 9.130e+02 1.365e+03 8.180e+02 7.800e+02\n",
      " 7.680e+02 3.091e+03 4.210e+02 3.930e+02 1.214e+03 4.460e+03 1.062e+03\n",
      " 1.090e+02 1.741e+03 3.650e+02 5.960e+02 5.800e+01 6.420e+02 4.090e+02\n",
      " 6.100e+01 1.048e+03 2.121e+03 1.296e+03 6.200e+01 5.580e+02 2.920e+02\n",
      " 3.510e+02 1.025e+03 1.010e+03 1.430e+02 1.630e+02 1.317e+03 1.014e+03\n",
      " 4.550e+02 6.023e+03 1.423e+03 2.837e+03 7.160e+02 1.030e+03 7.030e+02\n",
      " 1.222e+03 9.450e+02 8.380e+02 1.041e+03 5.940e+02 6.830e+02 7.170e+02\n",
      " 2.488e+03 2.500e+01 1.599e+03 1.565e+03 7.250e+02 1.210e+02 1.618e+03\n",
      " 6.140e+02 3.509e+03 1.150e+02 3.180e+02 1.450e+02 1.987e+03 9.840e+02\n",
      " 1.789e+03 2.760e+02 1.079e+03 1.080e+02 3.970e+02 1.143e+03 2.170e+02\n",
      " 4.020e+02 1.220e+03 2.127e+03 6.320e+02 7.200e+01 6.510e+02 8.430e+02\n",
      " 1.156e+03 7.000e+02 6.130e+02 9.460e+02 4.570e+02 2.280e+02 8.440e+02\n",
      " 1.540e+02 3.074e+03 9.930e+02 8.070e+02 6.550e+02 1.351e+03 1.410e+02\n",
      " 2.646e+03 9.830e+02 4.700e+02 5.190e+02 2.730e+02 4.860e+02 1.066e+03\n",
      " 2.104e+03 9.280e+02 5.950e+02 8.640e+02 9.680e+02 3.400e+01 8.310e+02\n",
      " 1.442e+03 5.250e+02 1.900e+02 1.896e+03 1.211e+03 6.200e+02 6.170e+02\n",
      " 9.600e+01 8.020e+02 4.950e+02 4.471e+03 7.300e+01 8.660e+02 5.280e+02\n",
      " 3.830e+02 6.460e+02 7.610e+02 4.290e+02 2.332e+03 6.470e+02 1.117e+03\n",
      " 7.260e+02 8.780e+02 2.140e+02 2.461e+03 1.709e+03 2.153e+03 5.100e+02\n",
      " 1.040e+02 1.005e+03 2.580e+02 4.080e+02 5.330e+02 2.396e+03 1.153e+03\n",
      " 7.350e+02 4.680e+02 1.078e+03 1.440e+02 3.050e+02 4.900e+02 9.040e+02\n",
      " 5.090e+02 5.140e+02 5.700e+01 7.320e+02 1.390e+02 1.820e+02 8.980e+02\n",
      " 1.063e+03 3.290e+02 1.570e+02 6.600e+01 8.680e+02 1.610e+02 6.960e+02\n",
      " 2.270e+02 6.020e+02 4.200e+02 1.367e+03 1.825e+03 1.250e+02 7.990e+02\n",
      " 2.790e+02 1.720e+02 2.560e+02 2.216e+03 1.512e+03 6.070e+02 1.015e+03\n",
      " 2.561e+03 3.120e+02 1.070e+02 7.570e+02 7.750e+02 7.010e+02 2.090e+02\n",
      " 9.760e+02 5.720e+02 1.349e+03 6.520e+02 2.228e+03 1.393e+03 3.230e+02\n",
      " 1.245e+03 2.871e+03 1.171e+03 6.820e+02 9.400e+02 9.540e+02 1.950e+03\n",
      " 9.520e+02 1.446e+03 7.210e+02 1.660e+02 1.968e+03 1.187e+03 2.890e+02\n",
      " 9.430e+02 9.400e+01 9.750e+02 8.120e+02 2.619e+03 1.370e+02 5.470e+02\n",
      " 4.790e+02 2.035e+03 3.260e+02 7.110e+02 2.237e+03 4.100e+02 9.710e+02\n",
      " 8.390e+02 1.260e+03 3.880e+02 3.340e+02 2.510e+02 9.700e+02 5.600e+02\n",
      " 6.940e+02 1.328e+03 9.650e+02 7.290e+02 4.170e+02 3.550e+02 1.773e+03\n",
      " 4.740e+02 3.700e+02 8.400e+02 1.322e+03 4.400e+02 6.210e+02 2.257e+03\n",
      " 1.319e+03 6.150e+02 1.464e+03 2.680e+02 2.142e+03 1.693e+03 1.307e+03\n",
      " 9.530e+02 3.429e+03 9.660e+02 1.039e+03 1.520e+02 1.636e+03 1.130e+02\n",
      " 7.980e+02 7.130e+02 6.620e+02 1.311e+03 2.156e+03 5.430e+02 1.258e+03\n",
      " 3.851e+03]\n",
      "유니크한 카테고리 수 :  715\n",
      "null 수              :  0 \n",
      "\n",
      "Spa \n",
      " [0.0000e+00 2.8230e+03 1.8100e+02 1.1300e+02 2.3700e+02 7.0000e+00\n",
      " 1.4000e+01 1.6870e+03 9.0300e+02 4.5700e+02 5.6900e+02 3.1440e+03\n",
      " 3.0000e+00 1.6090e+03 4.0000e+00 1.4090e+03 1.8000e+01 1.1800e+02\n",
      " 2.1400e+02 2.6500e+02 8.6600e+02 5.9900e+02 1.0000e+00 3.6000e+01\n",
      " 2.9000e+01 7.2300e+02 1.2130e+03 1.5020e+03 2.1000e+01 7.5600e+02\n",
      " 4.6500e+02 4.5720e+03 1.5500e+02 1.2767e+04 2.0000e+00 1.5310e+03\n",
      " 1.3000e+01 1.8130e+03 7.5100e+02 8.5000e+01 6.0200e+02 8.5170e+03\n",
      " 4.6000e+01 1.5600e+03 6.2000e+01 9.1900e+02 1.9360e+03 3.6800e+02\n",
      " 6.1300e+02 1.0790e+03 4.4000e+01 9.7800e+02 4.9660e+03 1.0000e+01\n",
      " 4.2000e+01 3.0800e+02 4.8820e+03 3.4900e+03 4.9300e+02 7.0400e+02\n",
      " 3.9300e+02 5.0000e+01 6.0000e+00 7.8300e+02 3.3200e+02 7.3600e+02\n",
      " 8.8200e+02 1.0280e+03 6.2800e+02 7.6680e+03 1.5000e+01 3.8000e+01\n",
      " 1.6300e+02 4.8600e+02 3.1500e+02 2.3800e+02 1.6000e+01 1.1210e+03\n",
      " 4.4200e+02 3.9500e+02 2.4880e+03 4.4700e+02 1.1540e+03 1.0200e+02\n",
      " 8.0000e+00 6.1000e+01 1.0243e+04 5.3200e+02 3.3300e+02 5.0500e+02\n",
      " 1.7000e+02 7.5500e+02 5.0000e+00 8.7400e+02 6.6000e+01 3.3600e+02\n",
      " 4.0000e+01 5.3900e+02 1.6900e+02 7.8700e+02 1.1000e+01 4.3400e+02\n",
      " 1.1970e+03 1.9350e+03 1.4120e+03 4.7800e+02 3.7170e+03 1.3420e+03\n",
      " 3.8600e+02 6.2100e+02 1.6630e+03 7.4020e+03 8.3000e+01 1.6780e+03\n",
      " 2.3000e+01 3.8580e+03 2.3930e+03 2.2000e+01 2.2170e+03 2.0000e+01\n",
      " 2.4100e+02 2.6730e+03 3.6500e+02 3.2430e+03 3.7500e+02 7.3200e+02\n",
      " 7.9800e+02 1.6000e+02 1.1600e+02 8.0800e+02 3.7000e+02 6.9620e+03\n",
      " 5.6000e+01 6.2820e+03 2.5600e+02 5.2700e+02 4.5000e+01 9.0000e+00\n",
      " 1.5000e+02 6.0000e+01 1.8800e+02 4.1000e+01 7.7000e+01 5.6200e+02\n",
      " 3.6130e+03 4.8400e+02 1.6240e+03 4.5840e+03 1.1200e+02 1.5140e+03\n",
      " 9.1000e+01 4.6300e+02 4.7000e+01 4.4900e+02 1.4200e+02 1.2000e+01\n",
      " 2.2100e+02 7.5700e+02 1.1490e+03 1.8240e+03 4.2400e+02 9.4800e+02\n",
      " 1.6120e+03 9.1000e+02 6.0100e+02 1.3100e+02 6.3600e+02 2.0000e+02\n",
      " 1.7000e+01 1.4460e+03 1.7070e+03 4.3000e+02 1.2600e+02 3.3700e+02\n",
      " 1.9080e+03 9.0100e+02 2.5100e+02 1.9000e+01 1.9200e+02 2.0100e+02\n",
      " 1.5100e+02 3.5000e+01 3.1830e+03 7.5360e+03 4.2200e+02 4.8500e+02\n",
      " 4.9000e+02 7.7100e+02 5.6800e+02 7.6000e+01 7.4980e+03 5.2200e+02\n",
      " 1.6160e+03 7.5800e+02 6.9900e+02 9.2000e+02 6.7100e+02 4.5200e+02\n",
      " 5.3500e+02 1.8900e+02 6.7200e+02 8.9990e+03 1.2200e+02 2.9800e+02\n",
      " 6.8000e+01 8.8600e+02 1.6320e+03 1.0380e+03 2.8180e+03 4.0200e+02\n",
      " 8.6000e+01 4.7900e+02 4.9760e+03 9.1300e+02 2.1200e+02 1.3010e+03\n",
      " 2.7100e+02 3.9400e+02 7.0700e+02 7.2000e+01 1.2200e+03 8.2200e+02\n",
      " 6.0600e+02 4.7640e+03 3.1700e+02 3.3800e+02 6.1000e+02 2.1800e+02\n",
      " 9.3000e+01 1.2000e+02 3.5100e+02 1.3840e+03 3.5500e+02 1.6700e+02\n",
      " 2.4000e+02 3.7480e+03 7.4100e+02 1.6410e+03 2.4500e+02 1.4500e+02\n",
      " 8.2700e+02 1.8500e+02 6.3200e+02 4.1600e+03 9.7400e+02 6.3800e+02\n",
      " 6.0000e+02 1.3110e+03 4.3900e+02 1.2600e+03 5.3000e+01 3.9100e+02\n",
      " 3.8900e+02 3.2700e+02 9.9400e+02 1.7570e+03 2.8300e+02 6.7000e+01\n",
      " 4.2570e+03 1.1000e+02 5.9000e+02 3.9000e+01 6.3000e+01 1.4700e+02\n",
      " 8.4000e+01 5.9300e+02 2.0700e+02 1.3700e+02 1.2400e+02 9.5100e+02\n",
      " 2.0600e+03 2.3200e+02 6.6300e+02 9.8000e+02 9.2900e+02 2.7000e+01\n",
      " 1.0600e+02 5.2900e+02 1.2730e+03 8.9700e+02 4.0600e+02 5.3600e+02\n",
      " 1.1230e+03 6.8800e+02 5.9500e+02 3.7000e+01 7.8800e+02 3.7800e+02\n",
      " 2.7600e+02 1.1880e+03 1.5520e+03 3.7100e+02 3.0000e+01 9.1700e+02\n",
      " 1.6100e+02 2.4000e+01 2.4600e+02 2.6000e+02 2.5200e+02 5.2000e+01\n",
      " 5.6000e+02 8.6700e+02 1.5850e+03 3.5880e+03 4.4100e+02 5.7000e+01\n",
      " 7.0900e+02 5.8310e+03 3.3000e+01 1.2100e+02 1.3300e+02 1.9900e+02\n",
      " 5.9570e+03 3.1100e+02 6.3100e+02 6.3000e+02 3.4000e+01 4.1710e+03\n",
      " 4.0000e+02 8.5800e+03 1.7590e+03 3.4500e+02 9.8500e+02 1.6850e+03\n",
      " 9.6300e+02 8.0200e+02 5.1500e+02 1.7000e+03 1.9400e+03 1.1180e+03\n",
      " 4.9000e+01 1.1870e+03 2.1840e+03 2.3400e+02 6.6500e+02 6.5000e+01\n",
      " 2.4270e+03 4.4300e+02 4.5100e+02 1.2960e+03 2.9280e+03 9.5000e+01\n",
      " 4.4500e+02 2.0510e+03 1.8300e+02 8.9500e+02 1.7710e+03 3.8290e+03\n",
      " 1.5733e+04 1.5255e+04 5.5340e+03 7.5000e+01 1.7420e+03 3.4200e+02\n",
      " 4.0100e+02 3.8100e+02 6.5500e+02 5.9200e+02 1.6560e+03 2.4050e+03\n",
      " 2.5000e+02 1.0900e+03 2.9700e+02 5.1000e+01 1.7300e+02 5.1560e+03\n",
      " 2.0800e+02 7.9600e+02 7.8400e+02 5.2520e+03 3.7520e+03 3.6400e+02\n",
      " 7.7500e+02 1.2842e+04 1.9230e+03 2.8820e+03 2.0610e+03 3.7450e+03\n",
      " 3.5600e+03 1.0900e+02 7.1200e+02 9.6000e+01 1.7880e+03 6.3860e+03\n",
      " 2.8000e+01 1.7500e+02 7.1600e+02 3.7620e+03 6.3070e+03 5.0600e+02\n",
      " 9.6600e+02 4.4670e+03 2.1700e+02 4.7200e+02 8.8400e+02 6.8400e+02\n",
      " 2.2870e+03 9.2200e+02 4.1900e+02 7.3400e+02 1.0232e+04 4.0700e+02\n",
      " 6.6700e+02 3.7690e+03 2.5000e+01 2.0500e+02 9.4000e+01 3.7440e+03\n",
      " 7.4400e+02 3.0200e+02 3.0900e+02 2.2000e+02 5.4900e+02 1.3170e+03\n",
      " 1.0450e+03 2.1400e+03 2.6750e+03 8.7000e+01 6.8600e+02 1.9660e+03\n",
      " 7.2000e+02 5.9000e+01 7.7000e+02 4.3000e+01 5.8400e+02 5.4000e+01\n",
      " 7.1800e+02 6.2890e+03 1.4300e+02 7.9000e+01 8.5100e+02 7.6200e+02\n",
      " 7.5000e+02 5.4100e+02 9.0000e+01 5.3700e+02 9.9100e+02 1.6750e+03\n",
      " 4.1460e+03 2.8600e+02 2.1000e+02 3.5600e+02 8.1000e+01 7.4500e+02\n",
      " 2.6000e+01 1.7700e+02 5.1900e+02 1.7800e+02 5.0300e+02 1.5230e+03\n",
      " 2.3600e+02 6.4400e+02 1.4240e+03 6.2510e+03 1.3960e+03 7.8100e+02\n",
      " 2.2840e+03 3.7200e+02 4.6950e+03 7.0680e+03 1.2437e+04 2.3470e+03\n",
      " 8.0400e+02 9.2600e+02 4.4600e+02 1.6030e+03 1.8470e+03 4.6100e+02\n",
      " 3.4600e+02 9.5700e+02 9.4300e+02 6.1900e+02 1.3800e+02 3.0600e+02\n",
      " 1.0610e+03 3.0500e+02 7.9400e+02 1.1850e+03 2.6700e+02 1.3120e+03\n",
      " 5.9280e+03 5.3100e+02 6.5200e+02 1.3900e+03 1.0300e+02 1.2800e+02\n",
      " 4.6180e+03 2.8480e+03 1.5080e+03 3.9900e+02 2.2600e+02 5.5200e+02\n",
      " 8.9100e+02 2.7400e+02 4.1500e+02 3.4390e+03 1.0670e+03 3.1000e+01\n",
      " 9.8600e+02 1.4950e+03 1.6500e+02 8.9000e+01 3.4100e+02 5.4820e+03\n",
      " 4.0720e+03 5.2300e+02 1.0350e+03 2.5250e+03 8.0960e+03 1.2790e+03\n",
      " 1.5900e+02 8.2500e+02 2.7800e+02 8.1900e+02 1.7840e+03 7.0600e+02\n",
      " 2.3000e+02 1.9400e+02 2.5970e+03 1.8660e+03 1.3510e+03 2.6100e+02\n",
      " 4.3300e+02 2.8800e+02 1.9300e+02 4.3600e+02 9.8000e+01 1.2380e+03\n",
      " 5.2290e+03 6.8900e+02 1.3400e+02 4.2900e+02 4.2000e+02 9.9000e+01\n",
      " 6.8100e+02 6.1620e+03 1.0800e+03 4.9600e+02 6.5400e+02 8.8900e+02\n",
      " 7.4600e+02 2.0260e+03 1.1483e+04 5.7000e+02 1.3580e+03 3.5900e+02\n",
      " 1.3200e+03 2.1820e+03 1.1160e+03 1.9610e+03 6.2980e+03 3.8170e+03\n",
      " 4.1700e+02 6.9200e+02 8.8000e+01 3.0810e+03 3.6050e+03 2.0950e+03\n",
      " 3.4000e+02 2.4900e+03 1.8220e+03 1.2840e+03 6.6000e+02 1.1790e+03\n",
      " 7.9100e+02 2.3100e+02 2.6400e+02 1.5100e+03 1.0290e+03 6.9000e+02\n",
      " 6.9600e+02 1.7290e+03 6.2900e+02 6.9700e+02 3.2400e+02 6.1600e+02\n",
      " 4.7000e+02 1.5300e+02 2.5670e+03 7.0500e+02 1.1700e+02 4.9200e+02\n",
      " 1.1080e+03 3.8500e+02 2.6900e+03 6.6800e+02 3.0480e+03 2.1460e+03\n",
      " 2.8400e+02 1.9844e+04 5.8000e+02 8.7100e+02 5.5000e+01 6.2500e+02\n",
      " 6.3900e+02 1.5200e+02 2.8100e+02 2.6240e+03 1.7990e+03 1.0400e+02\n",
      " 5.3870e+03 4.0180e+03 7.9500e+02 8.9800e+02 2.5540e+03 2.2300e+02\n",
      " 4.8000e+01 1.4600e+03 6.9000e+01 8.8300e+02 4.9800e+02 2.7580e+03\n",
      " 1.0630e+03 1.1920e+03 1.3983e+04 6.9400e+02 7.3900e+02 7.6900e+02\n",
      " 2.5930e+03 6.5900e+02 1.4820e+03 4.0500e+02 2.7900e+02 1.3940e+03\n",
      " 1.0650e+03 8.4400e+02 2.5800e+02 6.8500e+02 2.9410e+03 4.8300e+02\n",
      " 1.6440e+03 2.0770e+03 1.2690e+04 1.4400e+02 7.1400e+02 5.2600e+02\n",
      " 7.4000e+02 7.2200e+02 7.6100e+02 2.4420e+03 3.7700e+02 1.0100e+02\n",
      " 6.6200e+02 4.6700e+02 3.8280e+03 1.1780e+03 3.2800e+02 2.4200e+03\n",
      " 3.2000e+02 1.2700e+02 3.6900e+02 1.6470e+03 5.5400e+02 5.5500e+02\n",
      " 9.3800e+02 1.2940e+03 1.5800e+02 6.4200e+02 1.1400e+02 1.4600e+02\n",
      " 6.9800e+02 4.9500e+02 4.2100e+02 3.4300e+03 5.8650e+03 9.6400e+02\n",
      " 1.0800e+02 6.7400e+02 1.7380e+03 3.7300e+02 8.3800e+02 4.9900e+02\n",
      " 1.4440e+03 6.0500e+02 2.0160e+03 6.4720e+03 1.9060e+03 5.0800e+02\n",
      " 5.1250e+03 4.5400e+02 6.6030e+03 3.4930e+03 3.4730e+03 1.6230e+03\n",
      " 5.5300e+02 6.8700e+02 9.4500e+02 6.4000e+01 9.8910e+03 1.4790e+03\n",
      " 4.1200e+02 1.4710e+03 7.6900e+03 9.7900e+02 2.9200e+02 5.8600e+02\n",
      " 3.4800e+02 1.1000e+03 2.0300e+02 1.1900e+02 2.2060e+03 7.4900e+02\n",
      " 3.8200e+02 1.4470e+03 1.0190e+03 5.6600e+02 6.0300e+02 1.8200e+02\n",
      " 1.7130e+03 5.9800e+02 3.2100e+02 1.3440e+03 8.9110e+03 5.7200e+02\n",
      " 7.0000e+01 2.5400e+02 6.4500e+02 9.7000e+01 3.6700e+02 1.4390e+03\n",
      " 1.2270e+03 9.0900e+02 5.7370e+03 7.8000e+02 7.7700e+02 6.5300e+02\n",
      " 1.2630e+03 3.5700e+02 2.4120e+03 1.3200e+02 9.0200e+02 1.6140e+03\n",
      " 7.5400e+02 1.0470e+03 3.0430e+03 2.9300e+02 4.4410e+03 8.1100e+02\n",
      " 7.6800e+02 1.5200e+03 1.8820e+03 1.0700e+02 5.8000e+01 1.6200e+02\n",
      " 2.9900e+02 1.5340e+03 2.6300e+02 2.3500e+02 3.5300e+02 6.1700e+02\n",
      " 6.5800e+02 1.2480e+03 4.7300e+02 9.2000e+01 7.3000e+01 1.8560e+03\n",
      " 2.5360e+03 2.1190e+03 7.0100e+02 1.0880e+03 1.1520e+03 4.7240e+03\n",
      " 3.6580e+03 1.6810e+03 1.7400e+02 7.2500e+02 1.5190e+03 2.5900e+02\n",
      " 7.3000e+02 7.8500e+02 2.4800e+02 2.9100e+02 1.5180e+03 1.0070e+03\n",
      " 8.9600e+02 1.1100e+02 8.1800e+02 8.7300e+02 2.6200e+02 1.4730e+03\n",
      " 9.8700e+02 3.2370e+03 4.8000e+02 6.7700e+02 1.0660e+03 1.9990e+03\n",
      " 1.8270e+03 3.7100e+03 2.5500e+02 4.9230e+03 1.7200e+02 2.6060e+03\n",
      " 3.0130e+03 5.5000e+02 1.3690e+03 7.8900e+02 4.7020e+03 3.1040e+03\n",
      " 1.4900e+02 7.2800e+02 7.1000e+01 4.3310e+03 1.1470e+03 6.5550e+03\n",
      " 4.9700e+02 5.1880e+03 3.9930e+03 1.0130e+03 2.5820e+03 9.0600e+02\n",
      " 3.1200e+02 3.5270e+03 9.6700e+02 3.7830e+03 3.7600e+02 5.4200e+02\n",
      " 1.6270e+03 1.4252e+04 3.1960e+03 7.2600e+02 2.2120e+03 1.2250e+03\n",
      " 1.3660e+03 1.0990e+03 6.8120e+03 2.1530e+03 1.0230e+03 1.8720e+03\n",
      " 7.8000e+01 1.7160e+03 1.3770e+03 1.8120e+03 1.6800e+02]\n",
      "유니크한 카테고리 수 :  833\n",
      "null 수              :  0 \n",
      "\n",
      "VRDeck \n",
      " [0.0000e+00 5.8500e+02 6.0000e+01 1.0100e+03 2.2400e+02 9.2000e+01\n",
      " 6.9000e+02 3.4300e+02 6.3000e+01 1.2350e+03 1.5500e+02 4.2200e+02\n",
      " 2.3000e+01 3.6700e+02 1.5000e+01 4.8400e+02 5.5600e+02 6.8600e+02\n",
      " 1.5760e+03 2.4580e+03 9.7000e+01 5.0900e+02 3.5230e+03 2.2300e+02\n",
      " 1.3600e+02 1.1720e+03 4.5000e+02 5.8000e+01 1.6300e+02 1.8000e+01\n",
      " 1.1930e+03 2.4390e+03 4.1000e+01 1.7140e+03 6.4390e+03 7.8000e+01\n",
      " 1.0000e+02 1.2560e+03 9.4000e+01 1.0000e+00 2.2000e+01 4.0550e+03\n",
      " 2.8100e+02 5.2400e+02 6.0800e+02 1.2400e+02 9.0000e+00 5.2200e+02\n",
      " 7.9600e+02 7.0000e+00 5.8320e+03 1.3730e+03 1.9750e+03 3.0000e+00\n",
      " 5.2880e+03 5.5300e+02 1.4000e+01 1.4440e+03 5.0000e+00 8.1000e+01\n",
      " 8.3500e+02 6.8900e+02 4.6100e+02 2.7560e+03 1.6000e+01 4.6000e+01\n",
      " 2.1000e+01 6.5900e+02 1.2980e+03 4.4600e+02 2.4710e+03 4.6410e+03\n",
      " 1.1300e+03 3.2000e+01 4.2400e+02 1.0250e+03 1.7000e+01 1.3850e+03\n",
      " 2.5600e+02 5.7000e+01 3.1000e+01 3.0400e+02 4.3700e+02 4.6790e+03\n",
      " 2.5830e+03 2.3100e+02 1.7100e+02 2.7600e+02 6.0000e+00 7.8800e+02\n",
      " 1.7200e+02 5.2000e+01 1.4400e+02 2.5050e+03 4.4190e+03 1.4560e+03\n",
      " 3.5060e+03 1.5940e+04 5.1000e+01 1.0000e+01 5.0600e+02 2.0000e+00\n",
      " 3.0000e+01 2.7700e+02 4.0000e+00 4.6200e+02 9.3000e+01 8.3300e+02\n",
      " 5.9150e+03 6.2200e+02 6.5120e+03 5.2180e+03 3.4000e+01 4.1500e+02\n",
      " 4.8200e+02 6.5300e+02 6.3200e+02 9.0800e+02 1.5600e+02 2.2570e+03\n",
      " 1.6500e+02 1.4050e+03 7.4000e+01 9.0000e+01 5.3000e+01 5.0800e+02\n",
      " 4.4900e+02 1.9000e+01 1.2863e+04 1.1000e+01 5.1300e+02 6.1190e+03\n",
      " 4.9000e+01 2.4000e+01 6.1510e+03 1.3800e+02 2.1580e+03 2.1500e+02\n",
      " 7.5300e+02 9.4200e+02 2.1700e+03 3.0700e+02 1.5820e+03 3.6100e+02\n",
      " 1.2000e+02 5.8150e+03 1.7420e+03 1.0310e+03 2.8820e+03 2.7500e+02\n",
      " 7.9000e+01 2.7000e+01 6.5600e+02 5.3800e+02 2.6000e+01 3.6000e+01\n",
      " 3.3110e+03 3.4100e+02 8.0000e+00 4.4800e+02 7.5570e+03 1.5900e+02\n",
      " 6.1100e+02 2.8000e+02 1.2600e+03 5.0000e+01 1.6100e+02 1.0300e+02\n",
      " 3.5100e+02 4.1900e+02 6.6100e+02 5.7000e+02 5.5000e+01 9.5000e+01\n",
      " 1.0600e+02 7.3820e+03 7.4700e+03 1.2000e+01 4.7000e+01 5.8240e+03\n",
      " 2.1880e+03 5.9000e+01 2.7350e+03 1.4200e+02 2.0000e+01 8.9000e+01\n",
      " 3.8770e+03 5.7800e+02 4.4200e+02 1.3500e+02 1.4268e+04 8.0030e+03\n",
      " 5.9100e+02 6.9200e+02 1.0800e+02 4.0000e+01 1.1240e+03 2.7320e+03\n",
      " 7.2100e+02 2.9700e+02 3.6200e+02 3.2700e+02 8.7900e+02 3.0800e+02\n",
      " 1.8300e+02 1.4000e+02 1.8100e+02 1.1410e+03 2.8900e+02 1.6620e+03\n",
      " 6.0830e+03 5.5500e+03 2.6000e+02 6.5200e+02 2.1490e+03 1.3300e+02\n",
      " 1.9610e+03 5.3000e+02 1.2226e+04 4.3000e+02 1.0500e+02 8.4000e+01\n",
      " 3.8700e+02 1.5100e+02 1.0680e+03 1.1700e+02 8.7000e+01 7.0460e+03\n",
      " 4.4110e+03 3.0200e+02 3.5000e+02 7.1000e+01 1.1250e+03 8.6000e+01\n",
      " 8.8000e+02 1.5200e+02 6.5400e+02 1.4080e+03 1.0550e+03 3.1550e+03\n",
      " 9.5400e+02 4.9200e+02 8.7200e+02 6.1700e+02 8.7800e+02 3.9000e+01\n",
      " 8.8200e+02 7.7900e+02 2.6010e+03 1.9170e+03 1.3100e+02 8.6500e+02\n",
      " 8.9600e+02 1.1632e+04 3.5500e+02 2.5200e+02 2.7200e+02 3.2090e+03\n",
      " 2.6900e+02 6.6600e+02 1.1280e+03 3.0300e+02 1.1100e+02 7.1610e+03\n",
      " 6.5700e+02 6.4300e+02 1.5930e+03 1.5125e+04 8.3000e+01 2.2700e+02\n",
      " 2.3580e+03 2.0060e+03 6.5000e+01 1.7650e+03 3.7700e+02 2.0300e+02\n",
      " 6.8500e+02 6.0310e+03 1.9086e+04 6.3100e+02 4.5000e+01 2.8270e+03\n",
      " 7.2000e+01 5.6000e+01 9.8200e+02 1.8200e+02 4.9600e+02 8.7300e+02\n",
      " 1.3250e+03 2.4060e+03 1.1800e+02 3.2170e+03 3.2230e+03 4.7800e+02\n",
      " 3.2000e+02 2.3300e+02 5.8700e+02 3.1500e+02 3.6900e+02 6.7000e+01\n",
      " 1.0650e+03 7.0300e+02 3.7900e+02 1.3170e+03 1.2010e+03 6.6800e+02\n",
      " 7.5000e+01 1.2810e+03 2.5000e+01 9.1000e+01 3.8080e+03 5.7200e+02\n",
      " 3.1400e+02 6.2000e+02 2.0700e+02 7.6600e+02 1.2920e+03 3.5900e+03\n",
      " 2.6500e+02 9.6000e+02 8.2000e+01 1.3000e+01 3.5400e+02 2.1660e+03\n",
      " 6.4200e+02 3.1780e+03 1.5700e+02 5.5200e+02 5.1400e+02 7.6000e+01\n",
      " 4.3200e+02 4.4840e+03 1.3870e+03 3.2900e+02 1.9190e+03 1.9400e+02\n",
      " 2.0800e+02 4.7300e+02 4.0300e+02 1.1330e+03 2.1800e+02 5.5100e+02\n",
      " 1.2390e+03 1.3700e+02 4.7360e+03 7.1200e+02 8.0800e+02 4.4000e+01\n",
      " 8.0300e+02 7.3900e+02 3.6560e+03 3.7620e+03 1.2700e+02 7.0900e+02\n",
      " 1.5690e+03 1.4700e+03 1.5630e+03 5.4700e+02 1.1520e+03 2.8620e+03\n",
      " 2.4300e+02 1.1333e+04 3.9600e+02 1.1130e+03 6.0300e+02 1.7900e+02\n",
      " 1.1110e+03 1.7400e+02 2.4130e+03 6.4500e+02 6.2000e+01 5.1800e+02\n",
      " 6.0760e+03 7.6900e+02 7.4200e+02 7.7000e+01 5.5800e+02 8.7700e+02\n",
      " 2.8600e+02 3.3000e+01 5.5130e+03 2.3600e+02 3.8000e+01 2.1860e+03\n",
      " 2.9220e+03 3.6000e+02 2.2430e+03 2.7900e+02 4.3000e+01 6.7700e+02\n",
      " 6.8000e+01 4.6300e+02 2.5100e+02 2.3340e+03 1.2060e+03 6.7800e+02\n",
      " 1.1700e+03 2.6040e+03 1.0400e+02 3.1040e+03 2.7220e+03 2.1700e+02\n",
      " 9.5900e+02 1.0040e+03 1.2970e+03 1.3970e+03 2.0190e+03 4.0900e+02\n",
      " 8.9700e+02 5.3400e+02 6.4900e+02 6.4950e+03 1.0780e+03 4.2000e+01\n",
      " 2.1950e+03 1.2300e+02 2.9440e+03 6.6300e+02 1.5680e+03 8.2550e+03\n",
      " 3.7200e+02 2.8210e+03 1.3200e+02 3.7000e+01 2.0600e+02 8.0000e+01\n",
      " 3.2950e+03 3.4200e+02 3.0810e+03 2.7100e+02 5.8830e+03 2.5500e+02\n",
      " 6.3660e+03 7.7000e+02 4.0000e+02 4.5900e+02 3.2500e+02 4.5300e+02\n",
      " 7.4800e+02 1.2500e+02 3.5210e+03 6.8100e+02 1.3900e+03 8.7100e+02\n",
      " 3.5700e+02 5.0500e+02 1.5440e+03 7.6300e+02 7.2600e+02 7.1700e+02\n",
      " 5.5000e+02 7.1500e+02 1.8850e+03 3.6410e+03 3.1290e+03 1.2200e+02\n",
      " 1.2800e+02 4.1110e+03 1.0200e+03 1.4200e+03 5.9200e+02 6.3800e+02\n",
      " 1.1090e+03 4.3400e+02 5.3900e+02 1.4210e+03 1.5350e+03 3.4500e+02\n",
      " 5.2800e+02 1.1225e+04 1.9800e+02 8.4430e+03 2.9910e+03 3.8200e+02\n",
      " 1.6380e+03 1.6490e+03 4.5200e+02 1.7790e+03 1.9100e+02 7.5500e+02\n",
      " 3.6300e+02 7.7490e+03 8.1800e+02 4.0930e+03 5.4000e+01 1.1450e+03\n",
      " 2.8500e+02 4.4300e+02 2.0500e+02 7.9850e+03 1.7840e+03 3.1520e+03\n",
      " 6.1000e+01 7.0000e+01 7.7800e+02 2.1750e+03 7.5600e+02 3.0620e+03\n",
      " 2.1040e+03 1.4300e+02 7.6500e+02 3.6800e+02 2.9000e+01 9.3300e+02\n",
      " 3.8100e+02 5.8600e+02 1.5080e+03 5.2200e+03 2.5900e+02 4.1020e+03\n",
      " 8.8600e+02 2.0920e+03 1.2100e+03 1.3400e+02 7.3120e+03 5.0700e+02\n",
      " 4.3800e+02 1.1150e+03 4.2500e+02 3.1000e+02 3.2800e+02 3.3000e+02\n",
      " 7.2800e+02 3.4900e+02 5.0300e+02 8.8300e+02 1.7000e+02 1.9300e+02\n",
      " 7.8500e+02 3.0850e+03 4.8800e+02 1.0750e+03 1.3660e+03 4.7900e+02\n",
      " 2.7770e+03 6.2400e+02 2.1600e+02 6.4000e+01 6.3000e+02 3.1700e+02\n",
      " 8.1400e+02 8.0900e+02 3.4000e+02 9.0900e+02 2.2730e+03 8.1700e+02\n",
      " 6.0400e+02 6.3400e+02 5.5400e+02 1.9940e+03 1.5040e+03 1.3040e+03\n",
      " 5.6100e+02 2.1300e+02 1.2730e+03 4.8000e+01 3.7300e+02 1.5400e+02\n",
      " 1.9860e+03 7.3500e+02 9.7700e+02 5.1600e+02 3.8600e+02 4.4350e+03\n",
      " 4.8500e+02 4.3300e+02 4.3260e+03 2.8870e+03 7.2400e+02 5.2700e+02\n",
      " 1.4700e+02 5.8100e+02 2.4000e+02 1.6240e+03 8.6300e+02 3.2340e+03\n",
      " 5.6400e+02 5.6110e+03 1.6514e+04 1.5150e+03 1.0587e+04 3.2720e+03\n",
      " 1.3940e+03 1.1900e+03 2.1200e+02 7.6430e+03 1.5190e+03 3.2400e+02\n",
      " 7.9200e+02 7.2300e+02 6.7000e+02 1.4587e+04 7.3000e+02 8.2800e+02\n",
      " 9.0000e+02 6.8200e+02 2.6800e+02 1.1940e+03 1.0200e+02 8.5000e+01\n",
      " 8.0200e+02 6.6000e+01 3.9940e+03 1.0810e+03 1.0980e+03 2.2272e+04\n",
      " 1.0100e+02 7.2200e+02 1.4100e+02 2.8400e+02 1.8800e+02 9.1300e+02\n",
      " 7.0700e+02 9.1400e+02 1.8440e+03 3.1850e+03 2.1680e+03 7.3600e+02\n",
      " 1.2900e+02 1.4120e+03 1.6600e+02 7.0000e+02 9.9700e+02 1.0900e+02\n",
      " 4.8900e+02 2.2500e+02 1.8670e+04 9.6260e+03 1.7540e+03 4.5320e+03\n",
      " 1.4500e+02 1.1500e+02 3.6500e+02 1.7300e+02 7.7300e+02 2.4400e+02\n",
      " 9.6130e+03 6.2600e+02 1.6700e+02 2.0000e+03 4.7200e+02 1.0050e+03\n",
      " 9.1900e+02 5.8790e+03 5.1000e+02 4.0700e+02 2.6550e+03 2.0790e+03\n",
      " 4.8300e+02 8.8000e+01 2.2600e+02 1.5520e+03 5.5900e+02 8.7600e+02\n",
      " 1.3340e+03 3.2100e+02 1.2380e+03 8.5200e+02 7.1900e+02 8.9200e+02\n",
      " 7.2290e+03 6.5800e+02 1.9660e+03 3.1900e+02 3.3100e+02 1.8680e+03\n",
      " 1.0260e+03 1.6170e+03 1.7210e+03 5.6800e+02 2.2000e+02 6.2900e+02\n",
      " 4.8600e+02 7.3300e+02 1.1200e+03 3.2280e+03 3.1530e+03 1.1650e+03\n",
      " 7.8300e+02 4.0090e+03 2.3420e+03 1.1060e+03 7.1300e+02 2.2100e+02\n",
      " 3.0500e+02 5.1900e+02 9.0100e+02 4.5400e+02 3.5800e+02 3.9300e+02\n",
      " 1.2410e+03 8.4900e+02 3.8600e+03 1.1300e+02 3.7400e+02 7.8900e+02\n",
      " 7.5800e+02 1.0720e+03 2.4200e+02 2.6980e+03 1.3390e+03 1.6060e+03\n",
      " 6.9460e+03 8.5300e+02 7.9000e+02 2.0670e+03 1.6120e+03 6.9000e+01\n",
      " 9.9000e+01 4.1100e+02 9.7400e+02 1.6200e+02 7.1600e+02 2.2190e+03\n",
      " 4.5500e+02 6.2500e+02 8.4400e+02 1.9050e+03 1.3760e+03 1.9500e+02\n",
      " 2.5670e+03 2.6990e+03 5.8650e+03 1.9700e+02 1.7570e+03 1.5110e+03\n",
      " 3.3300e+02 7.0500e+02 5.2760e+03 6.6400e+02 4.4570e+03 1.2160e+03\n",
      " 1.5000e+02 2.8900e+03 4.2810e+03 1.5180e+03 1.1900e+02 2.6400e+02\n",
      " 7.9400e+02 3.9100e+02 7.7200e+02 5.1820e+03 1.4600e+02 1.6160e+03\n",
      " 2.4900e+02 1.3020e+03 7.5400e+02 1.0700e+03 1.0360e+03 8.0500e+02\n",
      " 7.3400e+02 1.9220e+03 2.5620e+03 1.2260e+03 2.7000e+02 4.5740e+03\n",
      " 1.5620e+03 3.6490e+03 1.4850e+03 2.7240e+03 3.1600e+02 3.9700e+02\n",
      " 1.2430e+03 1.7600e+02 1.8130e+03 1.2113e+04 2.2720e+03 3.5000e+01\n",
      " 5.4300e+02 4.8100e+02 1.7500e+02 1.8900e+02 9.0600e+02 6.0020e+03\n",
      " 1.1440e+03 3.0000e+03 3.3800e+02 9.2800e+02 9.9800e+02 2.1460e+03\n",
      " 2.6280e+03 4.6170e+03 1.9950e+03 3.1300e+02 9.6300e+02 1.8410e+03\n",
      " 8.6700e+02 1.7200e+03 2.6500e+03 3.1080e+03 3.0930e+03 3.3400e+02\n",
      " 1.4834e+04 4.9400e+02 6.1800e+02 5.2300e+02]\n",
      "유니크한 카테고리 수 :  796\n",
      "null 수              :  0 \n",
      "\n",
      "group \n",
      " [  13   18   19 ... 9271 9273 9277]\n",
      "유니크한 카테고리 수 :  3063\n",
      "null 수              :  0 \n",
      "\n",
      "last_name \n",
      " ['Carsoning' 'Peckers' 'Unhearfus' ... 'Roses' 'Pumpeshaft' 'Cure']\n",
      "유니크한 카테고리 수 :  1725\n",
      "null 수              :  0 \n",
      "\n",
      "spend_fillna_0 \n",
      " [   0. 2832. 7418. ... 5478. 1818. 3898.]\n",
      "유니크한 카테고리 수 :  1437\n",
      "null 수              :  0 \n",
      "\n",
      "VIP_fill_na_False \n",
      " [False  True]\n",
      "유니크한 카테고리 수 :  2\n",
      "null 수              :  0 \n",
      "\n",
      "deck \n",
      " ['G' 'F' 'C' 'B' 'D' 'E' 'A' 'T']\n",
      "유니크한 카테고리 수 :  8\n",
      "null 수              :  0 \n",
      "\n",
      "num \n",
      " [   3    4    0 ... 1890 1794 1503]\n",
      "유니크한 카테고리 수 :  1505\n",
      "null 수              :  0 \n",
      "\n",
      "side \n",
      " ['S' 'P']\n",
      "유니크한 카테고리 수 :  2\n",
      "null 수              :  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in df2.columns :\n",
    "    print(col,\"\\n\", df2[col].unique())\n",
    "    print('유니크한 카테고리 수 : ',len(df2[col].unique()))\n",
    "    print('null 수              : ', df2[col].isna().sum(),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>group</th>\n",
       "      <th>last_name</th>\n",
       "      <th>spend_fillna_0</th>\n",
       "      <th>VIP_fill_na_False</th>\n",
       "      <th>deck</th>\n",
       "      <th>num</th>\n",
       "      <th>side</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Earth</td>\n",
       "      <td>True</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>Carsoning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>G</td>\n",
       "      <td>3</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2823.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>Peckers</td>\n",
       "      <td>2832.0</td>\n",
       "      <td>False</td>\n",
       "      <td>F</td>\n",
       "      <td>4</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Europa</td>\n",
       "      <td>True</td>\n",
       "      <td>55 Cancri e</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>Unhearfus</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6652.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>585.0</td>\n",
       "      <td>21</td>\n",
       "      <td>Caltilter</td>\n",
       "      <td>7418.0</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>635.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>Harperez</td>\n",
       "      <td>645.0</td>\n",
       "      <td>False</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4272</th>\n",
       "      <td>Earth</td>\n",
       "      <td>True</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9266</td>\n",
       "      <td>Peter</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>G</td>\n",
       "      <td>1496</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4273</th>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>9269</td>\n",
       "      <td>Scheron</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>False</td>\n",
       "      <td>D</td>\n",
       "      <td>296</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4274</th>\n",
       "      <td>Mars</td>\n",
       "      <td>True</td>\n",
       "      <td>55 Cancri e</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9271</td>\n",
       "      <td>Pore</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>D</td>\n",
       "      <td>296</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4275</th>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>PSO J318.5-22</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>9273</td>\n",
       "      <td>Conale</td>\n",
       "      <td>3203.0</td>\n",
       "      <td>False</td>\n",
       "      <td>D</td>\n",
       "      <td>297</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>Earth</td>\n",
       "      <td>True</td>\n",
       "      <td>PSO J318.5-22</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9277</td>\n",
       "      <td>Leonzaley</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>G</td>\n",
       "      <td>1498</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4277 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     HomePlanet CryoSleep    Destination   Age  RoomService  FoodCourt  \\\n",
       "0         Earth      True    TRAPPIST-1e  27.0          0.0        0.0   \n",
       "1         Earth     False    TRAPPIST-1e  19.0          0.0        9.0   \n",
       "2        Europa      True    55 Cancri e  31.0          0.0        0.0   \n",
       "3        Europa     False    TRAPPIST-1e  38.0          0.0     6652.0   \n",
       "4         Earth     False    TRAPPIST-1e  20.0         10.0        0.0   \n",
       "...         ...       ...            ...   ...          ...        ...   \n",
       "4272      Earth      True    TRAPPIST-1e  34.0          0.0        0.0   \n",
       "4273      Earth     False    TRAPPIST-1e  42.0          0.0      847.0   \n",
       "4274       Mars      True    55 Cancri e  27.0          0.0        0.0   \n",
       "4275     Europa     False  PSO J318.5-22  27.0          0.0     2680.0   \n",
       "4276      Earth      True  PSO J318.5-22  43.0          0.0        0.0   \n",
       "\n",
       "      ShoppingMall     Spa  VRDeck  group  last_name  spend_fillna_0  \\\n",
       "0              0.0     0.0     0.0     13  Carsoning             0.0   \n",
       "1              0.0  2823.0     0.0     18    Peckers          2832.0   \n",
       "2              0.0     0.0     0.0     19  Unhearfus             0.0   \n",
       "3              0.0   181.0   585.0     21  Caltilter          7418.0   \n",
       "4            635.0     0.0     0.0     23   Harperez           645.0   \n",
       "...            ...     ...     ...    ...        ...             ...   \n",
       "4272           0.0     0.0     0.0   9266      Peter             0.0   \n",
       "4273          17.0    10.0   144.0   9269    Scheron          1018.0   \n",
       "4274           0.0     0.0     0.0   9271       Pore             0.0   \n",
       "4275           0.0     0.0   523.0   9273     Conale          3203.0   \n",
       "4276           0.0     0.0     0.0   9277  Leonzaley             0.0   \n",
       "\n",
       "      VIP_fill_na_False deck   num side  \n",
       "0                 False    G     3    S  \n",
       "1                 False    F     4    S  \n",
       "2                 False    C     0    S  \n",
       "3                 False    C     1    S  \n",
       "4                 False    F     5    S  \n",
       "...                 ...  ...   ...  ...  \n",
       "4272              False    G  1496    S  \n",
       "4273              False    D   296    P  \n",
       "4274              False    D   296    P  \n",
       "4275              False    D   297    P  \n",
       "4276              False    G  1498    S  \n",
       "\n",
       "[4277 rows x 16 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder_col = ['HomePlanet', 'CryoSleep', 'Destination','last_name','VIP_fill_na_False', 'deck','side']\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# LabelEncoder를 객체로 생성\n",
    "encoder_HomePlanet = LabelEncoder()\n",
    "# fit, transform 메소드를 통한 레이블 인코딩\n",
    "df['HomePlanet'] = encoder_HomePlanet.fit_transform(df['HomePlanet'])\n",
    "df2['HomePlanet'] = encoder_HomePlanet.transform(df2['HomePlanet'])\n",
    "\n",
    "# LabelEncoder를 객체로 생성\n",
    "encoder_CryoSleep = LabelEncoder()\n",
    "# fit, transform 메소드를 통한 레이블 인코딩\n",
    "df['CryoSleep'] = encoder_CryoSleep.fit_transform(df['CryoSleep'])\n",
    "df2['CryoSleep'] = encoder_CryoSleep.transform(df2['CryoSleep'])\n",
    "\n",
    "# LabelEncoder를 객체로 생성\n",
    "encoder_Destination = LabelEncoder()\n",
    "# fit, transform 메소드를 통한 레이블 인코딩\n",
    "df['Destination'] = encoder_Destination.fit_transform(df['Destination'])\n",
    "df2['Destination'] = encoder_Destination.transform(df2['Destination'])\n",
    "\n",
    "# LabelEncoder를 객체로 생성\n",
    "encoder_last_name = LabelEncoder()\n",
    "# fit, transform 메소드를 통한 레이블 인코딩\n",
    "df['last_name'] = encoder_last_name.fit_transform(df['last_name'])\n",
    "df2['last_name'] = df2['last_name'].apply(lambda x: encoder_last_name.transform([x])[0] if x in encoder_last_name.classes_ else 0)\n",
    "\n",
    "# LabelEncoder를 객체로 생성\n",
    "encoder_deck = LabelEncoder()\n",
    "# fit, transform 메소드를 통한 레이블 인코딩\n",
    "df['deck'] = encoder_deck.fit_transform(df['deck'])\n",
    "df2['deck'] = encoder_deck.transform(df2['deck'])\n",
    "\n",
    "encoder_side = LabelEncoder()\n",
    "# fit, transform 메소드를 통한 레이블 인코딩\n",
    "df['side'] = encoder_side.fit_transform(df['side'])\n",
    "df2['side'] = encoder_side.transform(df2['side'])\n",
    "\n",
    "\n",
    "df['VIP_fill_na_False'] = df['VIP_fill_na_False'].astype(int)\n",
    "df2['VIP_fill_na_False'] = df2['VIP_fill_na_False'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Transported</th>\n",
       "      <th>group</th>\n",
       "      <th>last_name</th>\n",
       "      <th>spend_fillna_0</th>\n",
       "      <th>VIP_fill_na_False</th>\n",
       "      <th>deck</th>\n",
       "      <th>num</th>\n",
       "      <th>side</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HomePlanet</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.088953</td>\n",
       "      <td>0.039393</td>\n",
       "      <td>0.132723</td>\n",
       "      <td>0.212442</td>\n",
       "      <td>0.068821</td>\n",
       "      <td>0.101507</td>\n",
       "      <td>0.053451</td>\n",
       "      <td>0.040193</td>\n",
       "      <td>0.116881</td>\n",
       "      <td>-0.004601</td>\n",
       "      <td>0.004435</td>\n",
       "      <td>0.148622</td>\n",
       "      <td>0.124166</td>\n",
       "      <td>-0.410312</td>\n",
       "      <td>-0.198557</td>\n",
       "      <td>0.001229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CryoSleep</th>\n",
       "      <td>0.088953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.094802</td>\n",
       "      <td>-0.077806</td>\n",
       "      <td>-0.250173</td>\n",
       "      <td>-0.211061</td>\n",
       "      <td>-0.212977</td>\n",
       "      <td>-0.203249</td>\n",
       "      <td>-0.197524</td>\n",
       "      <td>0.467230</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>0.014009</td>\n",
       "      <td>-0.386081</td>\n",
       "      <td>-0.081318</td>\n",
       "      <td>0.016910</td>\n",
       "      <td>-0.036606</td>\n",
       "      <td>0.018982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Destination</th>\n",
       "      <td>0.039393</td>\n",
       "      <td>-0.094802</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005473</td>\n",
       "      <td>0.046382</td>\n",
       "      <td>-0.110375</td>\n",
       "      <td>0.025525</td>\n",
       "      <td>-0.056524</td>\n",
       "      <td>-0.072374</td>\n",
       "      <td>-0.110135</td>\n",
       "      <td>-0.003366</td>\n",
       "      <td>-0.015588</td>\n",
       "      <td>-0.098442</td>\n",
       "      <td>-0.044469</td>\n",
       "      <td>0.187318</td>\n",
       "      <td>0.102247</td>\n",
       "      <td>-0.011132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.132723</td>\n",
       "      <td>-0.077806</td>\n",
       "      <td>-0.005473</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068629</td>\n",
       "      <td>0.127390</td>\n",
       "      <td>0.033148</td>\n",
       "      <td>0.120946</td>\n",
       "      <td>0.099590</td>\n",
       "      <td>-0.074233</td>\n",
       "      <td>-0.009099</td>\n",
       "      <td>0.005079</td>\n",
       "      <td>0.184628</td>\n",
       "      <td>0.091863</td>\n",
       "      <td>-0.244236</td>\n",
       "      <td>-0.132380</td>\n",
       "      <td>0.010871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoomService</th>\n",
       "      <td>0.212442</td>\n",
       "      <td>-0.250173</td>\n",
       "      <td>0.046382</td>\n",
       "      <td>0.068629</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015126</td>\n",
       "      <td>0.052337</td>\n",
       "      <td>0.009244</td>\n",
       "      <td>-0.018624</td>\n",
       "      <td>-0.241124</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>-0.016133</td>\n",
       "      <td>0.234374</td>\n",
       "      <td>0.056566</td>\n",
       "      <td>-0.030345</td>\n",
       "      <td>-0.013001</td>\n",
       "      <td>-0.009727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FoodCourt</th>\n",
       "      <td>0.068821</td>\n",
       "      <td>-0.211061</td>\n",
       "      <td>-0.110375</td>\n",
       "      <td>0.127390</td>\n",
       "      <td>-0.015126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.013717</td>\n",
       "      <td>0.221468</td>\n",
       "      <td>0.224572</td>\n",
       "      <td>0.045583</td>\n",
       "      <td>-0.009200</td>\n",
       "      <td>0.012275</td>\n",
       "      <td>0.742608</td>\n",
       "      <td>0.125499</td>\n",
       "      <td>-0.317544</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>0.016528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ShoppingMall</th>\n",
       "      <td>0.101507</td>\n",
       "      <td>-0.212977</td>\n",
       "      <td>0.025525</td>\n",
       "      <td>0.033148</td>\n",
       "      <td>0.052337</td>\n",
       "      <td>-0.013717</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014542</td>\n",
       "      <td>-0.007849</td>\n",
       "      <td>0.009391</td>\n",
       "      <td>0.017796</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.220529</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>-0.030913</td>\n",
       "      <td>0.002405</td>\n",
       "      <td>-0.018390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spa</th>\n",
       "      <td>0.053451</td>\n",
       "      <td>-0.203249</td>\n",
       "      <td>-0.056524</td>\n",
       "      <td>0.120946</td>\n",
       "      <td>0.009244</td>\n",
       "      <td>0.221468</td>\n",
       "      <td>0.014542</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.147658</td>\n",
       "      <td>-0.218545</td>\n",
       "      <td>-0.005198</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.592656</td>\n",
       "      <td>0.060991</td>\n",
       "      <td>-0.222740</td>\n",
       "      <td>-0.129796</td>\n",
       "      <td>0.002587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VRDeck</th>\n",
       "      <td>0.040193</td>\n",
       "      <td>-0.197524</td>\n",
       "      <td>-0.072374</td>\n",
       "      <td>0.099590</td>\n",
       "      <td>-0.018624</td>\n",
       "      <td>0.224572</td>\n",
       "      <td>-0.007849</td>\n",
       "      <td>0.147658</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.204874</td>\n",
       "      <td>0.015945</td>\n",
       "      <td>0.007743</td>\n",
       "      <td>0.585684</td>\n",
       "      <td>0.123061</td>\n",
       "      <td>-0.251901</td>\n",
       "      <td>-0.134377</td>\n",
       "      <td>-0.009258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transported</th>\n",
       "      <td>0.116881</td>\n",
       "      <td>0.467230</td>\n",
       "      <td>-0.110135</td>\n",
       "      <td>-0.074233</td>\n",
       "      <td>-0.241124</td>\n",
       "      <td>0.045583</td>\n",
       "      <td>0.009391</td>\n",
       "      <td>-0.218545</td>\n",
       "      <td>-0.204874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021491</td>\n",
       "      <td>0.015370</td>\n",
       "      <td>-0.199514</td>\n",
       "      <td>-0.037261</td>\n",
       "      <td>-0.117590</td>\n",
       "      <td>-0.046512</td>\n",
       "      <td>0.103359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <td>-0.004601</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>-0.003366</td>\n",
       "      <td>-0.009099</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>-0.009200</td>\n",
       "      <td>0.017796</td>\n",
       "      <td>-0.005198</td>\n",
       "      <td>0.015945</td>\n",
       "      <td>0.021491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.014035</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.013608</td>\n",
       "      <td>-0.000370</td>\n",
       "      <td>0.680925</td>\n",
       "      <td>-0.006824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_name</th>\n",
       "      <td>0.004435</td>\n",
       "      <td>0.014009</td>\n",
       "      <td>-0.015588</td>\n",
       "      <td>0.005079</td>\n",
       "      <td>-0.016133</td>\n",
       "      <td>0.012275</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.007743</td>\n",
       "      <td>0.015370</td>\n",
       "      <td>-0.014035</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007172</td>\n",
       "      <td>-0.000562</td>\n",
       "      <td>-0.022861</td>\n",
       "      <td>-0.022568</td>\n",
       "      <td>0.024559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spend_fillna_0</th>\n",
       "      <td>0.148622</td>\n",
       "      <td>-0.386081</td>\n",
       "      <td>-0.098442</td>\n",
       "      <td>0.184628</td>\n",
       "      <td>0.234374</td>\n",
       "      <td>0.742608</td>\n",
       "      <td>0.220529</td>\n",
       "      <td>0.592656</td>\n",
       "      <td>0.585684</td>\n",
       "      <td>-0.199514</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.007172</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.162987</td>\n",
       "      <td>-0.385887</td>\n",
       "      <td>-0.211337</td>\n",
       "      <td>0.000487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIP_fill_na_False</th>\n",
       "      <td>0.124166</td>\n",
       "      <td>-0.081318</td>\n",
       "      <td>-0.044469</td>\n",
       "      <td>0.091863</td>\n",
       "      <td>0.056566</td>\n",
       "      <td>0.125499</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>0.060991</td>\n",
       "      <td>0.123061</td>\n",
       "      <td>-0.037261</td>\n",
       "      <td>0.013608</td>\n",
       "      <td>-0.000562</td>\n",
       "      <td>0.162987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.181667</td>\n",
       "      <td>-0.098585</td>\n",
       "      <td>-0.008279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deck</th>\n",
       "      <td>-0.410312</td>\n",
       "      <td>0.016910</td>\n",
       "      <td>0.187318</td>\n",
       "      <td>-0.244236</td>\n",
       "      <td>-0.030345</td>\n",
       "      <td>-0.317544</td>\n",
       "      <td>-0.030913</td>\n",
       "      <td>-0.222740</td>\n",
       "      <td>-0.251901</td>\n",
       "      <td>-0.117590</td>\n",
       "      <td>-0.000370</td>\n",
       "      <td>-0.022861</td>\n",
       "      <td>-0.385887</td>\n",
       "      <td>-0.181667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.531690</td>\n",
       "      <td>-0.034944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num</th>\n",
       "      <td>-0.198557</td>\n",
       "      <td>-0.036606</td>\n",
       "      <td>0.102247</td>\n",
       "      <td>-0.132380</td>\n",
       "      <td>-0.013001</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>0.002405</td>\n",
       "      <td>-0.129796</td>\n",
       "      <td>-0.134377</td>\n",
       "      <td>-0.046512</td>\n",
       "      <td>0.680925</td>\n",
       "      <td>-0.022568</td>\n",
       "      <td>-0.211337</td>\n",
       "      <td>-0.098585</td>\n",
       "      <td>0.531690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.039158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>side</th>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.018982</td>\n",
       "      <td>-0.011132</td>\n",
       "      <td>0.010871</td>\n",
       "      <td>-0.009727</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>-0.018390</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>-0.009258</td>\n",
       "      <td>0.103359</td>\n",
       "      <td>-0.006824</td>\n",
       "      <td>0.024559</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>-0.008279</td>\n",
       "      <td>-0.034944</td>\n",
       "      <td>-0.039158</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   HomePlanet  CryoSleep  Destination       Age  RoomService  \\\n",
       "HomePlanet           1.000000   0.088953     0.039393  0.132723     0.212442   \n",
       "CryoSleep            0.088953   1.000000    -0.094802 -0.077806    -0.250173   \n",
       "Destination          0.039393  -0.094802     1.000000 -0.005473     0.046382   \n",
       "Age                  0.132723  -0.077806    -0.005473  1.000000     0.068629   \n",
       "RoomService          0.212442  -0.250173     0.046382  0.068629     1.000000   \n",
       "FoodCourt            0.068821  -0.211061    -0.110375  0.127390    -0.015126   \n",
       "ShoppingMall         0.101507  -0.212977     0.025525  0.033148     0.052337   \n",
       "Spa                  0.053451  -0.203249    -0.056524  0.120946     0.009244   \n",
       "VRDeck               0.040193  -0.197524    -0.072374  0.099590    -0.018624   \n",
       "Transported          0.116881   0.467230    -0.110135 -0.074233    -0.241124   \n",
       "group               -0.004601  -0.004304    -0.003366 -0.009099     0.000375   \n",
       "last_name            0.004435   0.014009    -0.015588  0.005079    -0.016133   \n",
       "spend_fillna_0       0.148622  -0.386081    -0.098442  0.184628     0.234374   \n",
       "VIP_fill_na_False    0.124166  -0.081318    -0.044469  0.091863     0.056566   \n",
       "deck                -0.410312   0.016910     0.187318 -0.244236    -0.030345   \n",
       "num                 -0.198557  -0.036606     0.102247 -0.132380    -0.013001   \n",
       "side                 0.001229   0.018982    -0.011132  0.010871    -0.009727   \n",
       "\n",
       "                   FoodCourt  ShoppingMall       Spa    VRDeck  Transported  \\\n",
       "HomePlanet          0.068821      0.101507  0.053451  0.040193     0.116881   \n",
       "CryoSleep          -0.211061     -0.212977 -0.203249 -0.197524     0.467230   \n",
       "Destination        -0.110375      0.025525 -0.056524 -0.072374    -0.110135   \n",
       "Age                 0.127390      0.033148  0.120946  0.099590    -0.074233   \n",
       "RoomService        -0.015126      0.052337  0.009244 -0.018624    -0.241124   \n",
       "FoodCourt           1.000000     -0.013717  0.221468  0.224572     0.045583   \n",
       "ShoppingMall       -0.013717      1.000000  0.014542 -0.007849     0.009391   \n",
       "Spa                 0.221468      0.014542  1.000000  0.147658    -0.218545   \n",
       "VRDeck              0.224572     -0.007849  0.147658  1.000000    -0.204874   \n",
       "Transported         0.045583      0.009391 -0.218545 -0.204874     1.000000   \n",
       "group              -0.009200      0.017796 -0.005198  0.015945     0.021491   \n",
       "last_name           0.012275      0.001924  0.001100  0.007743     0.015370   \n",
       "spend_fillna_0      0.742608      0.220529  0.592656  0.585684    -0.199514   \n",
       "VIP_fill_na_False   0.125499      0.018412  0.060991  0.123061    -0.037261   \n",
       "deck               -0.317544     -0.030913 -0.222740 -0.251901    -0.117590   \n",
       "num                -0.179688      0.002405 -0.129796 -0.134377    -0.046512   \n",
       "side                0.016528     -0.018390  0.002587 -0.009258     0.103359   \n",
       "\n",
       "                      group  last_name  spend_fillna_0  VIP_fill_na_False  \\\n",
       "HomePlanet        -0.004601   0.004435        0.148622           0.124166   \n",
       "CryoSleep         -0.004304   0.014009       -0.386081          -0.081318   \n",
       "Destination       -0.003366  -0.015588       -0.098442          -0.044469   \n",
       "Age               -0.009099   0.005079        0.184628           0.091863   \n",
       "RoomService        0.000375  -0.016133        0.234374           0.056566   \n",
       "FoodCourt         -0.009200   0.012275        0.742608           0.125499   \n",
       "ShoppingMall       0.017796   0.001924        0.220529           0.018412   \n",
       "Spa               -0.005198   0.001100        0.592656           0.060991   \n",
       "VRDeck             0.015945   0.007743        0.585684           0.123061   \n",
       "Transported        0.021491   0.015370       -0.199514          -0.037261   \n",
       "group              1.000000  -0.014035        0.003012           0.013608   \n",
       "last_name         -0.014035   1.000000        0.007172          -0.000562   \n",
       "spend_fillna_0     0.003012   0.007172        1.000000           0.162987   \n",
       "VIP_fill_na_False  0.013608  -0.000562        0.162987           1.000000   \n",
       "deck              -0.000370  -0.022861       -0.385887          -0.181667   \n",
       "num                0.680925  -0.022568       -0.211337          -0.098585   \n",
       "side              -0.006824   0.024559        0.000487          -0.008279   \n",
       "\n",
       "                       deck       num      side  \n",
       "HomePlanet        -0.410312 -0.198557  0.001229  \n",
       "CryoSleep          0.016910 -0.036606  0.018982  \n",
       "Destination        0.187318  0.102247 -0.011132  \n",
       "Age               -0.244236 -0.132380  0.010871  \n",
       "RoomService       -0.030345 -0.013001 -0.009727  \n",
       "FoodCourt         -0.317544 -0.179688  0.016528  \n",
       "ShoppingMall      -0.030913  0.002405 -0.018390  \n",
       "Spa               -0.222740 -0.129796  0.002587  \n",
       "VRDeck            -0.251901 -0.134377 -0.009258  \n",
       "Transported       -0.117590 -0.046512  0.103359  \n",
       "group             -0.000370  0.680925 -0.006824  \n",
       "last_name         -0.022861 -0.022568  0.024559  \n",
       "spend_fillna_0    -0.385887 -0.211337  0.000487  \n",
       "VIP_fill_na_False -0.181667 -0.098585 -0.008279  \n",
       "deck               1.000000  0.531690 -0.034944  \n",
       "num                0.531690  1.000000 -0.039158  \n",
       "side              -0.034944 -0.039158  1.000000  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>group</th>\n",
       "      <th>last_name</th>\n",
       "      <th>spend_fillna_0</th>\n",
       "      <th>VIP_fill_na_False</th>\n",
       "      <th>deck</th>\n",
       "      <th>num</th>\n",
       "      <th>side</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2823.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1505</td>\n",
       "      <td>2832.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>2070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6652.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>585.0</td>\n",
       "      <td>21</td>\n",
       "      <td>324</td>\n",
       "      <td>7418.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>635.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>926</td>\n",
       "      <td>645.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4272</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9266</td>\n",
       "      <td>1531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1496</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4273</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>9269</td>\n",
       "      <td>1791</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4274</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9271</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4275</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>9273</td>\n",
       "      <td>457</td>\n",
       "      <td>3203.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>297</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9277</td>\n",
       "      <td>1190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1498</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4277 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HomePlanet  CryoSleep  Destination   Age  RoomService  FoodCourt  \\\n",
       "0              0          1            2  27.0          0.0        0.0   \n",
       "1              0          0            2  19.0          0.0        9.0   \n",
       "2              1          1            0  31.0          0.0        0.0   \n",
       "3              1          0            2  38.0          0.0     6652.0   \n",
       "4              0          0            2  20.0         10.0        0.0   \n",
       "...          ...        ...          ...   ...          ...        ...   \n",
       "4272           0          1            2  34.0          0.0        0.0   \n",
       "4273           0          0            2  42.0          0.0      847.0   \n",
       "4274           2          1            0  27.0          0.0        0.0   \n",
       "4275           1          0            1  27.0          0.0     2680.0   \n",
       "4276           0          1            1  43.0          0.0        0.0   \n",
       "\n",
       "      ShoppingMall     Spa  VRDeck  group  last_name  spend_fillna_0  \\\n",
       "0              0.0     0.0     0.0     13        344             0.0   \n",
       "1              0.0  2823.0     0.0     18       1505          2832.0   \n",
       "2              0.0     0.0     0.0     19       2070             0.0   \n",
       "3              0.0   181.0   585.0     21        324          7418.0   \n",
       "4            635.0     0.0     0.0     23        926           645.0   \n",
       "...            ...     ...     ...    ...        ...             ...   \n",
       "4272           0.0     0.0     0.0   9266       1531             0.0   \n",
       "4273          17.0    10.0   144.0   9269       1791          1018.0   \n",
       "4274           0.0     0.0     0.0   9271          0             0.0   \n",
       "4275           0.0     0.0   523.0   9273        457          3203.0   \n",
       "4276           0.0     0.0     0.0   9277       1190             0.0   \n",
       "\n",
       "      VIP_fill_na_False  deck   num  side  \n",
       "0                     0     6     3     1  \n",
       "1                     0     5     4     1  \n",
       "2                     0     2     0     1  \n",
       "3                     0     2     1     1  \n",
       "4                     0     5     5     1  \n",
       "...                 ...   ...   ...   ...  \n",
       "4272                  0     6  1496     1  \n",
       "4273                  0     3   296     0  \n",
       "4274                  0     3   296     0  \n",
       "4275                  0     3   297     0  \n",
       "4276                  0     6  1498     1  \n",
       "\n",
       "[4277 rows x 16 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Transported'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3802\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3803\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Transported'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-13b52043be8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Transported'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Transported'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3806\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3807\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3808\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3803\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3804\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3805\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3806\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Transported'"
     ]
    }
   ],
   "source": [
    "# col = ['HomePlanet', 'CryoSleep', 'Destination', 'Age', 'RoomService',\n",
    "#        'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'group', 'last_name',\n",
    "#        'spend_fillna_0', 'VIP_fill_na_False', 'deck', 'num', 'side']\n",
    "\n",
    "\n",
    "y = df['Transported']\n",
    "df = df.drop('Transported', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col = ['HomePlanet', 'CryoSleep', 'Destination', 'Age', 'RoomService',\n",
    "#        'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'group', 'last_name',\n",
    "#        'spend_fillna_0', 'VIP_fill_na_False', 'deck', 'num', 'side']\n",
    "\n",
    "col = ['HomePlanet', 'CryoSleep', 'Destination', 'Age', 'RoomService',\n",
    "       'FoodCourt',  'Spa', 'VRDeck', 'group', 'last_name',\n",
    "       'spend_fillna_0', 'VIP_fill_na_False', 'deck', 'num', 'side']\n",
    "\n",
    "df_ = df.loc[:,col]\n",
    "df2_ = df2.loc[:,col]\n",
    "\n",
    "X_train = np.array(df_)\n",
    "y_train = np.array(y)\n",
    "\n",
    "X_test = np.array(df2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>group</th>\n",
       "      <th>last_name</th>\n",
       "      <th>spend_fillna_0</th>\n",
       "      <th>VIP_fill_na_False</th>\n",
       "      <th>deck</th>\n",
       "      <th>num</th>\n",
       "      <th>side</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2109</td>\n",
       "      <td>736.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>58.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1990</td>\n",
       "      <td>10383.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1990</td>\n",
       "      <td>5176.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1778</td>\n",
       "      <td>1091.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8688</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6819.0</td>\n",
       "      <td>1643.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9276</td>\n",
       "      <td>1416</td>\n",
       "      <td>8536.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8689</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9278</td>\n",
       "      <td>1341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1499</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8690</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9279</td>\n",
       "      <td>470</td>\n",
       "      <td>1873.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8691</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>3235.0</td>\n",
       "      <td>9280</td>\n",
       "      <td>996</td>\n",
       "      <td>4637.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>44.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>4688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9280</td>\n",
       "      <td>996</td>\n",
       "      <td>4826.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8693 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HomePlanet  CryoSleep  Destination   Age  RoomService  FoodCourt  \\\n",
       "0              1          0            2  39.0          0.0        0.0   \n",
       "1              0          0            2  24.0        109.0        9.0   \n",
       "2              1          0            2  58.0         43.0     3576.0   \n",
       "3              1          0            2  33.0          0.0     1283.0   \n",
       "4              0          0            2  16.0        303.0       70.0   \n",
       "...          ...        ...          ...   ...          ...        ...   \n",
       "8688           1          0            0  41.0          0.0     6819.0   \n",
       "8689           0          1            1  18.0          0.0        0.0   \n",
       "8690           0          0            2  26.0          0.0        0.0   \n",
       "8691           1          0            0  32.0          0.0     1049.0   \n",
       "8692           1          0            2  44.0        126.0     4688.0   \n",
       "\n",
       "         Spa  VRDeck  group  last_name  spend_fillna_0  VIP_fill_na_False  \\\n",
       "0        0.0     0.0      1       1431             0.0                  0   \n",
       "1      549.0    44.0      2       2109           736.0                  0   \n",
       "2     6715.0    49.0      3       1990         10383.0                  1   \n",
       "3     3329.0   193.0      3       1990          5176.0                  0   \n",
       "4      565.0     2.0      4       1778          1091.0                  0   \n",
       "...      ...     ...    ...        ...             ...                ...   \n",
       "8688  1643.0    74.0   9276       1416          8536.0                  1   \n",
       "8689     0.0     0.0   9278       1341             0.0                  0   \n",
       "8690     1.0     0.0   9279        470          1873.0                  0   \n",
       "8691   353.0  3235.0   9280        996          4637.0                  0   \n",
       "8692     0.0    12.0   9280        996          4826.0                  0   \n",
       "\n",
       "      deck   num  side  \n",
       "0        1     0     0  \n",
       "1        5     0     1  \n",
       "2        0     0     1  \n",
       "3        0     0     1  \n",
       "4        5     1     1  \n",
       "...    ...   ...   ...  \n",
       "8688     0    98     0  \n",
       "8689     6  1499     1  \n",
       "8690     6  1500     1  \n",
       "8691     4   608     1  \n",
       "8692     4   608     1  \n",
       "\n",
       "[8693 rows x 15 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4277, 15)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    " \n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler() \n",
    "\n",
    "scaler.fit(X_train) \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=12) # 주성분을 몇개로 할지 결정\n",
    "# X_train_pca = pca.fit_transform(X_train)\n",
    "# X_test_pca = pca.transform(X_test)\n",
    "# print(np.cumsum(pca.explained_variance_ratio_)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,train_size=0.8,random_state=32, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6954, 15)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 왜 AGE를 빼야 학습이 진행되는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "435/435 [==============================] - 2s 2ms/step - loss: 0.8260 - binary_accuracy: 0.6483 - val_loss: 0.6596 - val_binary_accuracy: 0.7683\n",
      "Epoch 2/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.7149 - binary_accuracy: 0.7183 - val_loss: 0.6231 - val_binary_accuracy: 0.7752\n",
      "Epoch 3/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.6810 - binary_accuracy: 0.7366 - val_loss: 0.6017 - val_binary_accuracy: 0.7832\n",
      "Epoch 4/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.6462 - binary_accuracy: 0.7453 - val_loss: 0.5816 - val_binary_accuracy: 0.7826\n",
      "Epoch 5/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.6255 - binary_accuracy: 0.7606 - val_loss: 0.5587 - val_binary_accuracy: 0.7895\n",
      "Epoch 6/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.6102 - binary_accuracy: 0.7627 - val_loss: 0.5477 - val_binary_accuracy: 0.7890\n",
      "Epoch 7/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.5852 - binary_accuracy: 0.7708 - val_loss: 0.5286 - val_binary_accuracy: 0.7959\n",
      "Epoch 8/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.5745 - binary_accuracy: 0.7669 - val_loss: 0.5182 - val_binary_accuracy: 0.7964\n",
      "Epoch 9/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.5657 - binary_accuracy: 0.7692 - val_loss: 0.5070 - val_binary_accuracy: 0.7947\n",
      "Epoch 10/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.5470 - binary_accuracy: 0.7796 - val_loss: 0.4960 - val_binary_accuracy: 0.7964\n",
      "Epoch 11/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.5412 - binary_accuracy: 0.7747 - val_loss: 0.4922 - val_binary_accuracy: 0.7999\n",
      "Epoch 12/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.5267 - binary_accuracy: 0.7826 - val_loss: 0.4839 - val_binary_accuracy: 0.7976\n",
      "Epoch 13/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.5231 - binary_accuracy: 0.7804 - val_loss: 0.4827 - val_binary_accuracy: 0.7976\n",
      "Epoch 14/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.5162 - binary_accuracy: 0.7803 - val_loss: 0.4767 - val_binary_accuracy: 0.8016\n",
      "Epoch 15/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.5127 - binary_accuracy: 0.7816 - val_loss: 0.4715 - val_binary_accuracy: 0.7941\n",
      "Epoch 16/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.5103 - binary_accuracy: 0.7797 - val_loss: 0.4713 - val_binary_accuracy: 0.7936\n",
      "Epoch 17/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.5066 - binary_accuracy: 0.7873 - val_loss: 0.4717 - val_binary_accuracy: 0.8005\n",
      "Epoch 18/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4958 - binary_accuracy: 0.7856 - val_loss: 0.4599 - val_binary_accuracy: 0.8022\n",
      "Epoch 19/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.5017 - binary_accuracy: 0.7819 - val_loss: 0.4568 - val_binary_accuracy: 0.7982\n",
      "Epoch 20/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4914 - binary_accuracy: 0.7882 - val_loss: 0.4572 - val_binary_accuracy: 0.7987\n",
      "Epoch 21/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4926 - binary_accuracy: 0.7850 - val_loss: 0.4560 - val_binary_accuracy: 0.8045\n",
      "Epoch 22/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4896 - binary_accuracy: 0.7837 - val_loss: 0.4525 - val_binary_accuracy: 0.7964\n",
      "Epoch 23/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4874 - binary_accuracy: 0.7863 - val_loss: 0.4500 - val_binary_accuracy: 0.8005\n",
      "Epoch 24/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4826 - binary_accuracy: 0.7885 - val_loss: 0.4473 - val_binary_accuracy: 0.7947\n",
      "Epoch 25/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4882 - binary_accuracy: 0.7850 - val_loss: 0.4519 - val_binary_accuracy: 0.8028\n",
      "Epoch 26/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4811 - binary_accuracy: 0.7919 - val_loss: 0.4495 - val_binary_accuracy: 0.7924\n",
      "Epoch 27/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4771 - binary_accuracy: 0.7948 - val_loss: 0.4386 - val_binary_accuracy: 0.8068\n",
      "Epoch 28/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4809 - binary_accuracy: 0.7892 - val_loss: 0.4466 - val_binary_accuracy: 0.7976\n",
      "Epoch 29/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4806 - binary_accuracy: 0.7834 - val_loss: 0.4457 - val_binary_accuracy: 0.7924\n",
      "Epoch 30/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4831 - binary_accuracy: 0.7839 - val_loss: 0.4552 - val_binary_accuracy: 0.7982\n",
      "Epoch 31/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4787 - binary_accuracy: 0.7810 - val_loss: 0.4412 - val_binary_accuracy: 0.7959\n",
      "Epoch 32/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4742 - binary_accuracy: 0.7915 - val_loss: 0.4385 - val_binary_accuracy: 0.7999\n",
      "Epoch 33/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4766 - binary_accuracy: 0.7849 - val_loss: 0.4349 - val_binary_accuracy: 0.7976\n",
      "Epoch 34/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4785 - binary_accuracy: 0.7857 - val_loss: 0.4393 - val_binary_accuracy: 0.8045\n",
      "Epoch 35/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4759 - binary_accuracy: 0.7892 - val_loss: 0.4289 - val_binary_accuracy: 0.8068\n",
      "Epoch 36/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4708 - binary_accuracy: 0.7912 - val_loss: 0.4347 - val_binary_accuracy: 0.8039\n",
      "Epoch 37/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4734 - binary_accuracy: 0.7875 - val_loss: 0.4462 - val_binary_accuracy: 0.8010\n",
      "Epoch 38/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4762 - binary_accuracy: 0.7888 - val_loss: 0.4416 - val_binary_accuracy: 0.7987\n",
      "Epoch 39/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4748 - binary_accuracy: 0.7892 - val_loss: 0.4437 - val_binary_accuracy: 0.7884\n",
      "Epoch 40/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4756 - binary_accuracy: 0.7862 - val_loss: 0.4399 - val_binary_accuracy: 0.7953\n",
      "Epoch 41/100\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 0.4684 - binary_accuracy: 0.7915 - val_loss: 0.4327 - val_binary_accuracy: 0.8016\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(12, input_shape=(X_train.shape[1],), \n",
    "                       activation='relu', \n",
    "                       kernel_regularizer=tf.keras.regularizers.l1(0.001),\n",
    "                       kernel_initializer='he_normal'\n",
    "                      ))\n",
    "\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(30, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.001),\n",
    "                       kernel_initializer='he_normal'))\n",
    "\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 6, mode = 'auto')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=16,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks = [early_stopping]\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?? 근데 왜 난 여지껏 트레인 비율을 0.2로 한거임?\n",
    "\n",
    "# layer 3층 64,64,1, 배치사이즈 16, 각 레이어에 l1규제 (0.001), 레이어 사이 배치정규화, dropout(0.3) 적용. -> val 성능 79%\n",
    "# layer 4층 8,64,64,1, 배치사이즈 16, 각 레이어에 l1규제 (0.001), 레이어 사이 배치정규화, dropout(0.2) 적용. -> val 성능 79.47%\n",
    "# 쓸데없는 변수는 버려야 하나?\n",
    "# 변수간의 상관성이 아마 매우 클텐데\n",
    "# layer 4층 8,12,12,1, 배치사이즈 16, 각 레이어에 l2규제 (0.001), 레이어 사이 배치정규화, dropout(0.2) 적용. -> val 성능 79.30% 오히려 train loss 매우 흔들림\n",
    "# layer 4층 4,96,64,1, 배치사이즈 16, 각 레이어에 l2규제 (0.001), 레이어 사이 배치정규화, dropout(0.2) 적용. -> val 성능 77.70% train loss, val loss 매우 흔들림\n",
    "# layer 4층 4,64,64,1, 배치사이즈 16, 각 레이어에 l2규제 (0.001), 배치정규화 삭제, dropout(0.2) 적용. -> val 성능 78.84%\n",
    "# layer 4층 4,64,64,1, 배치사이즈 16, 각 레이어에 l2규제 (0.001), 배치정규화 삭제, 헤 가중치 초기화, dropout(0.2) 적용. -> val 성능 79.17% 초반 학습이 안정적\n",
    "# layer 4층 8,64,64,1, 배치사이즈 16, 각 레이어에 l2규제 (0.01), 레이어 사이 배치정규화, dropout(0.2) 적용 , 헤 가중치 초기화, dropout(0.2) 적용. -> val 성능 79.12% \n",
    "# layer 4층 16,64,64,1, 배치사이즈 16, 각 레이어에 l2규제 (0.01), 레이어 사이 배치정규화, dropout(0.2) 적용 , 헤 가중치 초기화, dropout(0.2) 적용. -> val 성능 79.25% \n",
    "\n",
    "# 확실히 input shape를 늘려야 성능이 올라가네. 근데 왜냐 이게.. 차원 축소하면 상관관계 사라지지 않나?\n",
    "\n",
    "# pca적용 -> 8개 차원, 75%  분산 설명\n",
    "# layer 4층 16,64,64,1, 배치사이즈 16, 각 레이어에 l2규제 (0.01), 레이어 사이 배치정규화, dropout(0.2) 적용 , 헤 가중치 초기화, dropout(0.2) 적용. -> val 성능 74.32% \n",
    "\n",
    "# pca적용 -> 12개 차원, 95%  분산 설명\n",
    "# layer 4층 16,64,64,1, 배치사이즈 16, 각 레이어에 l2규제 (0.01), 레이어 사이 배치정규화, dropout(0.2) 적용 , 헤 가중치 초기화, dropout(0.2) 적용. -> val 성능 77.61% 반복시에 성능 상승(79.31)이 보이긴 했으나, 최종적으론 79-77 사이를 오감\n",
    "\n",
    "\n",
    "# 1D cnn 적용 -> 커널 5, 채널 32,32,1, dropout(0.2) -> 78.02 -> 이게 근데 사실 되는게 이상하긴해 ㅋㅋ, val loss 많이 벌어짐\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "516/544 [===========================>..] - ETA: 0s - loss: 0.9043 - binary_accuracy: 0.6773WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.8991 - binary_accuracy: 0.6799\n",
      "Epoch 2/50\n",
      "528/544 [============================>.] - ETA: 0s - loss: 0.7921 - binary_accuracy: 0.7395WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.7917 - binary_accuracy: 0.7400\n",
      "Epoch 3/50\n",
      "533/544 [============================>.] - ETA: 0s - loss: 0.7368 - binary_accuracy: 0.7472WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.7358 - binary_accuracy: 0.7473\n",
      "Epoch 4/50\n",
      "530/544 [============================>.] - ETA: 0s - loss: 0.6915 - binary_accuracy: 0.7600WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.6918 - binary_accuracy: 0.7592\n",
      "Epoch 5/50\n",
      "528/544 [============================>.] - ETA: 0s - loss: 0.6458 - binary_accuracy: 0.7708WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.6450 - binary_accuracy: 0.7707\n",
      "Epoch 6/50\n",
      "529/544 [============================>.] - ETA: 0s - loss: 0.6166 - binary_accuracy: 0.7786WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.6175 - binary_accuracy: 0.7771\n",
      "Epoch 7/50\n",
      "530/544 [============================>.] - ETA: 0s - loss: 0.5869 - binary_accuracy: 0.7821WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.5852 - binary_accuracy: 0.7826\n",
      "Epoch 8/50\n",
      "528/544 [============================>.] - ETA: 0s - loss: 0.5738 - binary_accuracy: 0.7817WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.5750 - binary_accuracy: 0.7810\n",
      "Epoch 9/50\n",
      "520/544 [===========================>..] - ETA: 0s - loss: 0.5554 - binary_accuracy: 0.7787WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.5530 - binary_accuracy: 0.7792\n",
      "Epoch 10/50\n",
      "531/544 [============================>.] - ETA: 0s - loss: 0.5398 - binary_accuracy: 0.7843WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.5405 - binary_accuracy: 0.7836\n",
      "Epoch 11/50\n",
      "531/544 [============================>.] - ETA: 0s - loss: 0.5279 - binary_accuracy: 0.7865WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.5286 - binary_accuracy: 0.7868\n",
      "Epoch 12/50\n",
      "529/544 [============================>.] - ETA: 0s - loss: 0.5153 - binary_accuracy: 0.7854WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.5157 - binary_accuracy: 0.7856\n",
      "Epoch 13/50\n",
      "528/544 [============================>.] - ETA: 0s - loss: 0.5086 - binary_accuracy: 0.7900WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.5114 - binary_accuracy: 0.7879\n",
      "Epoch 14/50\n",
      "524/544 [===========================>..] - ETA: 0s - loss: 0.5037 - binary_accuracy: 0.7852WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.5042 - binary_accuracy: 0.7853\n",
      "Epoch 15/50\n",
      "525/544 [===========================>..] - ETA: 0s - loss: 0.5035 - binary_accuracy: 0.7885WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.5025 - binary_accuracy: 0.7886\n",
      "Epoch 16/50\n",
      "529/544 [============================>.] - ETA: 0s - loss: 0.4970 - binary_accuracy: 0.7942WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4979 - binary_accuracy: 0.7932\n",
      "Epoch 17/50\n",
      "525/544 [===========================>..] - ETA: 0s - loss: 0.4879 - binary_accuracy: 0.7888WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4896 - binary_accuracy: 0.7881\n",
      "Epoch 18/50\n",
      "533/544 [============================>.] - ETA: 0s - loss: 0.4914 - binary_accuracy: 0.7874WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4907 - binary_accuracy: 0.7878\n",
      "Epoch 19/50\n",
      "531/544 [============================>.] - ETA: 0s - loss: 0.4838 - binary_accuracy: 0.7959WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4854 - binary_accuracy: 0.7951\n",
      "Epoch 20/50\n",
      "532/544 [============================>.] - ETA: 0s - loss: 0.4844 - binary_accuracy: 0.7915WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4845 - binary_accuracy: 0.7918\n",
      "Epoch 21/50\n",
      "524/544 [===========================>..] - ETA: 0s - loss: 0.4793 - binary_accuracy: 0.7969WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4792 - binary_accuracy: 0.7963\n",
      "Epoch 22/50\n",
      "525/544 [===========================>..] - ETA: 0s - loss: 0.4843 - binary_accuracy: 0.7930WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4843 - binary_accuracy: 0.7928\n",
      "Epoch 23/50\n",
      "523/544 [===========================>..] - ETA: 0s - loss: 0.4780 - binary_accuracy: 0.7912WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4789 - binary_accuracy: 0.7906\n",
      "Epoch 24/50\n",
      "544/544 [==============================] - ETA: 0s - loss: 0.4792 - binary_accuracy: 0.7957WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4792 - binary_accuracy: 0.7957\n",
      "Epoch 25/50\n",
      "531/544 [============================>.] - ETA: 0s - loss: 0.4777 - binary_accuracy: 0.7888WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4774 - binary_accuracy: 0.7895\n",
      "Epoch 26/50\n",
      "517/544 [===========================>..] - ETA: 0s - loss: 0.4740 - binary_accuracy: 0.7935WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4744 - binary_accuracy: 0.7929\n",
      "Epoch 27/50\n",
      "528/544 [============================>.] - ETA: 0s - loss: 0.4785 - binary_accuracy: 0.7874WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4769 - binary_accuracy: 0.7881\n",
      "Epoch 28/50\n",
      "522/544 [===========================>..] - ETA: 0s - loss: 0.4747 - binary_accuracy: 0.7950WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4742 - binary_accuracy: 0.7958\n",
      "Epoch 29/50\n",
      "526/544 [============================>.] - ETA: 0s - loss: 0.4819 - binary_accuracy: 0.7849WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4817 - binary_accuracy: 0.7850\n",
      "Epoch 30/50\n",
      "519/544 [===========================>..] - ETA: 0s - loss: 0.4778 - binary_accuracy: 0.7923WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4758 - binary_accuracy: 0.7933\n",
      "Epoch 31/50\n",
      "526/544 [============================>.] - ETA: 0s - loss: 0.4785 - binary_accuracy: 0.7872WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4796 - binary_accuracy: 0.7880\n",
      "Epoch 32/50\n",
      "529/544 [============================>.] - ETA: 0s - loss: 0.4771 - binary_accuracy: 0.7924WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4765 - binary_accuracy: 0.7924\n",
      "Epoch 33/50\n",
      "526/544 [============================>.] - ETA: 0s - loss: 0.4687 - binary_accuracy: 0.7988WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4703 - binary_accuracy: 0.7980\n",
      "Epoch 34/50\n",
      "535/544 [============================>.] - ETA: 0s - loss: 0.4726 - binary_accuracy: 0.7943WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4722 - binary_accuracy: 0.7941\n",
      "Epoch 35/50\n",
      "520/544 [===========================>..] - ETA: 0s - loss: 0.4734 - binary_accuracy: 0.7855WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4741 - binary_accuracy: 0.7856\n",
      "Epoch 36/50\n",
      "526/544 [============================>.] - ETA: 0s - loss: 0.4724 - binary_accuracy: 0.7935WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4733 - binary_accuracy: 0.7925\n",
      "Epoch 37/50\n",
      "532/544 [============================>.] - ETA: 0s - loss: 0.4749 - binary_accuracy: 0.7898WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4755 - binary_accuracy: 0.7896\n",
      "Epoch 38/50\n",
      "530/544 [============================>.] - ETA: 0s - loss: 0.4661 - binary_accuracy: 0.7974WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4658 - binary_accuracy: 0.7967\n",
      "Epoch 39/50\n",
      "526/544 [============================>.] - ETA: 0s - loss: 0.4712 - binary_accuracy: 0.7938WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4720 - binary_accuracy: 0.7928\n",
      "Epoch 40/50\n",
      "529/544 [============================>.] - ETA: 0s - loss: 0.4686 - binary_accuracy: 0.7903WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4676 - binary_accuracy: 0.7902\n",
      "Epoch 41/50\n",
      "528/544 [============================>.] - ETA: 0s - loss: 0.4669 - binary_accuracy: 0.7925WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4669 - binary_accuracy: 0.7925\n",
      "Epoch 42/50\n",
      "527/544 [============================>.] - ETA: 0s - loss: 0.4671 - binary_accuracy: 0.7967WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4681 - binary_accuracy: 0.7964\n",
      "Epoch 43/50\n",
      "520/544 [===========================>..] - ETA: 0s - loss: 0.4682 - binary_accuracy: 0.7899WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4707 - binary_accuracy: 0.7888\n",
      "Epoch 44/50\n",
      "520/544 [===========================>..] - ETA: 0s - loss: 0.4676 - binary_accuracy: 0.7957WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4686 - binary_accuracy: 0.7935\n",
      "Epoch 45/50\n",
      "536/544 [============================>.] - ETA: 0s - loss: 0.4706 - binary_accuracy: 0.7915WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4713 - binary_accuracy: 0.7906\n",
      "Epoch 46/50\n",
      "519/544 [===========================>..] - ETA: 0s - loss: 0.4656 - binary_accuracy: 0.7883WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4654 - binary_accuracy: 0.7893\n",
      "Epoch 47/50\n",
      "525/544 [===========================>..] - ETA: 0s - loss: 0.4698 - binary_accuracy: 0.7933WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4686 - binary_accuracy: 0.7941\n",
      "Epoch 48/50\n",
      "521/544 [===========================>..] - ETA: 0s - loss: 0.4723 - binary_accuracy: 0.7905WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4731 - binary_accuracy: 0.7904\n",
      "Epoch 49/50\n",
      "525/544 [===========================>..] - ETA: 0s - loss: 0.4702 - binary_accuracy: 0.7925WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 2ms/step - loss: 0.4694 - binary_accuracy: 0.7924\n",
      "Epoch 50/50\n",
      "543/544 [============================>.] - ETA: 0s - loss: 0.4607 - binary_accuracy: 0.7901WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
      "544/544 [==============================] - 1s 1ms/step - loss: 0.4605 - binary_accuracy: 0.7902\n"
     ]
    }
   ],
   "source": [
    "## 실험 최고 모델\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(30, input_shape=(X_train.shape[1],), \n",
    "                       activation='relu', \n",
    "                       kernel_regularizer=tf.keras.regularizers.l1(0.001),\n",
    "                       kernel_initializer='he_normal'\n",
    "                      ))\n",
    "\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(30, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.001),\n",
    "                       kernel_initializer='he_normal'))\n",
    "\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 6, mode = 'auto')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=50, \n",
    "                    batch_size=16,\n",
    "#                     validation_data=(X_val, y_val),\n",
    "                    callbacks = [early_stopping]\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 0s 970us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.54320294],\n",
       "       [0.10087153],\n",
       "       [0.977016  ],\n",
       "       ...,\n",
       "       [0.7777112 ],\n",
       "       [0.6305897 ],\n",
       "       [0.57586414]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_test 데이터에 대한 예측 수행\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 예측 결과 출력\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"C:\\\\Users\\\\김승호\\\\spaceship-titanic\\\\sample_submission.csv\")\n",
    "preds = [p[0] >= 0.5 for p in y_pred]\n",
    "submission['Transported'] = preds\n",
    "submission.to_csv(\"C:\\\\Users\\\\김승호\\\\spaceship-titanic\\\\Include_Age_submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "109/109 [==============================] - 3s 12ms/step - loss: nan - binary_accuracy: 0.5144 - val_loss: nan - val_binary_accuracy: 0.4963\n",
      "Epoch 2/100\n",
      "109/109 [==============================] - 1s 12ms/step - loss: nan - binary_accuracy: 0.4965 - val_loss: nan - val_binary_accuracy: 0.4963\n",
      "Epoch 3/100\n",
      "109/109 [==============================] - 2s 14ms/step - loss: nan - binary_accuracy: 0.4965 - val_loss: nan - val_binary_accuracy: 0.4963\n",
      "Epoch 4/100\n",
      "109/109 [==============================] - 2s 14ms/step - loss: nan - binary_accuracy: 0.4965 - val_loss: nan - val_binary_accuracy: 0.4963\n",
      "Epoch 5/100\n",
      "101/109 [==========================>...] - ETA: 0s - loss: nan - binary_accuracy: 0.4938"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-128356bc7a1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m history = model.fit(X_train.reshape(X_train.shape[0], X_train.shape[1], 1), y_train, \n\u001b[0m\u001b[0;32m     25\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1443\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1445\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1446\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1447\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1754\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1756\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1757\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1758\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2451\u001b[0m       (graph_function,\n\u001b[0;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1859\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1860\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Conv1D(32, kernel_size=5, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv1D(32, kernel_size=5, activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                       kernel_initializer='he_normal'))\n",
    "\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 8, mode = 'auto')\n",
    "\n",
    "\n",
    "history = model.fit(X_train.reshape(X_train.shape[0], X_train.shape[1], 1), y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=16,\n",
    "                    validation_data=(X_val.reshape(X_val.shape[0], X_val.shape[1], 1), y_val),\n",
    "                    callbacks = [early_stopping]\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하이퍼 파라미터 k,d : ( 10 10 )\n",
      "정확도 :  0.7728253055355859\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 10 15 )\n",
      "정확도 :  0.7739755571531273\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 10 20 )\n",
      "정확도 :  0.7725377426312006\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 10 25 )\n",
      "정확도 :  0.77196261682243\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 10 30 )\n",
      "정확도 :  0.77196261682243\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 10 35 )\n",
      "정확도 :  0.77196261682243\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 10 40 )\n",
      "정확도 :  0.77196261682243\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 10 45 )\n",
      "정확도 :  0.77196261682243\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 10 50 )\n",
      "정확도 :  0.77196261682243\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 10 )\n",
      "정확도 :  0.7787203450754853\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 15 )\n",
      "정확도 :  0.7800143781452192\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 20 )\n",
      "정확도 :  0.784040258806614\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 25 )\n",
      "정확도 :  0.7768511861969806\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 30 )\n",
      "정확도 :  0.7768511861969806\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 35 )\n",
      "정확도 :  0.7768511861969806\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 40 )\n",
      "정확도 :  0.7768511861969806\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 45 )\n",
      "정확도 :  0.7768511861969806\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 20 50 )\n",
      "정확도 :  0.7768511861969806\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 10 )\n",
      "정확도 :  0.779726815240834\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 15 )\n",
      "정확도 :  0.7833213515456506\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 20 )\n",
      "정확도 :  0.7807332854061826\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 25 )\n",
      "정확도 :  0.7795830337886412\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 30 )\n",
      "정확도 :  0.7795830337886412\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 35 )\n",
      "정확도 :  0.7795830337886412\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 40 )\n",
      "정확도 :  0.7795830337886412\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 45 )\n",
      "정확도 :  0.7795830337886412\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 30 50 )\n",
      "정확도 :  0.7795830337886412\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 10 )\n",
      "정확도 :  0.7846153846153846\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 15 )\n",
      "정확도 :  0.7864845434938893\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 20 )\n",
      "정확도 :  0.7851905104241553\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 25 )\n",
      "정확도 :  0.7830337886412653\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 30 )\n",
      "정확도 :  0.7830337886412653\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 35 )\n",
      "정확도 :  0.7830337886412653\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 40 )\n",
      "정확도 :  0.7830337886412653\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 45 )\n",
      "정확도 :  0.7830337886412653\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 40 50 )\n",
      "정확도 :  0.7830337886412653\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 10 )\n",
      "정확도 :  0.7838964773544213\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 15 )\n",
      "정확도 :  0.7889288281811646\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 20 )\n",
      "정확도 :  0.7869158878504673\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 25 )\n",
      "정확도 :  0.785765636232926\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 30 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 35 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 40 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 45 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 50 50 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 10 )\n",
      "정확도 :  0.7851905104241553\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 15 )\n",
      "정확도 :  0.7866283249460819\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 20 )\n",
      "정확도 :  0.7866283249460819\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 25 )\n",
      "정확도 :  0.7860531991373113\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 30 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 35 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 40 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 45 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 60 50 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 10 )\n",
      "정확도 :  0.7841840402588066\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 15 )\n",
      "정확도 :  0.7867721063982747\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 20 )\n",
      "정확도 :  0.7874910136592379\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 25 )\n",
      "정확도 :  0.7863407620416967\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 30 )\n",
      "정확도 :  0.7863407620416967\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 35 )\n",
      "정확도 :  0.7863407620416967\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 40 )\n",
      "정확도 :  0.7863407620416967\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 45 )\n",
      "정확도 :  0.7863407620416967\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 70 50 )\n",
      "정확도 :  0.7863407620416967\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 10 )\n",
      "정확도 :  0.7859094176851186\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 15 )\n",
      "정확도 :  0.7833213515456506\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 20 )\n",
      "정확도 :  0.787922358015816\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 25 )\n",
      "정확도 :  0.78490294751977\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 30 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 35 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 40 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 45 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 80 50 )\n",
      "정확도 :  0.7854780733285406\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 10 )\n",
      "정확도 :  0.78490294751977\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 15 )\n",
      "정확도 :  0.7828900071890726\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 20 )\n",
      "정확도 :  0.7869158878504673\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 25 )\n",
      "정확도 :  0.784040258806614\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 30 )\n",
      "정확도 :  0.7837526959022286\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 35 )\n",
      "정확도 :  0.7837526959022286\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 40 )\n",
      "정확도 :  0.7837526959022286\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 45 )\n",
      "정확도 :  0.7837526959022286\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 90 50 )\n",
      "정확도 :  0.7837526959022286\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 10 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 15 )\n",
      "정확도 :  0.7833213515456506\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 20 )\n",
      "정확도 :  0.7876347951114306\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 25 )\n",
      "정확도 :  0.7850467289719626\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 30 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 35 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 40 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 45 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n",
      "하이퍼 파라미터 k,d : ( 100 50 )\n",
      "정확도 :  0.7847591660675772\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score # 정확도 함수\n",
    "\n",
    "\n",
    "for n in range(10,101,10):\n",
    "    for j in range(10,51,5):\n",
    "        clf = RandomForestClassifier(n_estimators=n, max_depth=j,random_state=0)\n",
    "        clf.fit(X_train,y_train)\n",
    "\n",
    "        predict1 = clf.predict(X_val)\n",
    "        print('하이퍼 파라미터 k,d : (',n,j,')')\n",
    "        print('정확도 : ',accuracy_score(y_val,predict1))\n",
    "        print('=======================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost(Extreme Gradient Boosting)는 GBM(Gradient Boosting Machine) 기반의 앙상블 학습 알고리즘 중 하나로, 다양한 데이터 분류 및 회귀 문제에 대해 높은 성능을 보이고 있습니다.\n",
    "\n",
    "XGBoost의 주요 특징은 다음과 같습니다.\n",
    "\n",
    "* Regularization: L1, L2 regularization 및 max depth 등을 통해 overfitting 방지\n",
    "* Tree Pruning: 불필요한 가지 제거를 통한 효율적인 모델 생성\n",
    "* Handling Missing Values: 결측치 처리 기능 제공\n",
    "* Cross-Validation: 교차 검증 기능 제공\n",
    "* Parallel Processing: 멀티코어 CPU를 이용한 병렬처리\n",
    "\n",
    "하이퍼파라미터 세팅은 다음과 같은 것들이 있습니다.\n",
    "\n",
    "* n_estimators: 생성할 tree의 개수\n",
    "* max_depth: 트리의 깊이\n",
    "* learning_rate: 각 트리마다 가중치를 얼마나 적용할지를 결정하는 파라미터\n",
    "* gamma: loss function의 값이 얼마나 감소해야 leaf node를 생성할 것인지 결정하는 파라미터\n",
    "* subsample: 데이터를 샘플링하는 비율\n",
    "* colsample_bytree: 각 tree 생성 시, feature를 샘플링하는 비율\n",
    "* reg_alpha: L1 regularization 계수\n",
    "* reg_lambda: L2 regularization 계수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\anaconda\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\anaconda\\lib\\site-packages (from xgboost) (1.5.2)\n",
      "Requirement already satisfied: numpy in c:\\anaconda\\lib\\site-packages (from xgboost) (1.22.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78058950395399"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 모델 선언 예시\n",
    "model = XGBClassifier(n_estimators=1000, learning_rate=0.02, max_depth=25, random_state = 32)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val) \n",
    "\n",
    "# 예측 라벨과 실제 라벨 사이의 정확도 측정\n",
    "accuracy_score(y_pred, y_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1738, 15)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:39:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"early_stopping_rounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.88369\tval-auc:0.87393\n",
      "[50]\ttrain-auc:0.91092\tval-auc:0.88745\n",
      "[100]\ttrain-auc:0.91695\tval-auc:0.89137\n",
      "[150]\ttrain-auc:0.92092\tval-auc:0.89274\n",
      "[200]\ttrain-auc:0.92506\tval-auc:0.89451\n",
      "[250]\ttrain-auc:0.92930\tval-auc:0.89665\n",
      "[300]\ttrain-auc:0.93314\tval-auc:0.89799\n",
      "[350]\ttrain-auc:0.93642\tval-auc:0.89894\n",
      "[400]\ttrain-auc:0.93924\tval-auc:0.89989\n",
      "[450]\ttrain-auc:0.94169\tval-auc:0.90064\n",
      "[500]\ttrain-auc:0.94382\tval-auc:0.90109\n",
      "[550]\ttrain-auc:0.94619\tval-auc:0.90154\n",
      "[600]\ttrain-auc:0.94820\tval-auc:0.90189\n",
      "[650]\ttrain-auc:0.95028\tval-auc:0.90210\n",
      "[700]\ttrain-auc:0.95218\tval-auc:0.90238\n",
      "[750]\ttrain-auc:0.95398\tval-auc:0.90272\n",
      "[800]\ttrain-auc:0.95588\tval-auc:0.90312\n",
      "[850]\ttrain-auc:0.95758\tval-auc:0.90331\n",
      "[900]\ttrain-auc:0.95911\tval-auc:0.90357\n",
      "[950]\ttrain-auc:0.96098\tval-auc:0.90400\n",
      "[1000]\ttrain-auc:0.96279\tval-auc:0.90424\n",
      "[1050]\ttrain-auc:0.96433\tval-auc:0.90445\n",
      "[1100]\ttrain-auc:0.96592\tval-auc:0.90477\n",
      "[1150]\ttrain-auc:0.96734\tval-auc:0.90491\n",
      "[1200]\ttrain-auc:0.96874\tval-auc:0.90487\n",
      "[1250]\ttrain-auc:0.97006\tval-auc:0.90500\n",
      "[1300]\ttrain-auc:0.97133\tval-auc:0.90491\n",
      "[1350]\ttrain-auc:0.97271\tval-auc:0.90512\n",
      "[1400]\ttrain-auc:0.97397\tval-auc:0.90539\n",
      "[1450]\ttrain-auc:0.97505\tval-auc:0.90540\n",
      "[1500]\ttrain-auc:0.97610\tval-auc:0.90553\n",
      "[1550]\ttrain-auc:0.97715\tval-auc:0.90545\n",
      "[1600]\ttrain-auc:0.97819\tval-auc:0.90558\n",
      "[1650]\ttrain-auc:0.97915\tval-auc:0.90539\n",
      "[1700]\ttrain-auc:0.98010\tval-auc:0.90542\n",
      "[1750]\ttrain-auc:0.98105\tval-auc:0.90543\n",
      "[1800]\ttrain-auc:0.98184\tval-auc:0.90538\n",
      "[1850]\ttrain-auc:0.98269\tval-auc:0.90535\n",
      "[1900]\ttrain-auc:0.98343\tval-auc:0.90523\n",
      "[1950]\ttrain-auc:0.98403\tval-auc:0.90521\n",
      "[1999]\ttrain-auc:0.98465\tval-auc:0.90532\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# X_train, y_train, X_val, y_val 데이터가 있다고 가정합니다.\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "params = {\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.01,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'early_stopping_rounds': 20,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# 모델 학습\n",
    "model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=2000,\n",
    "    evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "# 모델 예측\n",
    "y_pred = model.predict(xgb.DMatrix(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과 출력\n",
    "y_preds= [p >= 0.5 for p in y_pred]\n",
    "\n",
    "submission = pd.read_csv(\"C:\\\\Users\\\\김승호\\\\spaceship-titanic\\\\sample_submission.csv\")\n",
    "\n",
    "submission['Transported'] = y_preds\n",
    "submission.to_csv(\"C:\\\\Users\\\\김승호\\\\spaceship-titanic\\\\xgboost2_submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0017215164"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6955"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "xgb.plot_importance(model, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# 여기서 (f+'숫자') 조합은, 원래 df의 column의 index와 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = model.get_score(importance_type='gain')\n",
    "importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "important_column_index = [int(f[1:]) for f, _ in importance]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 중요 변수 :  CryoSleep\n",
      "1 번째 중요 변수 :  HomePlanet\n",
      "2 번째 중요 변수 :  spend_fillna_0\n",
      "3 번째 중요 변수 :  deck\n",
      "4 번째 중요 변수 :  Spa\n",
      "5 번째 중요 변수 :  FoodCourt\n",
      "6 번째 중요 변수 :  side\n",
      "7 번째 중요 변수 :  VRDeck\n",
      "8 번째 중요 변수 :  ShoppingMall\n",
      "9 번째 중요 변수 :  RoomService\n",
      "10 번째 중요 변수 :  Destination\n",
      "11 번째 중요 변수 :  num\n",
      "12 번째 중요 변수 :  group\n",
      "13 번째 중요 변수 :  last_name\n",
      "14 번째 중요 변수 :  VIP_fill_na_False\n"
     ]
    }
   ],
   "source": [
    "for i,ind in enumerate(important_column_index) :\n",
    "    print(i,'번째 중요 변수 : ',df.columns[ind])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 왜 내가 제일 중요하다 생각했던 group이랑 VIP 가 중요도가 이렇게 낮지?\n",
    "\n",
    "My Q) \n",
    "\n",
    "위는 XGboost 방식으로 계산한 변수 중요도야. 그런데 조금 궁금한게, VIP_fill_na_False 변수가 True일 경우, 사례는 적지만  target이 true인 경향이 60%로 매우 높았고, VIP_fill_na_False 변수가 False일 경우, 사례는 매우 많지만 target이 true인 경향이 49%로, VIP_fill_na 변수가 타겟변수와 유의미한 상관관계를 가질 것으로 예측했고, 실제 corr로 계산한 피어슨 상관계수도 타 변수에 비해 어느정도 높았는데 왜 예측에서 쓰이지 않은걸까? 네가 생각하는 이유를 설명해줘\n",
    "\n",
    "<br>\n",
    "\n",
    "Chat gpt A) \n",
    "\n",
    "VIP_fill_na_False 변수가 타겟 변수와 상관관계가 높다고 해서 반드시 모델에 반영되는 것은 아닙니다. 변수 중요도는 해당 변수가 모델 예측에 얼마나 중요한 역할을 하는지를 나타내는 지표일 뿐, 실제로 모델에서 사용되는 변수와 중요도가 높은 변수가 일치하지 않을 수도 있습니다.\n",
    "\n",
    "또한, VIP_fill_na_False 변수가 True일 경우 target이 true인 경향이 높다고 하더라도, 이 변수 자체가 다른 변수와 중복되거나 정보의 양이 적어서 모델에서 더이상 유의미한 변수가 아닐 수도 있습니다. 따라서 변수 중요도만으로는 모델 예측 결과를 충분히 설명할 수 없습니다.\n",
    "\n",
    "만약 VIP_fill_na_False 변수가 모델 예측에 미치는 영향을 확인하고 싶다면, 이 변수를 제외한 다른 변수들과 함께 모델을 학습하고 변수 중요도를 계산해보는 것이 좋습니다. 이를 통해 VIP_fill_na_False 변수의 유의미성을 다시한번 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
